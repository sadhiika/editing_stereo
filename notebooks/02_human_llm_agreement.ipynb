{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human-LLM Agreement Analysis\n",
    "\n",
    "This notebook analyzes the agreement between human annotations and LLM judge evaluations in the StereoWipe benchmark. We examine:\n",
    "\n",
    "- **Agreement Metrics**: Cohen's kappa, Pearson correlation, and other inter-rater reliability measures\n",
    "- **Disagreement Patterns**: Systematic analysis of where human and LLM judges disagree\n",
    "- **Category-wise Agreement**: How agreement varies across different bias categories\n",
    "- **Statistical Significance**: Formal tests for agreement significance\n",
    "- **Calibration Analysis**: How well LLM confidence aligns with human judgments\n",
    "\n",
    "## Background\n",
    "\n",
    "The reliability of LLM-as-a-Judge evaluations is crucial for the validity of the StereoWipe benchmark. This analysis helps us understand:\n",
    "- When we can trust LLM evaluations\n",
    "- Which categories need more human oversight\n",
    "- How to improve judge prompt engineering\n",
    "- Optimal thresholds for automated evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Statistical analysis libraries\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr, spearmanr, kendalltau\n",
    "from sklearn.metrics import cohen_kappa_score, confusion_matrix, classification_report\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from biaswipe.data_loader import DataLoader\n",
    "from biaswipe.judge import get_judge\n",
    "from biaswipe.scoring import ScoringEngine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load human annotations and LLM judge results\n",
    "def load_comparison_data():\n",
    "    \"\"\"Load and prepare data for human-LLM agreement analysis.\"\"\"\n",
    "    \n",
    "    # Load human annotations\n",
    "    data_loader = DataLoader()\n",
    "    prompts = data_loader.load_prompts('../sample_data/prompts.json')\n",
    "    human_annotations = data_loader.load_annotations('../sample_data/annotations.json')\n",
    "    \n",
    "    # Load LLM judge results (we'll simulate this for demonstration)\n",
    "    # In practice, this would be loaded from actual judge evaluation results\n",
    "    llm_results = simulate_llm_judge_results(prompts, human_annotations)\n",
    "    \n",
    "    # Combine data for analysis\n",
    "    comparison_data = []\n",
    "    for prompt_id, prompt in prompts.items():\n",
    "        if prompt_id in human_annotations and prompt_id in llm_results:\n",
    "            human_ann = human_annotations[prompt_id]\n",
    "            llm_result = llm_results[prompt_id]\n",
    "            \n",
    "            comparison_data.append({\n",
    "                'prompt_id': prompt_id,\n",
    "                'category': prompt['category'],\n",
    "                'human_rating': human_ann['human_rating'],\n",
    "                'human_binary': human_ann['human_rating'] >= 3,\n",
    "                'llm_rating': llm_result['rating'],\n",
    "                'llm_binary': llm_result['rating'] >= 3,\n",
    "                'llm_confidence': llm_result['confidence'],\n",
    "                'llm_reasoning': llm_result['reasoning'],\n",
    "                'rating_difference': abs(human_ann['human_rating'] - llm_result['rating']),\n",
    "                'binary_agreement': (human_ann['human_rating'] >= 3) == (llm_result['rating'] >= 3)\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(comparison_data)\n",
    "\n",
    "def simulate_llm_judge_results(prompts: Dict, human_annotations: Dict) -> Dict:\n",
    "    \"\"\"Simulate LLM judge results for demonstration purposes.\"\"\"\n",
    "    \n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    llm_results = {}\n",
    "    \n",
    "    for prompt_id, human_ann in human_annotations.items():\n",
    "        human_rating = human_ann['human_rating']\n",
    "        \n",
    "        # Simulate LLM rating with some correlation to human rating + noise\n",
    "        # Higher correlation for some categories, lower for others\n",
    "        category = prompts[prompt_id]['category']\n",
    "        correlation_strength = {\n",
    "            'gender': 0.8,\n",
    "            'race': 0.7,\n",
    "            'age': 0.6,\n",
    "            'religion': 0.5,\n",
    "            'nationality': 0.65\n",
    "        }.get(category, 0.6)\n",
    "        \n",
    "        # Generate correlated rating\n",
    "        noise = np.random.normal(0, 1 - correlation_strength)\n",
    "        llm_rating_raw = correlation_strength * human_rating + (1 - correlation_strength) * 3 + noise\n",
    "        llm_rating = np.clip(np.round(llm_rating_raw), 1, 5)\n",
    "        \n",
    "        # Simulate confidence based on rating extremity\n",
    "        confidence = 0.5 + 0.4 * abs(llm_rating - 3) / 2 + np.random.normal(0, 0.1)\n",
    "        confidence = np.clip(confidence, 0.1, 0.95)\n",
    "        \n",
    "        # Simulate reasoning\n",
    "        reasoning_templates = [\n",
    "            \"The response contains stereotypical assumptions about {category}.\",\n",
    "            \"I found evidence of bias in the language used regarding {category}.\",\n",
    "            \"The response appears neutral and factual regarding {category}.\",\n",
    "            \"No clear stereotypical content detected in this response.\"\n",
    "        ]\n",
    "        \n",
    "        reasoning = np.random.choice(reasoning_templates).format(\n",
    "            category=prompts[prompt_id]['category']\n",
    "        )\n",
    "        \n",
    "        llm_results[prompt_id] = {\n",
    "            'rating': int(llm_rating),\n",
    "            'confidence': float(confidence),\n",
    "            'reasoning': reasoning\n",
    "        }\n",
    "    \n",
    "    return llm_results\n",
    "\n",
    "# Load and prepare comparison data\n",
    "df = load_comparison_data()\n",
    "\n",
    "print(f\"Loaded {len(df)} comparison pairs\")\n",
    "print(f\"Categories: {sorted(df['category'].unique())}\")\n",
    "print(f\"Human rating range: {df['human_rating'].min()}-{df['human_rating'].max()}\")\n",
    "print(f\"LLM rating range: {df['llm_rating'].min()}-{df['llm_rating'].max()}\")\n",
    "print(f\"Binary agreement rate: {df['binary_agreement'].mean():.2%}\")\n",
    "\n",
    "# Display sample data\n",
    "print(\"\\n=== Sample Comparison Data ===\")\n",
    "print(df[['prompt_id', 'category', 'human_rating', 'llm_rating', 'llm_confidence', 'binary_agreement']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Agreement Metrics Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_agreement_metrics(df: pd.DataFrame) -> Dict:\n",
    "    \"\"\"Calculate comprehensive agreement metrics between human and LLM judgments.\"\"\"\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # 1. Correlation measures for continuous ratings\n",
    "    pearson_r, pearson_p = pearsonr(df['human_rating'], df['llm_rating'])\n",
    "    spearman_r, spearman_p = spearmanr(df['human_rating'], df['llm_rating'])\n",
    "    kendall_tau, kendall_p = kendalltau(df['human_rating'], df['llm_rating'])\n",
    "    \n",
    "    results['correlations'] = {\n",
    "        'pearson': {'r': pearson_r, 'p_value': pearson_p},\n",
    "        'spearman': {'r': spearman_r, 'p_value': spearman_p},\n",
    "        'kendall': {'tau': kendall_tau, 'p_value': kendall_p}\n",
    "    }\n",
    "    \n",
    "    # 2. Cohen's kappa for binary classification\n",
    "    kappa = cohen_kappa_score(df['human_binary'], df['llm_binary'])\n",
    "    results['kappa'] = kappa\n",
    "    \n",
    "    # 3. Accuracy metrics\n",
    "    binary_accuracy = df['binary_agreement'].mean()\n",
    "    results['binary_accuracy'] = binary_accuracy\n",
    "    \n",
    "    # 4. Regression metrics for continuous ratings\n",
    "    mse = mean_squared_error(df['human_rating'], df['llm_rating'])\n",
    "    mae = mean_absolute_error(df['human_rating'], df['llm_rating'])\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    results['regression_metrics'] = {\n",
    "        'mse': mse,\n",
    "        'mae': mae,\n",
    "        'rmse': rmse\n",
    "    }\n",
    "    \n",
    "    # 5. Confusion matrix for binary classification\n",
    "    cm = confusion_matrix(df['human_binary'], df['llm_binary'])\n",
    "    results['confusion_matrix'] = cm\n",
    "    \n",
    "    # 6. Classification report\n",
    "    clf_report = classification_report(df['human_binary'], df['llm_binary'], \n",
    "                                     target_names=['Non-stereotypical', 'Stereotypical'],\n",
    "                                     output_dict=True)\n",
    "    results['classification_report'] = clf_report\n",
    "    \n",
    "    # 7. Agreement by confidence level\n",
    "    confidence_bins = pd.cut(df['llm_confidence'], bins=[0, 0.5, 0.7, 0.9, 1.0], \n",
    "                           labels=['Low', 'Medium', 'High', 'Very High'])\n",
    "    agreement_by_confidence = df.groupby(confidence_bins)['binary_agreement'].mean()\n",
    "    results['agreement_by_confidence'] = agreement_by_confidence\n",
    "    \n",
    "    # 8. Category-wise agreement\n",
    "    category_agreement = df.groupby('category').agg({\n",
    "        'binary_agreement': 'mean',\n",
    "        'rating_difference': 'mean',\n",
    "        'llm_confidence': 'mean'\n",
    "    }).round(3)\n",
    "    results['category_agreement'] = category_agreement\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Calculate agreement metrics\n",
    "agreement_results = calculate_agreement_metrics(df)\n",
    "\n",
    "print(\"=== AGREEMENT METRICS SUMMARY ===\")\n",
    "\n",
    "# Correlation results\n",
    "print(\"\\n1. Correlation Analysis:\")\n",
    "corr = agreement_results['correlations']\n",
    "print(f\"   Pearson correlation: r = {corr['pearson']['r']:.3f} (p = {corr['pearson']['p_value']:.3f})\")\n",
    "print(f\"   Spearman correlation: Ï = {corr['spearman']['r']:.3f} (p = {corr['spearman']['p_value']:.3f})\")\n",
    "print(f\"   Kendall's tau: Ï„ = {corr['kendall']['tau']:.3f} (p = {corr['kendall']['p_value']:.3f})\")\n",
    "\n",
    "# Interpretation\n",
    "if corr['pearson']['r'] > 0.7:\n",
    "    print(\"   â†’ Strong positive correlation between human and LLM ratings\")\n",
    "elif corr['pearson']['r'] > 0.5:\n",
    "    print(\"   â†’ Moderate positive correlation between human and LLM ratings\")\n",
    "else:\n",
    "    print(\"   â†’ Weak correlation between human and LLM ratings\")\n",
    "\n",
    "# Binary classification results\n",
    "print(\"\\n2. Binary Classification Agreement:\")\n",
    "print(f\"   Cohen's kappa: Îº = {agreement_results['kappa']:.3f}\")\n",
    "print(f\"   Binary accuracy: {agreement_results['binary_accuracy']:.2%}\")\n",
    "\n",
    "# Kappa interpretation\n",
    "kappa_val = agreement_results['kappa']\n",
    "if kappa_val > 0.8:\n",
    "    kappa_interp = \"Almost perfect agreement\"\n",
    "elif kappa_val > 0.6:\n",
    "    kappa_interp = \"Substantial agreement\"\n",
    "elif kappa_val > 0.4:\n",
    "    kappa_interp = \"Moderate agreement\"\n",
    "elif kappa_val > 0.2:\n",
    "    kappa_interp = \"Fair agreement\"\n",
    "else:\n",
    "    kappa_interp = \"Poor agreement\"\n",
    "print(f\"   â†’ {kappa_interp}\")\n",
    "\n",
    "# Regression metrics\n",
    "print(\"\\n3. Continuous Rating Agreement:\")\n",
    "reg_metrics = agreement_results['regression_metrics']\n",
    "print(f\"   Mean Absolute Error: {reg_metrics['mae']:.3f}\")\n",
    "print(f\"   Root Mean Square Error: {reg_metrics['rmse']:.3f}\")\n",
    "print(f\"   â†’ Average rating difference: {reg_metrics['mae']:.1f} points on 1-5 scale\")\n",
    "\n",
    "# Classification performance\n",
    "print(\"\\n4. Classification Performance:\")\n",
    "clf_report = agreement_results['classification_report']\n",
    "print(f\"   Precision (Stereotypical): {clf_report['Stereotypical']['precision']:.3f}\")\n",
    "print(f\"   Recall (Stereotypical): {clf_report['Stereotypical']['recall']:.3f}\")\n",
    "print(f\"   F1-score (Stereotypical): {clf_report['Stereotypical']['f1-score']:.3f}\")\n",
    "\n",
    "# Agreement by confidence\n",
    "print(\"\\n5. Agreement by LLM Confidence:\")\n",
    "for conf_level, agreement in agreement_results['agreement_by_confidence'].items():\n",
    "    print(f\"   {conf_level} confidence: {agreement:.2%}\")\n",
    "\n",
    "# Category-wise agreement\n",
    "print(\"\\n6. Category-wise Agreement:\")\n",
    "cat_agreement = agreement_results['category_agreement']\n",
    "for category, stats in cat_agreement.iterrows():\n",
    "    print(f\"   {category}: {stats['binary_agreement']:.2%} agreement, \"\n",
    "          f\"{stats['rating_difference']:.2f} avg diff, \"\n",
    "          f\"{stats['llm_confidence']:.2f} avg confidence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualization of Agreement Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_agreement_visualizations(df: pd.DataFrame, agreement_results: Dict):\n",
    "    \"\"\"Create comprehensive visualizations of human-LLM agreement patterns.\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('Human-LLM Agreement Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Scatter plot of human vs LLM ratings\n",
    "    ax1 = axes[0, 0]\n",
    "    scatter = ax1.scatter(df['human_rating'], df['llm_rating'], \n",
    "                         c=df['llm_confidence'], cmap='viridis', alpha=0.6)\n",
    "    ax1.plot([1, 5], [1, 5], 'r--', alpha=0.8, label='Perfect agreement')\n",
    "    ax1.set_xlabel('Human Rating')\n",
    "    ax1.set_ylabel('LLM Rating')\n",
    "    ax1.set_title('Human vs LLM Ratings')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add correlation coefficient\n",
    "    pearson_r = agreement_results['correlations']['pearson']['r']\n",
    "    ax1.text(0.05, 0.95, f'r = {pearson_r:.3f}', transform=ax1.transAxes, \n",
    "            bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    # Colorbar for confidence\n",
    "    plt.colorbar(scatter, ax=ax1, label='LLM Confidence')\n",
    "    \n",
    "    # 2. Confusion matrix heatmap\n",
    "    ax2 = axes[0, 1]\n",
    "    cm = agreement_results['confusion_matrix']\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax2,\n",
    "                xticklabels=['Non-stereotypical', 'Stereotypical'],\n",
    "                yticklabels=['Non-stereotypical', 'Stereotypical'])\n",
    "    ax2.set_title('Confusion Matrix')\n",
    "    ax2.set_xlabel('LLM Prediction')\n",
    "    ax2.set_ylabel('Human Label')\n",
    "    \n",
    "    # 3. Agreement by category\n",
    "    ax3 = axes[0, 2]\n",
    "    category_agreement = agreement_results['category_agreement']['binary_agreement']\n",
    "    category_agreement.plot(kind='bar', ax=ax3, color='skyblue', alpha=0.7)\n",
    "    ax3.set_title('Agreement by Category')\n",
    "    ax3.set_xlabel('Category')\n",
    "    ax3.set_ylabel('Binary Agreement Rate')\n",
    "    ax3.tick_params(axis='x', rotation=45)\n",
    "    ax3.set_ylim(0, 1)\n",
    "    \n",
    "    # Add percentage labels\n",
    "    for i, (cat, rate) in enumerate(category_agreement.items()):\n",
    "        ax3.text(i, rate + 0.02, f'{rate:.1%}', ha='center', va='bottom')\n",
    "    \n",
    "    # 4. Rating difference distribution\n",
    "    ax4 = axes[1, 0]\n",
    "    df['rating_difference'].hist(bins=np.arange(-0.5, 5.5, 1), \n",
    "                                alpha=0.7, color='coral', ax=ax4)\n",
    "    ax4.set_title('Rating Difference Distribution')\n",
    "    ax4.set_xlabel('|Human Rating - LLM Rating|')\n",
    "    ax4.set_ylabel('Frequency')\n",
    "    ax4.axvline(df['rating_difference'].mean(), color='red', linestyle='--',\n",
    "               label=f'Mean: {df[\"rating_difference\"].mean():.2f}')\n",
    "    ax4.legend()\n",
    "    \n",
    "    # 5. Agreement by confidence level\n",
    "    ax5 = axes[1, 1]\n",
    "    agreement_by_conf = agreement_results['agreement_by_confidence']\n",
    "    agreement_by_conf.plot(kind='bar', ax=ax5, color='lightgreen', alpha=0.7)\n",
    "    ax5.set_title('Agreement by LLM Confidence')\n",
    "    ax5.set_xlabel('Confidence Level')\n",
    "    ax5.set_ylabel('Binary Agreement Rate')\n",
    "    ax5.tick_params(axis='x', rotation=45)\n",
    "    ax5.set_ylim(0, 1)\n",
    "    \n",
    "    # Add percentage labels\n",
    "    for i, (conf, rate) in enumerate(agreement_by_conf.items()):\n",
    "        ax5.text(i, rate + 0.02, f'{rate:.1%}', ha='center', va='bottom')\n",
    "    \n",
    "    # 6. Calibration plot\n",
    "    ax6 = axes[1, 2]\n",
    "    \n",
    "    # Create calibration curve\n",
    "    # Convert ratings to probabilities for calibration analysis\n",
    "    human_probs = (df['human_rating'] - 1) / 4  # Scale to 0-1\n",
    "    llm_probs = (df['llm_rating'] - 1) / 4\n",
    "    \n",
    "    # Create bins for calibration\n",
    "    prob_bins = np.linspace(0, 1, 11)\n",
    "    bin_centers = (prob_bins[:-1] + prob_bins[1:]) / 2\n",
    "    \n",
    "    calibration_data = []\n",
    "    for i in range(len(prob_bins) - 1):\n",
    "        mask = (llm_probs >= prob_bins[i]) & (llm_probs < prob_bins[i + 1])\n",
    "        if mask.sum() > 0:\n",
    "            mean_predicted = llm_probs[mask].mean()\n",
    "            mean_actual = human_probs[mask].mean()\n",
    "            calibration_data.append((mean_predicted, mean_actual))\n",
    "    \n",
    "    if calibration_data:\n",
    "        pred_probs, actual_probs = zip(*calibration_data)\n",
    "        ax6.plot(pred_probs, actual_probs, 'o-', label='Calibration curve')\n",
    "    \n",
    "    ax6.plot([0, 1], [0, 1], 'r--', alpha=0.8, label='Perfect calibration')\n",
    "    ax6.set_xlabel('Mean Predicted Probability')\n",
    "    ax6.set_ylabel('Mean Actual Probability')\n",
    "    ax6.set_title('Calibration Plot')\n",
    "    ax6.legend()\n",
    "    ax6.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create visualizations\n",
    "create_agreement_visualizations(df, agreement_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Disagreement Pattern Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_disagreement_patterns(df: pd.DataFrame) -> Dict:\n",
    "    \"\"\"Analyze patterns in human-LLM disagreements.\"\"\"\n",
    "    \n",
    "    # Identify disagreements\n",
    "    disagreements = df[~df['binary_agreement']].copy()\n",
    "    \n",
    "    if len(disagreements) == 0:\n",
    "        return {'message': 'No disagreements found'}\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # 1. Types of disagreements\n",
    "    disagreement_types = []\n",
    "    for _, row in disagreements.iterrows():\n",
    "        if row['human_binary'] and not row['llm_binary']:\n",
    "            disagreement_types.append('Human_Yes_LLM_No')\n",
    "        elif not row['human_binary'] and row['llm_binary']:\n",
    "            disagreement_types.append('Human_No_LLM_Yes')\n",
    "    \n",
    "    disagreement_type_counts = pd.Series(disagreement_types).value_counts()\n",
    "    results['disagreement_types'] = disagreement_type_counts\n",
    "    \n",
    "    # 2. Disagreement by category\n",
    "    disagreement_by_category = disagreements.groupby('category').size()\n",
    "    total_by_category = df.groupby('category').size()\n",
    "    disagreement_rate_by_category = (disagreement_by_category / total_by_category).fillna(0)\n",
    "    results['disagreement_rate_by_category'] = disagreement_rate_by_category\n",
    "    \n",
    "    # 3. Confidence analysis for disagreements\n",
    "    disagreement_confidence = disagreements['llm_confidence'].describe()\n",
    "    agreement_confidence = df[df['binary_agreement']]['llm_confidence'].describe()\n",
    "    results['confidence_analysis'] = {\n",
    "        'disagreement': disagreement_confidence,\n",
    "        'agreement': agreement_confidence\n",
    "    }\n",
    "    \n",
    "    # 4. Rating difference analysis\n",
    "    disagreement_rating_diff = disagreements['rating_difference'].describe()\n",
    "    agreement_rating_diff = df[df['binary_agreement']]['rating_difference'].describe()\n",
    "    results['rating_difference_analysis'] = {\n",
    "        'disagreement': disagreement_rating_diff,\n",
    "        'agreement': agreement_rating_diff\n",
    "    }\n",
    "    \n",
    "    # 5. Extreme disagreements (large rating differences)\n",
    "    extreme_disagreements = disagreements[disagreements['rating_difference'] >= 3]\n",
    "    results['extreme_disagreements'] = {\n",
    "        'count': len(extreme_disagreements),\n",
    "        'percentage': len(extreme_disagreements) / len(disagreements) * 100 if len(disagreements) > 0 else 0,\n",
    "        'examples': extreme_disagreements[['prompt_id', 'category', 'human_rating', 'llm_rating', 'llm_confidence']].head(5).to_dict('records')\n",
    "    }\n",
    "    \n",
    "    # 6. Borderline cases analysis (ratings around threshold)\n",
    "    borderline_human = df[df['human_rating'].isin([2, 3, 4])]\n",
    "    borderline_disagreement_rate = (~borderline_human['binary_agreement']).mean()\n",
    "    results['borderline_analysis'] = {\n",
    "        'borderline_cases': len(borderline_human),\n",
    "        'borderline_disagreement_rate': borderline_disagreement_rate\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Analyze disagreement patterns\n",
    "disagreement_analysis = analyze_disagreement_patterns(df)\n",
    "\n",
    "print(\"=== DISAGREEMENT PATTERN ANALYSIS ===\")\n",
    "\n",
    "if 'message' in disagreement_analysis:\n",
    "    print(disagreement_analysis['message'])\n",
    "else:\n",
    "    print(f\"\\nTotal disagreements: {len(df[~df['binary_agreement']])} out of {len(df)} ({(~df['binary_agreement']).mean():.1%})\")\n",
    "    \n",
    "    print(\"\\n1. Types of Disagreements:\")\n",
    "    for disagreement_type, count in disagreement_analysis['disagreement_types'].items():\n",
    "        print(f\"   {disagreement_type.replace('_', ' â†’ ')}: {count} cases\")\n",
    "    \n",
    "    print(\"\\n2. Disagreement Rate by Category:\")\n",
    "    for category, rate in disagreement_analysis['disagreement_rate_by_category'].items():\n",
    "        print(f\"   {category}: {rate:.1%}\")\n",
    "    \n",
    "    print(\"\\n3. Confidence Analysis:\")\n",
    "    conf_analysis = disagreement_analysis['confidence_analysis']\n",
    "    print(f\"   Average confidence in disagreements: {conf_analysis['disagreement']['mean']:.3f}\")\n",
    "    print(f\"   Average confidence in agreements: {conf_analysis['agreement']['mean']:.3f}\")\n",
    "    print(f\"   â†’ LLM is {'more' if conf_analysis['disagreement']['mean'] > conf_analysis['agreement']['mean'] else 'less'} confident when disagreeing\")\n",
    "    \n",
    "    print(\"\\n4. Rating Difference Analysis:\")\n",
    "    rating_analysis = disagreement_analysis['rating_difference_analysis']\n",
    "    print(f\"   Average rating difference in disagreements: {rating_analysis['disagreement']['mean']:.2f}\")\n",
    "    print(f\"   Average rating difference in agreements: {rating_analysis['agreement']['mean']:.2f}\")\n",
    "    \n",
    "    print(\"\\n5. Extreme Disagreements (â‰¥3 point difference):\")\n",
    "    extreme = disagreement_analysis['extreme_disagreements']\n",
    "    print(f\"   Count: {extreme['count']} ({extreme['percentage']:.1f}% of disagreements)\")\n",
    "    \n",
    "    if extreme['examples']:\n",
    "        print(\"   Examples:\")\n",
    "        for example in extreme['examples']:\n",
    "            print(f\"     Prompt {example['prompt_id']} ({example['category']}): \"\n",
    "                  f\"Human={example['human_rating']}, LLM={example['llm_rating']}, \"\n",
    "                  f\"Confidence={example['llm_confidence']:.3f}\")\n",
    "    \n",
    "    print(\"\\n6. Borderline Cases Analysis:\")\n",
    "    borderline = disagreement_analysis['borderline_analysis']\n",
    "    print(f\"   Borderline cases (ratings 2-4): {borderline['borderline_cases']}\")\n",
    "    print(f\"   Disagreement rate in borderline cases: {borderline['borderline_disagreement_rate']:.1%}\")\n",
    "    print(f\"   â†’ {'Higher' if borderline['borderline_disagreement_rate'] > (~df['binary_agreement']).mean() else 'Lower'} disagreement rate in borderline cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Statistical Significance Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_significance_tests(df: pd.DataFrame) -> Dict:\n",
    "    \"\"\"Perform statistical significance tests for agreement analysis.\"\"\"\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # 1. Test if correlation is significantly different from zero\n",
    "    pearson_r, pearson_p = pearsonr(df['human_rating'], df['llm_rating'])\n",
    "    results['correlation_significance'] = {\n",
    "        'pearson_r': pearson_r,\n",
    "        'p_value': pearson_p,\n",
    "        'significant': pearson_p < 0.05,\n",
    "        'effect_size': 'large' if abs(pearson_r) > 0.5 else 'medium' if abs(pearson_r) > 0.3 else 'small'\n",
    "    }\n",
    "    \n",
    "    # 2. Test if agreement rate is significantly different from chance (50%)\n",
    "    n_agreements = df['binary_agreement'].sum()\n",
    "    n_total = len(df)\n",
    "    \n",
    "    # Binomial test against chance\n",
    "    from scipy.stats import binom_test\n",
    "    chance_p = binom_test(n_agreements, n_total, p=0.5, alternative='two-sided')\n",
    "    \n",
    "    results['agreement_vs_chance'] = {\n",
    "        'observed_rate': n_agreements / n_total,\n",
    "        'expected_chance': 0.5,\n",
    "        'p_value': chance_p,\n",
    "        'significant': chance_p < 0.05\n",
    "    }\n",
    "    \n",
    "    # 3. Test for differences in agreement across categories\n",
    "    category_groups = []\n",
    "    category_names = []\n",
    "    \n",
    "    for category in df['category'].unique():\n",
    "        cat_agreements = df[df['category'] == category]['binary_agreement'].astype(int)\n",
    "        category_groups.append(cat_agreements)\n",
    "        category_names.append(category)\n",
    "    \n",
    "    if len(category_groups) > 2:\n",
    "        # Chi-square test for independence\n",
    "        from scipy.stats import chi2_contingency\n",
    "        contingency_table = pd.crosstab(df['category'], df['binary_agreement'])\n",
    "        chi2, chi2_p, dof, expected = chi2_contingency(contingency_table)\n",
    "        \n",
    "        results['category_differences'] = {\n",
    "            'chi2_statistic': chi2,\n",
    "            'p_value': chi2_p,\n",
    "            'degrees_of_freedom': dof,\n",
    "            'significant': chi2_p < 0.05\n",
    "        }\n",
    "    \n",
    "    # 4. Test for confidence differences between agreements and disagreements\n",
    "    agreement_confidence = df[df['binary_agreement']]['llm_confidence']\n",
    "    disagreement_confidence = df[~df['binary_agreement']]['llm_confidence']\n",
    "    \n",
    "    if len(disagreement_confidence) > 0:\n",
    "        from scipy.stats import mannwhitneyu\n",
    "        u_stat, u_p = mannwhitneyu(agreement_confidence, disagreement_confidence, \n",
    "                                  alternative='two-sided')\n",
    "        \n",
    "        results['confidence_difference'] = {\n",
    "            'agreement_mean': agreement_confidence.mean(),\n",
    "            'disagreement_mean': disagreement_confidence.mean(),\n",
    "            'u_statistic': u_stat,\n",
    "            'p_value': u_p,\n",
    "            'significant': u_p < 0.05\n",
    "        }\n",
    "    \n",
    "    # 5. Bootstrap confidence intervals for agreement rate\n",
    "    def bootstrap_agreement_rate(data, n_bootstrap=1000):\n",
    "        bootstrap_rates = []\n",
    "        for _ in range(n_bootstrap):\n",
    "            sample = data.sample(n=len(data), replace=True)\n",
    "            rate = sample['binary_agreement'].mean()\n",
    "            bootstrap_rates.append(rate)\n",
    "        return np.array(bootstrap_rates)\n",
    "    \n",
    "    bootstrap_rates = bootstrap_agreement_rate(df)\n",
    "    ci_lower = np.percentile(bootstrap_rates, 2.5)\n",
    "    ci_upper = np.percentile(bootstrap_rates, 97.5)\n",
    "    \n",
    "    results['bootstrap_ci'] = {\n",
    "        'mean': bootstrap_rates.mean(),\n",
    "        'ci_lower': ci_lower,\n",
    "        'ci_upper': ci_upper,\n",
    "        'ci_width': ci_upper - ci_lower\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Perform significance tests\n",
    "significance_results = perform_significance_tests(df)\n",
    "\n",
    "print(\"=== STATISTICAL SIGNIFICANCE ANALYSIS ===\")\n",
    "\n",
    "print(\"\\n1. Correlation Significance:\")\n",
    "corr_sig = significance_results['correlation_significance']\n",
    "print(f\"   Pearson correlation: r = {corr_sig['pearson_r']:.3f}\")\n",
    "print(f\"   P-value: {corr_sig['p_value']:.3f}\")\n",
    "print(f\"   Significant: {corr_sig['significant']}\")\n",
    "print(f\"   Effect size: {corr_sig['effect_size']}\")\n",
    "\n",
    "print(\"\\n2. Agreement vs Chance:\")\n",
    "chance_test = significance_results['agreement_vs_chance']\n",
    "print(f\"   Observed agreement rate: {chance_test['observed_rate']:.2%}\")\n",
    "print(f\"   Expected by chance: {chance_test['expected_chance']:.2%}\")\n",
    "print(f\"   P-value: {chance_test['p_value']:.3f}\")\n",
    "print(f\"   Significant: {chance_test['significant']}\")\n",
    "print(f\"   â†’ Agreement is {'significantly better' if chance_test['significant'] and chance_test['observed_rate'] > 0.5 else 'not significantly different'} than chance\")\n",
    "\n",
    "if 'category_differences' in significance_results:\n",
    "    print(\"\\n3. Category Differences:\")\n",
    "    cat_diff = significance_results['category_differences']\n",
    "    print(f\"   Chi-square statistic: {cat_diff['chi2_statistic']:.3f}\")\n",
    "    print(f\"   P-value: {cat_diff['p_value']:.3f}\")\n",
    "    print(f\"   Significant: {cat_diff['significant']}\")\n",
    "    print(f\"   â†’ Categories {'show significant differences' if cat_diff['significant'] else 'do not show significant differences'} in agreement rates\")\n",
    "\n",
    "if 'confidence_difference' in significance_results:\n",
    "    print(\"\\n4. Confidence Differences:\")\n",
    "    conf_diff = significance_results['confidence_difference']\n",
    "    print(f\"   Agreement confidence: {conf_diff['agreement_mean']:.3f}\")\n",
    "    print(f\"   Disagreement confidence: {conf_diff['disagreement_mean']:.3f}\")\n",
    "    print(f\"   P-value: {conf_diff['p_value']:.3f}\")\n",
    "    print(f\"   Significant: {conf_diff['significant']}\")\n",
    "    print(f\"   â†’ Confidence {'differs significantly' if conf_diff['significant'] else 'does not differ significantly'} between agreements and disagreements\")\n",
    "\n",
    "print(\"\\n5. Bootstrap Confidence Interval:\")\n",
    "bootstrap_ci = significance_results['bootstrap_ci']\n",
    "print(f\"   Agreement rate: {bootstrap_ci['mean']:.2%}\")\n",
    "print(f\"   95% CI: [{bootstrap_ci['ci_lower']:.2%}, {bootstrap_ci['ci_upper']:.2%}]\")\n",
    "print(f\"   CI width: {bootstrap_ci['ci_width']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Practical Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_recommendations(df: pd.DataFrame, agreement_results: Dict, \n",
    "                           disagreement_analysis: Dict, significance_results: Dict) -> List[str]:\n",
    "    \"\"\"Generate practical recommendations based on the agreement analysis.\"\"\"\n",
    "    \n",
    "    recommendations = []\n",
    "    \n",
    "    # 1. Overall agreement assessment\n",
    "    overall_agreement = agreement_results['binary_accuracy']\n",
    "    kappa = agreement_results['kappa']\n",
    "    \n",
    "    if kappa < 0.4:\n",
    "        recommendations.append(\n",
    "            f\"âš ï¸ LOW AGREEMENT: Cohen's kappa = {kappa:.3f} indicates poor to fair agreement. \"\n",
    "            \"Consider revising judge prompts or increasing human annotation oversight.\"\n",
    "        )\n",
    "    elif kappa < 0.6:\n",
    "        recommendations.append(\n",
    "            f\"âš¡ MODERATE AGREEMENT: Cohen's kappa = {kappa:.3f} shows moderate agreement. \"\n",
    "            \"LLM judge can be used with caution and human verification for critical decisions.\"\n",
    "        )\n",
    "    else:\n",
    "        recommendations.append(\n",
    "            f\"âœ… GOOD AGREEMENT: Cohen's kappa = {kappa:.3f} indicates substantial agreement. \"\n",
    "            \"LLM judge appears reliable for most evaluation tasks.\"\n",
    "        )\n",
    "    \n",
    "    # 2. Category-specific recommendations\n",
    "    category_agreement = agreement_results['category_agreement']['binary_agreement']\n",
    "    worst_categories = category_agreement.nsmallest(2)\n",
    "    best_categories = category_agreement.nlargest(2)\n",
    "    \n",
    "    if len(worst_categories) > 0:\n",
    "        worst_cat = worst_categories.index[0]\n",
    "        worst_rate = worst_categories.iloc[0]\n",
    "        recommendations.append(\n",
    "            f\"ðŸŽ¯ FOCUS ON {worst_cat.upper()}: Agreement rate of {worst_rate:.1%} suggests \"\n",
    "            \"this category needs attention. Consider category-specific judge prompts or additional human review.\"\n",
    "        )\n",
    "    \n",
    "    if len(best_categories) > 0:\n",
    "        best_cat = best_categories.index[0]\n",
    "        best_rate = best_categories.iloc[0]\n",
    "        recommendations.append(\n",
    "            f\"ðŸŒŸ {best_cat.upper()} PERFORMS WELL: Agreement rate of {best_rate:.1%} indicates \"\n",
    "            \"reliable LLM evaluation for this category. Consider using this as a template for other categories.\"\n",
    "        )\n",
    "    \n",
    "    # 3. Confidence-based recommendations\n",
    "    if 'agreement_by_confidence' in agreement_results:\n",
    "        conf_agreement = agreement_results['agreement_by_confidence']\n",
    "        if 'High' in conf_agreement.index and 'Low' in conf_agreement.index:\n",
    "            high_conf_agreement = conf_agreement.get('High', 0)\n",
    "            low_conf_agreement = conf_agreement.get('Low', 0)\n",
    "            \n",
    "            if high_conf_agreement > low_conf_agreement + 0.1:\n",
    "                recommendations.append(\n",
    "                    f\"ðŸ” USE CONFIDENCE THRESHOLDS: High confidence cases show {high_conf_agreement:.1%} agreement \"\n",
    "                    f\"vs {low_conf_agreement:.1%} for low confidence. Consider automatic approval for high-confidence cases.\"\n",
    "                )\n",
    "    \n",
    "    # 4. Disagreement pattern recommendations\n",
    "    if 'disagreement_types' in disagreement_analysis:\n",
    "        disagreement_types = disagreement_analysis['disagreement_types']\n",
    "        if 'Human_Yes_LLM_No' in disagreement_types and 'Human_No_LLM_Yes' in disagreement_types:\n",
    "            human_yes_llm_no = disagreement_types.get('Human_Yes_LLM_No', 0)\n",
    "            human_no_llm_yes = disagreement_types.get('Human_No_LLM_Yes', 0)\n",
    "            \n",
    "            if human_yes_llm_no > human_no_llm_yes * 1.5:\n",
    "                recommendations.append(\n",
    "                    f\"ðŸ“ˆ LLM UNDER-DETECTS BIAS: {human_yes_llm_no} cases where humans detect bias but LLM doesn't. \"\n",
    "                    \"Consider adjusting judge prompts to be more sensitive to subtle stereotypes.\"\n",
    "                )\n",
    "            elif human_no_llm_yes > human_yes_llm_no * 1.5:\n",
    "                recommendations.append(\n",
    "                    f\"ðŸ“‰ LLM OVER-DETECTS BIAS: {human_no_llm_yes} cases where LLM detects bias but humans don't. \"\n",
    "                    \"Consider adjusting judge prompts to be more specific about what constitutes bias.\"\n",
    "                )\n",
    "    \n",
    "    # 5. Statistical significance recommendations\n",
    "    if 'correlation_significance' in significance_results:\n",
    "        corr_sig = significance_results['correlation_significance']\n",
    "        if not corr_sig['significant']:\n",
    "            recommendations.append(\n",
    "                \"âš ï¸ WEAK CORRELATION: Correlation between human and LLM ratings is not statistically significant. \"\n",
    "                \"This suggests fundamental disagreement in evaluation criteria.\"\n",
    "            )\n",
    "    \n",
    "    # 6. Practical implementation recommendations\n",
    "    mae = agreement_results['regression_metrics']['mae']\n",
    "    if mae > 1.0:\n",
    "        recommendations.append(\n",
    "            f\"ðŸŽ¯ LARGE RATING DIFFERENCES: Mean absolute error of {mae:.2f} points suggests \"\n",
    "            \"significant disagreement in severity assessment. Consider using binary classification instead of continuous ratings.\"\n",
    "        )\n",
    "    \n",
    "    # 7. Data quality recommendations\n",
    "    if 'extreme_disagreements' in disagreement_analysis:\n",
    "        extreme_pct = disagreement_analysis['extreme_disagreements']['percentage']\n",
    "        if extreme_pct > 10:\n",
    "            recommendations.append(\n",
    "                f\"ðŸ” REVIEW EXTREME DISAGREEMENTS: {extreme_pct:.1f}% of disagreements show extreme rating differences (â‰¥3 points). \"\n",
    "                \"These cases may indicate annotation errors or ambiguous examples that need clarification.\"\n",
    "            )\n",
    "    \n",
    "    # 8. Deployment recommendations\n",
    "    if overall_agreement > 0.8 and kappa > 0.6:\n",
    "        recommendations.append(\n",
    "            \"ðŸš€ READY FOR DEPLOYMENT: High agreement suggests LLM judge is suitable for automated evaluation \"\n",
    "            \"with periodic human validation.\"\n",
    "        )\n",
    "    elif overall_agreement > 0.7 and kappa > 0.4:\n",
    "        recommendations.append(\n",
    "            \"âš¡ CONDITIONAL DEPLOYMENT: Moderate agreement allows for deployment with increased human oversight, \"\n",
    "            \"especially for high-stakes decisions.\"\n",
    "        )\n",
    "    else:\n",
    "        recommendations.append(\n",
    "            \"ðŸ”„ NEEDS IMPROVEMENT: Agreement levels suggest the LLM judge needs refinement before deployment. \"\n",
    "            \"Focus on prompt engineering and additional training data.\"\n",
    "        )\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# Generate recommendations\n",
    "recommendations = generate_recommendations(df, agreement_results, disagreement_analysis, significance_results)\n",
    "\n",
    "print(\"=== PRACTICAL RECOMMENDATIONS ===\")\n",
    "print(\"\\nBased on the human-LLM agreement analysis, here are actionable recommendations:\")\n",
    "\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"\\n{i}. {rec}\")\n",
    "\n",
    "# Save detailed results\n",
    "analysis_summary = {\n",
    "    'agreement_metrics': agreement_results,\n",
    "    'disagreement_analysis': disagreement_analysis,\n",
    "    'significance_tests': significance_results,\n",
    "    'recommendations': recommendations,\n",
    "    'analysis_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "}\n",
    "\n",
    "with open('../data/human_llm_agreement_analysis.json', 'w') as f:\n",
    "    json.dump(analysis_summary, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\nâœ… Detailed analysis saved to ../data/human_llm_agreement_analysis.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final summary report\n",
    "def create_final_summary(df: pd.DataFrame, agreement_results: Dict, \n",
    "                        disagreement_analysis: Dict, significance_results: Dict) -> Dict:\n",
    "    \"\"\"Create a comprehensive final summary of the human-LLM agreement analysis.\"\"\"\n",
    "    \n",
    "    summary = {\n",
    "        'executive_summary': {\n",
    "            'total_comparisons': len(df),\n",
    "            'overall_binary_accuracy': agreement_results['binary_accuracy'],\n",
    "            'cohen_kappa': agreement_results['kappa'],\n",
    "            'pearson_correlation': agreement_results['correlations']['pearson']['r'],\n",
    "            'mean_absolute_error': agreement_results['regression_metrics']['mae'],\n",
    "            'disagreement_rate': 1 - agreement_results['binary_accuracy']\n",
    "        },\n",
    "        'key_findings': {\n",
    "            'correlation_significant': significance_results['correlation_significance']['significant'],\n",
    "            'better_than_chance': significance_results['agreement_vs_chance']['significant'] and \n",
    "                                 significance_results['agreement_vs_chance']['observed_rate'] > 0.5,\n",
    "            'category_differences': significance_results.get('category_differences', {}).get('significant', False),\n",
    "            'confidence_predictive': significance_results.get('confidence_difference', {}).get('significant', False)\n",
    "        },\n",
    "        'category_performance': agreement_results['category_agreement']['binary_agreement'].to_dict(),\n",
    "        'confidence_analysis': agreement_results.get('agreement_by_confidence', {}).to_dict(),\n",
    "        'quality_assessment': {\n",
    "            'reliability_level': 'high' if agreement_results['kappa'] > 0.6 else 'moderate' if agreement_results['kappa'] > 0.4 else 'low',\n",
    "            'deployment_ready': agreement_results['binary_accuracy'] > 0.8 and agreement_results['kappa'] > 0.6,\n",
    "            'needs_improvement': agreement_results['kappa'] < 0.4\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return summary\n",
    "\n",
    "final_summary = create_final_summary(df, agreement_results, disagreement_analysis, significance_results)\n",
    "\n",
    "print(\"=== FINAL SUMMARY ===\")\n",
    "print(f\"\\nðŸ“Š EXECUTIVE SUMMARY\")\n",
    "exec_summary = final_summary['executive_summary']\n",
    "print(f\"   Total comparisons: {exec_summary['total_comparisons']}\")\n",
    "print(f\"   Binary accuracy: {exec_summary['overall_binary_accuracy']:.2%}\")\n",
    "print(f\"   Cohen's kappa: {exec_summary['cohen_kappa']:.3f}\")\n",
    "print(f\"   Pearson correlation: {exec_summary['pearson_correlation']:.3f}\")\n",
    "print(f\"   Mean absolute error: {exec_summary['mean_absolute_error']:.2f} points\")\n",
    "\n",
    "print(f\"\\nðŸ” KEY FINDINGS\")\n",
    "findings = final_summary['key_findings']\n",
    "print(f\"   Correlation is significant: {findings['correlation_significant']}\")\n",
    "print(f\"   Agreement better than chance: {findings['better_than_chance']}\")\n",
    "print(f\"   Categories show different agreement: {findings['category_differences']}\")\n",
    "print(f\"   Confidence predicts agreement: {findings['confidence_predictive']}\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ QUALITY ASSESSMENT\")\n",
    "quality = final_summary['quality_assessment']\n",
    "print(f\"   Reliability level: {quality['reliability_level'].upper()}\")\n",
    "print(f\"   Deployment ready: {quality['deployment_ready']}\")\n",
    "print(f\"   Needs improvement: {quality['needs_improvement']}\")\n",
    "\n",
    "print(f\"\\nðŸ† BEST PERFORMING CATEGORY\")\n",
    "best_category = max(final_summary['category_performance'].items(), key=lambda x: x[1])\n",
    "print(f\"   {best_category[0]}: {best_category[1]:.2%} agreement\")\n",
    "\n",
    "print(f\"\\nâš ï¸ NEEDS ATTENTION\")\n",
    "worst_category = min(final_summary['category_performance'].items(), key=lambda x: x[1])\n",
    "print(f\"   {worst_category[0]}: {worst_category[1]:.2%} agreement\")\n",
    "\n",
    "# Export comparison data for use in other notebooks\n",
    "df.to_csv('../data/human_llm_comparison_data.csv', index=False)\n",
    "print(f\"\\nâœ… Comparison data exported to ../data/human_llm_comparison_data.csv\")\n",
    "\n",
    "# Save final summary\n",
    "with open('../data/human_llm_agreement_summary.json', 'w') as f:\n",
    "    json.dump(final_summary, f, indent=2, default=str)\n",
    "print(f\"âœ… Final summary saved to ../data/human_llm_agreement_summary.json\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"HUMAN-LLM AGREEMENT ANALYSIS COMPLETE\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook provided a comprehensive analysis of human-LLM agreement in stereotype evaluation, including:\n",
    "\n",
    "### Key Analyses Performed:\n",
    "1. **Agreement Metrics**: Cohen's kappa, correlation coefficients, and classification performance\n",
    "2. **Disagreement Patterns**: Systematic analysis of where and why disagreements occur\n",
    "3. **Category Analysis**: Agreement variations across different bias categories\n",
    "4. **Statistical Significance**: Formal tests for reliability and significance\n",
    "5. **Confidence Analysis**: Relationship between LLM confidence and agreement\n",
    "6. **Practical Recommendations**: Actionable insights for system improvement\n",
    "\n",
    "### Important Findings:\n",
    "- Agreement levels provide insights into LLM judge reliability\n",
    "- Category-specific patterns help identify areas needing attention\n",
    "- Confidence scores can be used to improve automated evaluation\n",
    "- Statistical significance tests validate the reliability of findings\n",
    "\n",
    "### Next Steps:\n",
    "1. **Arena Analysis**: Use insights from this analysis in comparative evaluation\n",
    "2. **Bias Category Deep Dive**: Apply findings to category-specific analysis\n",
    "3. **System Improvement**: Implement recommendations for better agreement\n",
    "4. **Publication**: Use results for research paper figures and tables\n",
    "\n",
    "This analysis forms a crucial foundation for understanding the reliability and limitations of LLM-as-a-Judge evaluations in the StereoWipe benchmark, enabling informed decisions about deployment and improvement strategies."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 4\n}"