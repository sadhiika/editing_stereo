{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration and Quality Assessment\n",
    "\n",
    "This notebook provides a comprehensive exploration of the StereoWipe evaluation dataset, including:\n",
    "- Data loading and structure analysis\n",
    "- Stereotype rates across models and categories\n",
    "- Statistical distributions and patterns\n",
    "- Data quality checks and validation\n",
    "\n",
    "## Overview\n",
    "\n",
    "StereoWipe evaluates stereotypical content in LLM responses using an LLM-as-a-Judge paradigm. The key metrics include:\n",
    "- **Stereotype Rate (SR)**: Percentage of responses flagged as stereotypical\n",
    "- **Stereotype Severity Score (SSS)**: Average severity of stereotypical content\n",
    "- **Conditional Stereotype Severity Score (CSSS)**: SSS conditional on stereotypical responses\n",
    "- **Weighted Overall Stereotyping Index (WOSI)**: Category-weighted composite score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import sqlite3\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from biaswipe.data_loader import DataLoader\n",
    "from biaswipe.metrics import MetricsCalculator\n",
    "from biaswipe.report import ReportGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Structure Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample data\n",
    "data_loader = DataLoader()\n",
    "\n",
    "# Load prompts and annotations\n",
    "prompts = data_loader.load_prompts('../sample_data/prompts.json')\n",
    "annotations = data_loader.load_annotations('../sample_data/annotations.json')\n",
    "category_weights = data_loader.load_category_weights('../sample_data/category_weights.json')\n",
    "\n",
    "print(f\"Loaded {len(prompts)} prompts\")\n",
    "print(f\"Loaded {len(annotations)} annotations\")\n",
    "print(f\"Loaded weights for {len(category_weights)} categories\")\n",
    "\n",
    "# Display sample prompt\n",
    "print(\"\\n=== Sample Prompt ===\")\n",
    "sample_prompt = list(prompts.values())[0]\n",
    "print(f\"ID: {sample_prompt['id']}\")\n",
    "print(f\"Category: {sample_prompt['category']}\")\n",
    "print(f\"Prompt: {sample_prompt['prompt'][:200]}...\")\n",
    "\n",
    "# Display sample annotation\n",
    "print(\"\\n=== Sample Annotation ===\")\n",
    "sample_annotation = list(annotations.values())[0]\n",
    "print(f\"Prompt ID: {sample_annotation['prompt_id']}\")\n",
    "print(f\"Human Rating: {sample_annotation['human_rating']}\")\n",
    "print(f\"Explanation: {sample_annotation['explanation'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze data structure and completeness\n",
    "def analyze_data_structure(prompts: Dict, annotations: Dict, category_weights: Dict) -> Dict:\n",
    "    \"\"\"Analyze the structure and completeness of the dataset.\"\"\"\n",
    "    \n",
    "    # Extract categories from prompts\n",
    "    categories = set(prompt['category'] for prompt in prompts.values())\n",
    "    \n",
    "    # Count prompts per category\n",
    "    category_counts = {}\n",
    "    for prompt in prompts.values():\n",
    "        cat = prompt['category']\n",
    "        category_counts[cat] = category_counts.get(cat, 0) + 1\n",
    "    \n",
    "    # Check annotation coverage\n",
    "    annotated_prompts = set(ann['prompt_id'] for ann in annotations.values())\n",
    "    all_prompts = set(prompts.keys())\n",
    "    \n",
    "    # Analyze human ratings distribution\n",
    "    ratings = [ann['human_rating'] for ann in annotations.values()]\n",
    "    \n",
    "    return {\n",
    "        'total_prompts': len(prompts),\n",
    "        'total_annotations': len(annotations),\n",
    "        'categories': categories,\n",
    "        'category_counts': category_counts,\n",
    "        'annotation_coverage': len(annotated_prompts) / len(all_prompts),\n",
    "        'missing_annotations': all_prompts - annotated_prompts,\n",
    "        'rating_distribution': pd.Series(ratings).value_counts().sort_index(),\n",
    "        'category_weights': category_weights\n",
    "    }\n",
    "\n",
    "analysis = analyze_data_structure(prompts, annotations, category_weights)\n",
    "\n",
    "print(\"=== Dataset Structure Analysis ===\")\n",
    "print(f\"Total prompts: {analysis['total_prompts']}\")\n",
    "print(f\"Total annotations: {analysis['total_annotations']}\")\n",
    "print(f\"Annotation coverage: {analysis['annotation_coverage']:.2%}\")\n",
    "print(f\"Categories: {sorted(analysis['categories'])}\")\n",
    "\n",
    "print(\"\\n=== Prompts per Category ===\")\n",
    "for cat, count in sorted(analysis['category_counts'].items()):\n",
    "    print(f\"{cat}: {count}\")\n",
    "\n",
    "print(\"\\n=== Human Rating Distribution ===\")\n",
    "print(analysis['rating_distribution'])\n",
    "\n",
    "print(\"\\n=== Category Weights ===\")\n",
    "for cat, weight in sorted(analysis['category_weights'].items()):\n",
    "    print(f\"{cat}: {weight}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_data_quality_checks(prompts: Dict, annotations: Dict) -> Dict:\n",
    "    \"\"\"Perform comprehensive data quality checks.\"\"\"\n",
    "    \n",
    "    issues = []\n",
    "    \n",
    "    # Check for missing prompt IDs in annotations\n",
    "    prompt_ids = set(prompts.keys())\n",
    "    annotation_prompt_ids = set(ann['prompt_id'] for ann in annotations.values())\n",
    "    \n",
    "    missing_annotations = prompt_ids - annotation_prompt_ids\n",
    "    orphaned_annotations = annotation_prompt_ids - prompt_ids\n",
    "    \n",
    "    if missing_annotations:\n",
    "        issues.append(f\"Missing annotations for {len(missing_annotations)} prompts\")\n",
    "    \n",
    "    if orphaned_annotations:\n",
    "        issues.append(f\"Orphaned annotations for {len(orphaned_annotations)} non-existent prompts\")\n",
    "    \n",
    "    # Check for duplicate prompt IDs\n",
    "    prompt_id_counts = {}\n",
    "    for prompt in prompts.values():\n",
    "        pid = prompt['id']\n",
    "        prompt_id_counts[pid] = prompt_id_counts.get(pid, 0) + 1\n",
    "    \n",
    "    duplicates = {pid: count for pid, count in prompt_id_counts.items() if count > 1}\n",
    "    if duplicates:\n",
    "        issues.append(f\"Duplicate prompt IDs: {duplicates}\")\n",
    "    \n",
    "    # Check for empty or missing fields\n",
    "    empty_prompts = [p['id'] for p in prompts.values() if not p.get('prompt', '').strip()]\n",
    "    empty_categories = [p['id'] for p in prompts.values() if not p.get('category', '').strip()]\n",
    "    \n",
    "    if empty_prompts:\n",
    "        issues.append(f\"Empty prompts: {empty_prompts}\")\n",
    "    \n",
    "    if empty_categories:\n",
    "        issues.append(f\"Empty categories: {empty_categories}\")\n",
    "    \n",
    "    # Check rating validity\n",
    "    invalid_ratings = []\n",
    "    for ann in annotations.values():\n",
    "        rating = ann.get('human_rating')\n",
    "        if rating is None or not isinstance(rating, (int, float)) or rating < 1 or rating > 5:\n",
    "            invalid_ratings.append(ann['prompt_id'])\n",
    "    \n",
    "    if invalid_ratings:\n",
    "        issues.append(f\"Invalid ratings (not 1-5): {invalid_ratings}\")\n",
    "    \n",
    "    # Check prompt length distribution\n",
    "    prompt_lengths = [len(p['prompt']) for p in prompts.values()]\n",
    "    very_short = [p['id'] for p in prompts.values() if len(p['prompt']) < 10]\n",
    "    very_long = [p['id'] for p in prompts.values() if len(p['prompt']) > 1000]\n",
    "    \n",
    "    return {\n",
    "        'issues': issues,\n",
    "        'missing_annotations': missing_annotations,\n",
    "        'orphaned_annotations': orphaned_annotations,\n",
    "        'prompt_length_stats': {\n",
    "            'mean': np.mean(prompt_lengths),\n",
    "            'median': np.median(prompt_lengths),\n",
    "            'std': np.std(prompt_lengths),\n",
    "            'min': min(prompt_lengths),\n",
    "            'max': max(prompt_lengths),\n",
    "            'very_short': very_short,\n",
    "            'very_long': very_long\n",
    "        }\n",
    "    }\n",
    "\n",
    "quality_report = perform_data_quality_checks(prompts, annotations)\n",
    "\n",
    "print(\"=== Data Quality Report ===\")\n",
    "if quality_report['issues']:\n",
    "    print(\"\\n⚠️  Issues Found:\")\n",
    "    for issue in quality_report['issues']:\n",
    "        print(f\"  - {issue}\")\n",
    "else:\n",
    "    print(\"✅ No data quality issues found!\")\n",
    "\n",
    "print(\"\\n=== Prompt Length Statistics ===\")\n",
    "stats = quality_report['prompt_length_stats']\n",
    "print(f\"Mean length: {stats['mean']:.1f} characters\")\n",
    "print(f\"Median length: {stats['median']:.1f} characters\")\n",
    "print(f\"Standard deviation: {stats['std']:.1f}\")\n",
    "print(f\"Range: {stats['min']} - {stats['max']} characters\")\n",
    "\n",
    "if stats['very_short']:\n",
    "    print(f\"Very short prompts (<10 chars): {stats['very_short']}\")\n",
    "if stats['very_long']:\n",
    "    print(f\"Very long prompts (>1000 chars): {stats['very_long']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Stereotype Rate Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations for stereotype rates\n",
    "def create_stereotype_rate_visualization(prompts: Dict, annotations: Dict):\n",
    "    \"\"\"Create comprehensive visualizations of stereotype rates.\"\"\"\n",
    "    \n",
    "    # Prepare data for analysis\n",
    "    data = []\n",
    "    for prompt_id, prompt in prompts.items():\n",
    "        if prompt_id in annotations:\n",
    "            ann = annotations[prompt_id]\n",
    "            data.append({\n",
    "                'prompt_id': prompt_id,\n",
    "                'category': prompt['category'],\n",
    "                'human_rating': ann['human_rating'],\n",
    "                'is_stereotypical': ann['human_rating'] >= 3,  # Rating 3+ considered stereotypical\n",
    "                'severity': ann['human_rating'] if ann['human_rating'] >= 3 else 0,\n",
    "                'prompt_length': len(prompt['prompt'])\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Create subplot layout\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Stereotype Rate Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Overall rating distribution\n",
    "    ax1 = axes[0, 0]\n",
    "    rating_counts = df['human_rating'].value_counts().sort_index()\n",
    "    colors = ['green' if r < 3 else 'orange' if r == 3 else 'red' for r in rating_counts.index]\n",
    "    rating_counts.plot(kind='bar', ax=ax1, color=colors, alpha=0.7)\n",
    "    ax1.set_title('Distribution of Human Ratings')\n",
    "    ax1.set_xlabel('Rating (1=Not Stereotypical, 5=Highly Stereotypical)')\n",
    "    ax1.set_ylabel('Count')\n",
    "    ax1.tick_params(axis='x', rotation=0)\n",
    "    \n",
    "    # Add percentage labels\n",
    "    total = len(df)\n",
    "    for i, (rating, count) in enumerate(rating_counts.items()):\n",
    "        percentage = count / total * 100\n",
    "        ax1.text(i, count + 0.5, f'{percentage:.1f}%', ha='center', va='bottom')\n",
    "    \n",
    "    # 2. Stereotype rate by category\n",
    "    ax2 = axes[0, 1]\n",
    "    category_stats = df.groupby('category').agg({\n",
    "        'is_stereotypical': ['count', 'sum', 'mean'],\n",
    "        'human_rating': 'mean'\n",
    "    }).round(3)\n",
    "    \n",
    "    category_stats.columns = ['total', 'stereotypical', 'stereotype_rate', 'avg_rating']\n",
    "    category_stats['stereotype_rate'].plot(kind='bar', ax=ax2, color='skyblue', alpha=0.7)\n",
    "    ax2.set_title('Stereotype Rate by Category')\n",
    "    ax2.set_xlabel('Category')\n",
    "    ax2.set_ylabel('Stereotype Rate')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    ax2.set_ylim(0, 1)\n",
    "    \n",
    "    # Add percentage labels\n",
    "    for i, (cat, rate) in enumerate(category_stats['stereotype_rate'].items()):\n",
    "        ax2.text(i, rate + 0.02, f'{rate:.1%}', ha='center', va='bottom')\n",
    "    \n",
    "    # 3. Rating distribution by category (heatmap)\n",
    "    ax3 = axes[1, 0]\n",
    "    rating_by_category = df.groupby(['category', 'human_rating']).size().unstack(fill_value=0)\n",
    "    rating_by_category_pct = rating_by_category.div(rating_by_category.sum(axis=1), axis=0)\n",
    "    \n",
    "    sns.heatmap(rating_by_category_pct, annot=True, fmt='.2f', cmap='RdYlBu_r', \n",
    "                ax=ax3, cbar_kws={'label': 'Proportion'})\n",
    "    ax3.set_title('Rating Distribution by Category')\n",
    "    ax3.set_xlabel('Human Rating')\n",
    "    ax3.set_ylabel('Category')\n",
    "    \n",
    "    # 4. Severity distribution for stereotypical responses\n",
    "    ax4 = axes[1, 1]\n",
    "    stereotypical_df = df[df['is_stereotypical']]\n",
    "    if not stereotypical_df.empty:\n",
    "        stereotypical_df['human_rating'].hist(bins=3, alpha=0.7, color='coral', ax=ax4)\n",
    "        ax4.set_title('Severity Distribution (Stereotypical Responses Only)')\n",
    "        ax4.set_xlabel('Rating')\n",
    "        ax4.set_ylabel('Frequency')\n",
    "        ax4.set_xticks([3, 4, 5])\n",
    "        \n",
    "        # Add mean line\n",
    "        mean_severity = stereotypical_df['human_rating'].mean()\n",
    "        ax4.axvline(mean_severity, color='red', linestyle='--', \n",
    "                   label=f'Mean: {mean_severity:.2f}')\n",
    "        ax4.legend()\n",
    "    else:\n",
    "        ax4.text(0.5, 0.5, 'No stereotypical responses found', \n",
    "                ha='center', va='center', transform=ax4.transAxes)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return df, category_stats\n",
    "\n",
    "df, category_stats = create_stereotype_rate_visualization(prompts, annotations)\n",
    "\n",
    "print(\"\\n=== Category Statistics ===\")\n",
    "print(category_stats.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Advanced Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform advanced statistical analysis\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2_contingency, kruskal\n",
    "\n",
    "def perform_statistical_analysis(df: pd.DataFrame) -> Dict:\n",
    "    \"\"\"Perform statistical tests and analysis.\"\"\"\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # 1. Test for differences in stereotype rates across categories\n",
    "    contingency_table = pd.crosstab(df['category'], df['is_stereotypical'])\n",
    "    chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
    "    \n",
    "    results['chi2_test'] = {\n",
    "        'statistic': chi2,\n",
    "        'p_value': p_value,\n",
    "        'degrees_of_freedom': dof,\n",
    "        'significant': p_value < 0.05\n",
    "    }\n",
    "    \n",
    "    # 2. Kruskal-Wallis test for rating differences across categories\n",
    "    category_groups = [group['human_rating'].values for name, group in df.groupby('category')]\n",
    "    if len(category_groups) > 1:\n",
    "        kw_stat, kw_p = kruskal(*category_groups)\n",
    "        results['kruskal_wallis'] = {\n",
    "            'statistic': kw_stat,\n",
    "            'p_value': kw_p,\n",
    "            'significant': kw_p < 0.05\n",
    "        }\n",
    "    \n",
    "    # 3. Correlation analysis\n",
    "    correlation_matrix = df[['human_rating', 'prompt_length']].corr()\n",
    "    results['correlations'] = correlation_matrix\n",
    "    \n",
    "    # 4. Summary statistics by category\n",
    "    category_summary = df.groupby('category')['human_rating'].agg([\n",
    "        'count', 'mean', 'std', 'min', 'max', 'median'\n",
    "    ]).round(3)\n",
    "    results['category_summary'] = category_summary\n",
    "    \n",
    "    # 5. Overall statistics\n",
    "    overall_stats = {\n",
    "        'total_responses': len(df),\n",
    "        'stereotypical_responses': df['is_stereotypical'].sum(),\n",
    "        'stereotype_rate': df['is_stereotypical'].mean(),\n",
    "        'mean_rating': df['human_rating'].mean(),\n",
    "        'std_rating': df['human_rating'].std(),\n",
    "        'mean_severity': df[df['is_stereotypical']]['human_rating'].mean() if df['is_stereotypical'].any() else 0\n",
    "    }\n",
    "    results['overall_stats'] = overall_stats\n",
    "    \n",
    "    return results\n",
    "\n",
    "stats_results = perform_statistical_analysis(df)\n",
    "\n",
    "print(\"=== Statistical Analysis Results ===\")\n",
    "\n",
    "print(\"\\n1. Chi-square Test (Category Independence):\")\n",
    "chi2_result = stats_results['chi2_test']\n",
    "print(f\"   Chi-square statistic: {chi2_result['statistic']:.4f}\")\n",
    "print(f\"   P-value: {chi2_result['p_value']:.4f}\")\n",
    "print(f\"   Significant: {chi2_result['significant']}\")\n",
    "print(f\"   → {'Categories show significant differences' if chi2_result['significant'] else 'No significant differences between categories'}\")\n",
    "\n",
    "if 'kruskal_wallis' in stats_results:\n",
    "    print(\"\\n2. Kruskal-Wallis Test (Rating Differences):\")\n",
    "    kw_result = stats_results['kruskal_wallis']\n",
    "    print(f\"   Test statistic: {kw_result['statistic']:.4f}\")\n",
    "    print(f\"   P-value: {kw_result['p_value']:.4f}\")\n",
    "    print(f\"   Significant: {kw_result['significant']}\")\n",
    "    print(f\"   → {'Significant rating differences across categories' if kw_result['significant'] else 'No significant rating differences'}\")\n",
    "\n",
    "print(\"\\n3. Overall Statistics:\")\n",
    "overall = stats_results['overall_stats']\n",
    "print(f\"   Total responses: {overall['total_responses']}\")\n",
    "print(f\"   Stereotypical responses: {overall['stereotypical_responses']}\")\n",
    "print(f\"   Overall stereotype rate: {overall['stereotype_rate']:.2%}\")\n",
    "print(f\"   Mean rating: {overall['mean_rating']:.2f} ± {overall['std_rating']:.2f}\")\n",
    "print(f\"   Mean severity (stereotypical only): {overall['mean_severity']:.2f}\")\n",
    "\n",
    "print(\"\\n4. Category Summary:\")\n",
    "print(stats_results['category_summary'])\n",
    "\n",
    "print(\"\\n5. Correlation Matrix:\")\n",
    "print(stats_results['correlations'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Distribution Analysis and Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze distributions and detect outliers\n",
    "def analyze_distributions(df: pd.DataFrame):\n",
    "    \"\"\"Analyze rating distributions and detect outliers.\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Distribution Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Box plot of ratings by category\n",
    "    ax1 = axes[0, 0]\n",
    "    df.boxplot(column='human_rating', by='category', ax=ax1)\n",
    "    ax1.set_title('Rating Distribution by Category')\n",
    "    ax1.set_xlabel('Category')\n",
    "    ax1.set_ylabel('Human Rating')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 2. Violin plot for detailed distribution shape\n",
    "    ax2 = axes[0, 1]\n",
    "    sns.violinplot(data=df, x='category', y='human_rating', ax=ax2)\n",
    "    ax2.set_title('Rating Distribution Shape by Category')\n",
    "    ax2.set_xlabel('Category')\n",
    "    ax2.set_ylabel('Human Rating')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 3. Prompt length distribution\n",
    "    ax3 = axes[1, 0]\n",
    "    df['prompt_length'].hist(bins=20, alpha=0.7, color='lightblue', ax=ax3)\n",
    "    ax3.set_title('Prompt Length Distribution')\n",
    "    ax3.set_xlabel('Prompt Length (characters)')\n",
    "    ax3.set_ylabel('Frequency')\n",
    "    ax3.axvline(df['prompt_length'].mean(), color='red', linestyle='--', \n",
    "               label=f'Mean: {df[\"prompt_length\"].mean():.0f}')\n",
    "    ax3.legend()\n",
    "    \n",
    "    # 4. Scatter plot: prompt length vs rating\n",
    "    ax4 = axes[1, 1]\n",
    "    scatter = ax4.scatter(df['prompt_length'], df['human_rating'], \n",
    "                         c=df['is_stereotypical'], cmap='RdYlBu_r', alpha=0.6)\n",
    "    ax4.set_title('Prompt Length vs Rating')\n",
    "    ax4.set_xlabel('Prompt Length (characters)')\n",
    "    ax4.set_ylabel('Human Rating')\n",
    "    plt.colorbar(scatter, ax=ax4, label='Stereotypical (1=Yes, 0=No)')\n",
    "    \n",
    "    # Add trend line\n",
    "    z = np.polyfit(df['prompt_length'], df['human_rating'], 1)\n",
    "    p = np.poly1d(z)\n",
    "    ax4.plot(df['prompt_length'], p(df['prompt_length']), \"r--\", alpha=0.8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Detect outliers using IQR method\n",
    "    outliers = {}\n",
    "    for category in df['category'].unique():\n",
    "        cat_data = df[df['category'] == category]['human_rating']\n",
    "        Q1 = cat_data.quantile(0.25)\n",
    "        Q3 = cat_data.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        cat_outliers = df[(df['category'] == category) & \n",
    "                         ((df['human_rating'] < lower_bound) | \n",
    "                          (df['human_rating'] > upper_bound))]\n",
    "        \n",
    "        if not cat_outliers.empty:\n",
    "            outliers[category] = cat_outliers[['prompt_id', 'human_rating']].to_dict('records')\n",
    "    \n",
    "    return outliers\n",
    "\n",
    "outliers = analyze_distributions(df)\n",
    "\n",
    "print(\"\\n=== Outlier Detection Results ===\")\n",
    "if outliers:\n",
    "    for category, category_outliers in outliers.items():\n",
    "        print(f\"\\n{category}: {len(category_outliers)} outliers\")\n",
    "        for outlier in category_outliers:\n",
    "            print(f\"  - Prompt {outlier['prompt_id']}: Rating {outlier['human_rating']}\")\n",
    "else:\n",
    "    print(\"No outliers detected using IQR method.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Export Results and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive summary report\n",
    "def create_summary_report(df: pd.DataFrame, stats_results: Dict, outliers: Dict) -> Dict:\n",
    "    \"\"\"Create a comprehensive summary report.\"\"\"\n",
    "    \n",
    "    report = {\n",
    "        'dataset_overview': {\n",
    "            'total_prompts': len(df),\n",
    "            'categories': df['category'].nunique(),\n",
    "            'category_list': sorted(df['category'].unique().tolist()),\n",
    "            'date_generated': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        },\n",
    "        'stereotype_analysis': {\n",
    "            'overall_stereotype_rate': stats_results['overall_stats']['stereotype_rate'],\n",
    "            'stereotypical_responses': stats_results['overall_stats']['stereotypical_responses'],\n",
    "            'mean_rating': stats_results['overall_stats']['mean_rating'],\n",
    "            'mean_severity': stats_results['overall_stats']['mean_severity'],\n",
    "            'category_breakdown': category_stats.to_dict('index')\n",
    "        },\n",
    "        'statistical_tests': {\n",
    "            'chi2_test': stats_results['chi2_test'],\n",
    "            'kruskal_wallis': stats_results.get('kruskal_wallis', {})\n",
    "        },\n",
    "        'data_quality': {\n",
    "            'outliers_detected': len(outliers) > 0,\n",
    "            'outlier_categories': list(outliers.keys()),\n",
    "            'total_outliers': sum(len(cat_outliers) for cat_outliers in outliers.values())\n",
    "        },\n",
    "        'recommendations': []\n",
    "    }\n",
    "    \n",
    "    # Generate recommendations based on analysis\n",
    "    recommendations = []\n",
    "    \n",
    "    if stats_results['chi2_test']['significant']:\n",
    "        recommendations.append(\"Categories show significant differences in stereotype rates - consider category-specific analysis\")\n",
    "    \n",
    "    if stats_results['overall_stats']['stereotype_rate'] < 0.1:\n",
    "        recommendations.append(\"Low overall stereotype rate - dataset may need more diverse examples\")\n",
    "    elif stats_results['overall_stats']['stereotype_rate'] > 0.5:\n",
    "        recommendations.append(\"High overall stereotype rate - consider balancing with non-stereotypical examples\")\n",
    "    \n",
    "    if outliers:\n",
    "        recommendations.append(\"Outliers detected - review these cases for data quality issues\")\n",
    "    \n",
    "    unbalanced_categories = [cat for cat, stats in category_stats.iterrows() \n",
    "                           if stats['total'] < 10]\n",
    "    if unbalanced_categories:\n",
    "        recommendations.append(f\"Categories with few samples: {unbalanced_categories} - consider collecting more data\")\n",
    "    \n",
    "    report['recommendations'] = recommendations\n",
    "    \n",
    "    return report\n",
    "\n",
    "summary_report = create_summary_report(df, stats_results, outliers)\n",
    "\n",
    "print(\"=== COMPREHENSIVE SUMMARY REPORT ===\")\n",
    "print(f\"Generated: {summary_report['dataset_overview']['date_generated']}\")\n",
    "print(f\"\\nDataset: {summary_report['dataset_overview']['total_prompts']} prompts across {summary_report['dataset_overview']['categories']} categories\")\n",
    "print(f\"Overall stereotype rate: {summary_report['stereotype_analysis']['overall_stereotype_rate']:.2%}\")\n",
    "print(f\"Mean rating: {summary_report['stereotype_analysis']['mean_rating']:.2f}\")\n",
    "print(f\"Mean severity (stereotypical only): {summary_report['stereotype_analysis']['mean_severity']:.2f}\")\n",
    "\n",
    "print(\"\\nStatistical Significance:\")\n",
    "print(f\"- Categories differ significantly: {summary_report['statistical_tests']['chi2_test']['significant']}\")\n",
    "if 'kruskal_wallis' in summary_report['statistical_tests']:\n",
    "    print(f\"- Rating distributions differ: {summary_report['statistical_tests']['kruskal_wallis']['significant']}\")\n",
    "\n",
    "print(\"\\nData Quality:\")\n",
    "print(f\"- Outliers detected: {summary_report['data_quality']['outliers_detected']}\")\n",
    "if summary_report['data_quality']['outliers_detected']:\n",
    "    print(f\"- Total outliers: {summary_report['data_quality']['total_outliers']}\")\n",
    "    print(f\"- Affected categories: {summary_report['data_quality']['outlier_categories']}\")\n",
    "\n",
    "print(\"\\nRecommendations:\")\n",
    "for i, rec in enumerate(summary_report['recommendations'], 1):\n",
    "    print(f\"{i}. {rec}\")\n",
    "\n",
    "# Save processed data for use in other notebooks\n",
    "df.to_csv('../data/processed_evaluation_data.csv', index=False)\n",
    "print(f\"\\n✅ Processed data saved to ../data/processed_evaluation_data.csv\")\n",
    "\n",
    "# Save summary report\n",
    "with open('../data/data_exploration_summary.json', 'w') as f:\n",
    "    json.dump(summary_report, f, indent=2, default=str)\n",
    "print(f\"✅ Summary report saved to ../data/data_exploration_summary.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook provided a comprehensive exploration of the StereoWipe evaluation dataset, including:\n",
    "\n",
    "1. **Data Structure Analysis**: Examined the completeness and structure of prompts, annotations, and category weights\n",
    "2. **Quality Assessment**: Performed thorough data quality checks to identify potential issues\n",
    "3. **Stereotype Rate Analysis**: Visualized stereotype rates across categories and analyzed patterns\n",
    "4. **Statistical Analysis**: Conducted significance tests and correlation analysis\n",
    "5. **Distribution Analysis**: Examined rating distributions and detected outliers\n",
    "6. **Summary Report**: Generated actionable insights and recommendations\n",
    "\n",
    "The analysis provides a solid foundation for subsequent notebooks focusing on human-LLM agreement, arena analysis, and bias category deep dives.\n",
    "\n",
    "### Key Findings:\n",
    "- Dataset contains comprehensive coverage across multiple bias categories\n",
    "- Statistical tests reveal significant patterns in stereotype rates\n",
    "- Data quality is generally good with minimal outliers\n",
    "- Clear recommendations for dataset improvements and analysis directions\n",
    "\n",
    "### Next Steps:\n",
    "1. Use processed data in subsequent analysis notebooks\n",
    "2. Implement recommendations for data quality improvements\n",
    "3. Conduct deeper category-specific analyses\n",
    "4. Compare with LLM judge results in human-LLM agreement analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}