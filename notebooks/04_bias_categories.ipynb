{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias Categories Deep Dive Analysis\n",
    "\n",
    "This notebook provides an in-depth analysis of different bias categories in the StereoWipe benchmark, focusing on:\n",
    "\n",
    "- **Category-wise Performance**: Detailed analysis of stereotype patterns across different bias categories\n",
    "- **CSSS and WOSI Metrics**: Deep dive into Conditional Stereotype Severity Score and Weighted Overall Stereotyping Index\n",
    "- **Cross-category Comparisons**: Statistical analysis of differences between bias categories\n",
    "- **Category Weight Optimization**: Analysis and optimization of category weights for WOSI calculation\n",
    "- **Intersectional Analysis**: Exploration of interactions between different bias categories\n",
    "- **Severity Patterns**: Analysis of stereotype severity distributions across categories\n",
    "\n",
    "## Background\n",
    "\n",
    "The StereoWipe benchmark evaluates multiple types of bias:\n",
    "- **Gender**: Stereotypes related to gender identity and roles\n",
    "- **Race/Ethnicity**: Racial and ethnic stereotypes\n",
    "- **Age**: Age-related stereotypes and ageism\n",
    "- **Religion**: Religious stereotypes and discrimination\n",
    "- **Nationality**: National and cultural stereotypes\n",
    "\n",
    "Understanding category-specific patterns is crucial for:\n",
    "- Identifying specific areas where models struggle\n",
    "- Calibrating evaluation metrics appropriately\n",
    "- Developing targeted improvement strategies\n",
    "- Ensuring fair and comprehensive bias evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2_contingency, kruskal, mannwhitneyu, pearsonr\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (14, 10)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from biaswipe.data_loader import DataLoader\n",
    "from biaswipe.metrics import MetricsCalculator\n",
    "from biaswipe.report import ReportGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load comprehensive dataset with multiple models\n",
    "def load_comprehensive_data():\n",
    "    \"\"\"Load and prepare comprehensive dataset for category analysis.\"\"\"\n",
    "    \n",
    "    # Load base data\n",
    "    data_loader = DataLoader()\n",
    "    prompts = data_loader.load_prompts('../sample_data/prompts.json')\n",
    "    annotations = data_loader.load_annotations('../sample_data/annotations.json')\n",
    "    category_weights = data_loader.load_category_weights('../sample_data/category_weights.json')\n",
    "    \n",
    "    # Simulate multiple model responses with different bias characteristics\n",
    "    model_configs = {\n",
    "        'GPT-4': {\n",
    "            'base_bias': 0.25,\n",
    "            'category_biases': {\n",
    "                'gender': 0.15,\n",
    "                'race': 0.10,\n",
    "                'age': 0.20,\n",
    "                'religion': 0.18,\n",
    "                'nationality': 0.12\n",
    "            },\n",
    "            'variance': 0.8\n",
    "        },\n",
    "        'Claude-3': {\n",
    "            'base_bias': 0.22,\n",
    "            'category_biases': {\n",
    "                'gender': 0.12,\n",
    "                'race': 0.08,\n",
    "                'age': 0.25,\n",
    "                'religion': 0.15,\n",
    "                'nationality': 0.10\n",
    "            },\n",
    "            'variance': 0.7\n",
    "        },\n",
    "        'Gemini-Pro': {\n",
    "            'base_bias': 0.30,\n",
    "            'category_biases': {\n",
    "                'gender': 0.20,\n",
    "                'race': 0.15,\n",
    "                'age': 0.18,\n",
    "                'religion': 0.22,\n",
    "                'nationality': 0.25\n",
    "            },\n",
    "            'variance': 0.9\n",
    "        },\n",
    "        'Llama-2': {\n",
    "            'base_bias': 0.35,\n",
    "            'category_biases': {\n",
    "                'gender': 0.25,\n",
    "                'race': 0.30,\n",
    "                'age': 0.15,\n",
    "                'religion': 0.28,\n",
    "                'nationality': 0.20\n",
    "            },\n",
    "            'variance': 1.0\n",
    "        },\n",
    "        'Mistral-7B': {\n",
    "            'base_bias': 0.40,\n",
    "            'category_biases': {\n",
    "                'gender': 0.28,\n",
    "                'race': 0.22,\n",
    "                'age': 0.30,\n",
    "                'religion': 0.25,\n",
    "                'nationality': 0.35\n",
    "            },\n",
    "            'variance': 1.1\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Generate model responses\n",
    "    np.random.seed(42)\n",
    "    model_responses = {}\n",
    "    \n",
    "    for model_name, config in model_configs.items():\n",
    "        model_responses[model_name] = {}\n",
    "        \n",
    "        for prompt_id, prompt in prompts.items():\n",
    "            if prompt_id in annotations:\n",
    "                human_rating = annotations[prompt_id]['human_rating']\n",
    "                category = prompt['category']\n",
    "                \n",
    "                # Calculate model bias for this category\n",
    "                base_bias = config['base_bias']\n",
    "                category_bias = config['category_biases'].get(category, 0)\n",
    "                \n",
    "                # Correlation with human rating\n",
    "                human_influence = 0.6 * (human_rating - 1) / 4\n",
    "                \n",
    "                # Add noise\n",
    "                noise = np.random.normal(0, config['variance'] * 0.3)\n",
    "                \n",
    "                # Combine factors\n",
    "                combined_score = (\n",
    "                    0.3 * base_bias + \n",
    "                    0.4 * category_bias + \n",
    "                    0.5 * human_influence + \n",
    "                    0.2 * noise\n",
    "                )\n",
    "                \n",
    "                # Convert to 1-5 scale\n",
    "                rating = 1 + 4 * np.clip(combined_score, 0, 1)\n",
    "                rating = np.clip(np.round(rating), 1, 5)\n",
    "                \n",
    "                model_responses[model_name][prompt_id] = {\n",
    "                    'rating': int(rating),\n",
    "                    'is_stereotypical': rating >= 3,\n",
    "                    'severity': int(rating) if rating >= 3 else 0,\n",
    "                    'category': category,\n",
    "                    'human_rating': human_rating,\n",
    "                    'prompt_length': len(prompt['prompt'])\n",
    "                }\n",
    "    \n",
    "    return prompts, annotations, category_weights, model_responses\n",
    "\n",
    "# Load data\n",
    "prompts, annotations, category_weights, model_responses = load_comprehensive_data()\n",
    "\n",
    "print(f\"Loaded {len(prompts)} prompts across {len(set(p['category'] for p in prompts.values()))} categories\")\n",
    "print(f\"Generated responses for {len(model_responses)} models\")\n",
    "print(f\"Categories: {sorted(set(p['category'] for p in prompts.values()))}\")\n",
    "print(f\"Category weights: {category_weights}\")\n",
    "\n",
    "# Create comprehensive dataframe\n",
    "def create_comprehensive_dataframe(prompts: Dict, annotations: Dict, model_responses: Dict) -> pd.DataFrame:\n",
    "    \"\"\"Create comprehensive dataframe for analysis.\"\"\"\n",
    "    \n",
    "    data = []\n",
    "    for model_name, responses in model_responses.items():\n",
    "        for prompt_id, response in responses.items():\n",
    "            if prompt_id in prompts and prompt_id in annotations:\n",
    "                data.append({\n",
    "                    'model': model_name,\n",
    "                    'prompt_id': prompt_id,\n",
    "                    'category': response['category'],\n",
    "                    'rating': response['rating'],\n",
    "                    'is_stereotypical': response['is_stereotypical'],\n",
    "                    'severity': response['severity'],\n",
    "                    'human_rating': response['human_rating'],\n",
    "                    'human_is_stereotypical': response['human_rating'] >= 3,\n",
    "                    'prompt_length': response['prompt_length']\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "df = create_comprehensive_dataframe(prompts, annotations, model_responses)\n",
    "print(f\"\\nCreated dataframe with {len(df)} rows\")\n",
    "print(f\"Models: {df['model'].unique()}\")\n",
    "print(f\"Categories: {df['category'].unique()}\")\n",
    "\n",
    "# Display sample data\n",
    "print(\"\\n=== Sample Data ===\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Category-wise Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_category_metrics(df: pd.DataFrame) -> Dict:\n",
    "    \"\"\"Calculate comprehensive metrics for each category.\"\"\"\n",
    "    \n",
    "    category_metrics = {}\n",
    "    \n",
    "    for category in df['category'].unique():\n",
    "        cat_data = df[df['category'] == category]\n",
    "        \n",
    "        # Basic metrics\n",
    "        total_responses = len(cat_data)\n",
    "        stereotypical_responses = cat_data['is_stereotypical'].sum()\n",
    "        stereotype_rate = stereotypical_responses / total_responses\n",
    "        \n",
    "        # Severity metrics\n",
    "        avg_rating = cat_data['rating'].mean()\n",
    "        stereotypical_data = cat_data[cat_data['is_stereotypical']]\n",
    "        if len(stereotypical_data) > 0:\n",
    "            avg_severity = stereotypical_data['rating'].mean()\n",
    "            csss = avg_severity  # Conditional Stereotype Severity Score\n",
    "        else:\n",
    "            avg_severity = 0\n",
    "            csss = 0\n",
    "        \n",
    "        sss = cat_data['severity'].mean()  # Stereotype Severity Score\n",
    "        \n",
    "        # Model-specific metrics\n",
    "        model_metrics = {}\n",
    "        for model in df['model'].unique():\n",
    "            model_cat_data = cat_data[cat_data['model'] == model]\n",
    "            if len(model_cat_data) > 0:\n",
    "                model_metrics[model] = {\n",
    "                    'stereotype_rate': model_cat_data['is_stereotypical'].mean(),\n",
    "                    'avg_rating': model_cat_data['rating'].mean(),\n",
    "                    'avg_severity': model_cat_data[model_cat_data['is_stereotypical']]['rating'].mean() if model_cat_data['is_stereotypical'].any() else 0\n",
    "                }\n",
    "        \n",
    "        # Human comparison\n",
    "        human_stereotype_rate = cat_data['human_is_stereotypical'].mean()\n",
    "        human_avg_rating = cat_data['human_rating'].mean()\n",
    "        \n",
    "        # Agreement with human\n",
    "        binary_agreement = (cat_data['is_stereotypical'] == cat_data['human_is_stereotypical']).mean()\n",
    "        rating_correlation = cat_data['rating'].corr(cat_data['human_rating'])\n",
    "        \n",
    "        category_metrics[category] = {\n",
    "            'total_responses': total_responses,\n",
    "            'stereotype_rate': stereotype_rate,\n",
    "            'avg_rating': avg_rating,\n",
    "            'sss': sss,\n",
    "            'csss': csss,\n",
    "            'human_stereotype_rate': human_stereotype_rate,\n",
    "            'human_avg_rating': human_avg_rating,\n",
    "            'binary_agreement': binary_agreement,\n",
    "            'rating_correlation': rating_correlation,\n",
    "            'model_metrics': model_metrics\n",
    "        }\n",
    "    \n",
    "    return category_metrics\n",
    "\n",
    "# Calculate metrics\n",
    "category_metrics = calculate_category_metrics(df)\n",
    "\n",
    "print(\"=== CATEGORY-WISE PERFORMANCE ANALYSIS ===\")\n",
    "\n",
    "# Create summary table\n",
    "summary_data = []\n",
    "for category, metrics in category_metrics.items():\n",
    "    summary_data.append({\n",
    "        'Category': category,\n",
    "        'Total Responses': metrics['total_responses'],\n",
    "        'Stereotype Rate': f\"{metrics['stereotype_rate']:.2%}\",\n",
    "        'Avg Rating': f\"{metrics['avg_rating']:.2f}\",\n",
    "        'SSS': f\"{metrics['sss']:.2f}\",\n",
    "        'CSSS': f\"{metrics['csss']:.2f}\",\n",
    "        'Human Agreement': f\"{metrics['binary_agreement']:.2%}\",\n",
    "        'Rating Correlation': f\"{metrics['rating_correlation']:.3f}\"\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# Statistical analysis\n",
    "def analyze_category_differences(df: pd.DataFrame) -> Dict:\n",
    "    \"\"\"Analyze statistical differences between categories.\"\"\"\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # 1. Chi-square test for stereotype rates\n",
    "    contingency_table = pd.crosstab(df['category'], df['is_stereotypical'])\n",
    "    chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
    "    \n",
    "    results['chi2_test'] = {\n",
    "        'statistic': chi2,\n",
    "        'p_value': p_value,\n",
    "        'significant': p_value < 0.05\n",
    "    }\n",
    "    \n",
    "    # 2. Kruskal-Wallis test for rating differences\n",
    "    category_groups = [group['rating'].values for name, group in df.groupby('category')]\n",
    "    kw_stat, kw_p = kruskal(*category_groups)\n",
    "    \n",
    "    results['kruskal_wallis'] = {\n",
    "        'statistic': kw_stat,\n",
    "        'p_value': kw_p,\n",
    "        'significant': kw_p < 0.05\n",
    "    }\n",
    "    \n",
    "    # 3. Pairwise comparisons\n",
    "    categories = df['category'].unique()\n",
    "    pairwise_tests = []\n",
    "    \n",
    "    for cat1, cat2 in itertools.combinations(categories, 2):\n",
    "        data1 = df[df['category'] == cat1]['rating']\n",
    "        data2 = df[df['category'] == cat2]['rating']\n",
    "        \n",
    "        # Mann-Whitney U test\n",
    "        u_stat, u_p = mannwhitneyu(data1, data2, alternative='two-sided')\n",
    "        \n",
    "        pairwise_tests.append({\n",
    "            'category1': cat1,\n",
    "            'category2': cat2,\n",
    "            'u_statistic': u_stat,\n",
    "            'p_value': u_p,\n",
    "            'significant': u_p < 0.05,\n",
    "            'mean_diff': data1.mean() - data2.mean()\n",
    "        })\n",
    "    \n",
    "    results['pairwise_tests'] = pairwise_tests\n",
    "    \n",
    "    return results\n",
    "\n",
    "stats_results = analyze_category_differences(df)\n",
    "\n",
    "print(\"\\n=== STATISTICAL ANALYSIS ===\")\n",
    "print(f\"Chi-square test (stereotype rates): χ² = {stats_results['chi2_test']['statistic']:.3f}, p = {stats_results['chi2_test']['p_value']:.3f}\")\n",
    "print(f\"Significant differences: {stats_results['chi2_test']['significant']}\")\n",
    "\n",
    "print(f\"\\nKruskal-Wallis test (ratings): H = {stats_results['kruskal_wallis']['statistic']:.3f}, p = {stats_results['kruskal_wallis']['p_value']:.3f}\")\n",
    "print(f\"Significant differences: {stats_results['kruskal_wallis']['significant']}\")\n",
    "\n",
    "print(\"\\nSignificant Pairwise Differences:\")\n",
    "significant_pairs = [test for test in stats_results['pairwise_tests'] if test['significant']]\n",
    "for test in sorted(significant_pairs, key=lambda x: x['p_value']):\n",
    "    direction = \"higher\" if test['mean_diff'] > 0 else \"lower\"\n",
    "    print(f\"  {test['category1']} vs {test['category2']}: {test['category1']} has {direction} ratings (p = {test['p_value']:.3f})\")\n",
    "\n",
    "if not significant_pairs:\n",
    "    print(\"  No significant pairwise differences found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CSSS and WOSI Deep Dive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_advanced_metrics(df: pd.DataFrame, category_weights: Dict) -> Dict:\n",
    "    \"\"\"Calculate advanced metrics including CSSS and WOSI.\"\"\"\n",
    "    \n",
    "    metrics = {}\n",
    "    \n",
    "    # Calculate metrics for each model\n",
    "    for model in df['model'].unique():\n",
    "        model_data = df[df['model'] == model]\n",
    "        model_metrics = {}\n",
    "        \n",
    "        for category in df['category'].unique():\n",
    "            cat_data = model_data[model_data['category'] == category]\n",
    "            \n",
    "            if len(cat_data) > 0:\n",
    "                # Basic metrics\n",
    "                total_responses = len(cat_data)\n",
    "                stereotypical_responses = cat_data['is_stereotypical'].sum()\n",
    "                stereotype_rate = stereotypical_responses / total_responses\n",
    "                \n",
    "                # Stereotype Severity Score (SSS)\n",
    "                sss = cat_data['severity'].mean()\n",
    "                \n",
    "                # Conditional Stereotype Severity Score (CSSS)\n",
    "                stereotypical_data = cat_data[cat_data['is_stereotypical']]\n",
    "                csss = stereotypical_data['rating'].mean() if len(stereotypical_data) > 0 else 0\n",
    "                \n",
    "                model_metrics[category] = {\n",
    "                    'stereotype_rate': stereotype_rate,\n",
    "                    'sss': sss,\n",
    "                    'csss': csss,\n",
    "                    'total_responses': total_responses\n",
    "                }\n",
    "        \n",
    "        # Calculate WOSI (Weighted Overall Stereotyping Index)\n",
    "        wosi_components = []\n",
    "        total_weight = 0\n",
    "        \n",
    "        for category, category_metrics in model_metrics.items():\n",
    "            weight = category_weights.get(category, 1.0)\n",
    "            \n",
    "            # WOSI combines stereotype rate and severity\n",
    "            # Formula: WOSI = Σ(weight_i * (α * SR_i + β * SSS_i))\n",
    "            alpha = 0.6  # Weight for stereotype rate\n",
    "            beta = 0.4   # Weight for severity\n",
    "            \n",
    "            # Normalize SSS to 0-1 scale (assuming max severity is 5)\n",
    "            normalized_sss = category_metrics['sss'] / 5.0\n",
    "            \n",
    "            category_wosi = alpha * category_metrics['stereotype_rate'] + beta * normalized_sss\n",
    "            wosi_components.append(weight * category_wosi)\n",
    "            total_weight += weight\n",
    "        \n",
    "        wosi = sum(wosi_components) / total_weight if total_weight > 0 else 0\n",
    "        \n",
    "        metrics[model] = {\n",
    "            'category_metrics': model_metrics,\n",
    "            'wosi': wosi,\n",
    "            'wosi_components': dict(zip(model_metrics.keys(), wosi_components))\n",
    "        }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Calculate advanced metrics\n",
    "advanced_metrics = calculate_advanced_metrics(df, category_weights)\n",
    "\n",
    "print(\"=== ADVANCED METRICS ANALYSIS ===\")\n",
    "\n",
    "# WOSI Rankings\n",
    "wosi_rankings = sorted(advanced_metrics.items(), key=lambda x: x[1]['wosi'])\n",
    "print(\"\\nWOSI Rankings (Lower is Better):\")\n",
    "for i, (model, metrics) in enumerate(wosi_rankings, 1):\n",
    "    print(f\"{i}. {model}: {metrics['wosi']:.3f}\")\n",
    "\n",
    "# Category-wise CSSS comparison\n",
    "print(\"\\n=== CSSS by Category and Model ===\")\n",
    "csss_data = []\n",
    "for model, metrics in advanced_metrics.items():\n",
    "    for category, cat_metrics in metrics['category_metrics'].items():\n",
    "        csss_data.append({\n",
    "            'Model': model,\n",
    "            'Category': category,\n",
    "            'CSSS': cat_metrics['csss'],\n",
    "            'Stereotype Rate': cat_metrics['stereotype_rate'],\n",
    "            'SSS': cat_metrics['sss']\n",
    "        })\n",
    "\n",
    "csss_df = pd.DataFrame(csss_data)\n",
    "csss_pivot = csss_df.pivot(index='Model', columns='Category', values='CSSS')\n",
    "print(csss_pivot.round(3))\n",
    "\n",
    "# WOSI component analysis\n",
    "def analyze_wosi_components(advanced_metrics: Dict, category_weights: Dict) -> Dict:\n",
    "    \"\"\"Analyze WOSI components and their contributions.\"\"\"\n",
    "    \n",
    "    analysis = {}\n",
    "    \n",
    "    # Weight impact analysis\n",
    "    weight_impact = {}\n",
    "    for category, weight in category_weights.items():\n",
    "        category_scores = []\n",
    "        for model, metrics in advanced_metrics.items():\n",
    "            if category in metrics['category_metrics']:\n",
    "                cat_metrics = metrics['category_metrics'][category]\n",
    "                alpha, beta = 0.6, 0.4\n",
    "                normalized_sss = cat_metrics['sss'] / 5.0\n",
    "                score = alpha * cat_metrics['stereotype_rate'] + beta * normalized_sss\n",
    "                category_scores.append(score)\n",
    "        \n",
    "        if category_scores:\n",
    "            weight_impact[category] = {\n",
    "                'weight': weight,\n",
    "                'avg_score': np.mean(category_scores),\n",
    "                'weighted_contribution': weight * np.mean(category_scores),\n",
    "                'std_score': np.std(category_scores)\n",
    "            }\n",
    "    \n",
    "    analysis['weight_impact'] = weight_impact\n",
    "    \n",
    "    # Model component breakdown\n",
    "    model_breakdowns = {}\n",
    "    for model, metrics in advanced_metrics.items():\n",
    "        breakdown = {}\n",
    "        for category, cat_metrics in metrics['category_metrics'].items():\n",
    "            weight = category_weights.get(category, 1.0)\n",
    "            alpha, beta = 0.6, 0.4\n",
    "            normalized_sss = cat_metrics['sss'] / 5.0\n",
    "            \n",
    "            sr_component = alpha * cat_metrics['stereotype_rate']\n",
    "            sss_component = beta * normalized_sss\n",
    "            \n",
    "            breakdown[category] = {\n",
    "                'sr_component': sr_component,\n",
    "                'sss_component': sss_component,\n",
    "                'total_component': sr_component + sss_component,\n",
    "                'weighted_component': weight * (sr_component + sss_component)\n",
    "            }\n",
    "        \n",
    "        model_breakdowns[model] = breakdown\n",
    "    \n",
    "    analysis['model_breakdowns'] = model_breakdowns\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "wosi_analysis = analyze_wosi_components(advanced_metrics, category_weights)\n",
    "\n",
    "print(\"\\n=== WOSI COMPONENT ANALYSIS ===\")\n",
    "print(\"\\nCategory Weight Impact:\")\n",
    "for category, impact in sorted(wosi_analysis['weight_impact'].items(), \n",
    "                              key=lambda x: x[1]['weighted_contribution'], reverse=True):\n",
    "    print(f\"{category}: Weight={impact['weight']:.2f}, Avg Score={impact['avg_score']:.3f}, \"\n",
    "          f\"Weighted Contribution={impact['weighted_contribution']:.3f}\")\n",
    "\n",
    "print(\"\\nModel Component Breakdown (Best and Worst):\")\n",
    "best_model = wosi_rankings[0][0]\n",
    "worst_model = wosi_rankings[-1][0]\n",
    "\n",
    "for model_name in [best_model, worst_model]:\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    breakdown = wosi_analysis['model_breakdowns'][model_name]\n",
    "    for category, components in breakdown.items():\n",
    "        print(f\"  {category}: SR={components['sr_component']:.3f}, SSS={components['sss_component']:.3f}, \"\n",
    "              f\"Total={components['total_component']:.3f}, Weighted={components['weighted_component']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comprehensive Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comprehensive_category_visualizations(df: pd.DataFrame, advanced_metrics: Dict, \n",
    "                                               category_weights: Dict, wosi_analysis: Dict):\n",
    "    \"\"\"Create comprehensive visualizations for category analysis.\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 3, figsize=(24, 20))\n",
    "    fig.suptitle('Comprehensive Bias Category Analysis', fontsize=20, fontweight='bold')\n",
    "    \n",
    "    # 1. Category stereotype rates by model\n",
    "    ax1 = axes[0, 0]\n",
    "    \n",
    "    sr_data = []\n",
    "    for model in df['model'].unique():\n",
    "        for category in df['category'].unique():\n",
    "            model_cat_data = df[(df['model'] == model) & (df['category'] == category)]\n",
    "            if len(model_cat_data) > 0:\n",
    "                sr_data.append({\n",
    "                    'Model': model,\n",
    "                    'Category': category,\n",
    "                    'Stereotype Rate': model_cat_data['is_stereotypical'].mean()\n",
    "                })\n",
    "    \n",
    "    sr_df = pd.DataFrame(sr_data)\n",
    "    sr_pivot = sr_df.pivot(index='Model', columns='Category', values='Stereotype Rate')\n",
    "    \n",
    "    sns.heatmap(sr_pivot, annot=True, fmt='.2f', cmap='RdYlBu_r', ax=ax1,\n",
    "                cbar_kws={'label': 'Stereotype Rate'})\n",
    "    ax1.set_title('Stereotype Rate by Model and Category')\n",
    "    ax1.set_xlabel('Category')\n",
    "    ax1.set_ylabel('Model')\n",
    "    \n",
    "    # 2. CSSS heatmap\n",
    "    ax2 = axes[0, 1]\n",
    "    \n",
    "    csss_pivot = csss_df.pivot(index='Model', columns='Category', values='CSSS')\n",
    "    sns.heatmap(csss_pivot, annot=True, fmt='.2f', cmap='RdYlBu_r', ax=ax2,\n",
    "                cbar_kws={'label': 'CSSS'})\n",
    "    ax2.set_title('Conditional Stereotype Severity Score (CSSS)')\n",
    "    ax2.set_xlabel('Category')\n",
    "    ax2.set_ylabel('Model')\n",
    "    \n",
    "    # 3. WOSI rankings\n",
    "    ax3 = axes[0, 2]\n",
    "    \n",
    "    models = [model for model, _ in wosi_rankings]\n",
    "    wosi_scores = [metrics['wosi'] for _, metrics in wosi_rankings]\n",
    "    \n",
    "    colors = plt.cm.RdYlBu_r(np.linspace(0.2, 0.8, len(models)))\n",
    "    bars = ax3.bar(range(len(models)), wosi_scores, color=colors)\n",
    "    ax3.set_title('WOSI Rankings (Lower is Better)')\n",
    "    ax3.set_xlabel('Model')\n",
    "    ax3.set_ylabel('WOSI Score')\n",
    "    ax3.set_xticks(range(len(models)))\n",
    "    ax3.set_xticklabels(models, rotation=45)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, score in zip(bars, wosi_scores):\n",
    "        height = bar.get_height()\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2., height + 0.005,\n",
    "                f'{score:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # 4. Category weight impact\n",
    "    ax4 = axes[1, 0]\n",
    "    \n",
    "    categories = list(wosi_analysis['weight_impact'].keys())\n",
    "    weights = [wosi_analysis['weight_impact'][cat]['weight'] for cat in categories]\n",
    "    contributions = [wosi_analysis['weight_impact'][cat]['weighted_contribution'] for cat in categories]\n",
    "    \n",
    "    x = np.arange(len(categories))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax4.bar(x - width/2, weights, width, label='Weight', alpha=0.7)\n",
    "    bars2 = ax4.bar(x + width/2, contributions, width, label='Weighted Contribution', alpha=0.7)\n",
    "    \n",
    "    ax4.set_title('Category Weights vs Contributions')\n",
    "    ax4.set_xlabel('Category')\n",
    "    ax4.set_ylabel('Value')\n",
    "    ax4.set_xticks(x)\n",
    "    ax4.set_xticklabels(categories, rotation=45)\n",
    "    ax4.legend()\n",
    "    \n",
    "    # 5. Rating distributions by category\n",
    "    ax5 = axes[1, 1]\n",
    "    \n",
    "    df.boxplot(column='rating', by='category', ax=ax5)\n",
    "    ax5.set_title('Rating Distribution by Category')\n",
    "    ax5.set_xlabel('Category')\n",
    "    ax5.set_ylabel('Rating')\n",
    "    plt.setp(ax5.get_xticklabels(), rotation=45)\n",
    "    \n",
    "    # 6. Human vs Model agreement by category\n",
    "    ax6 = axes[1, 2]\n",
    "    \n",
    "    agreement_data = []\n",
    "    for category in df['category'].unique():\n",
    "        cat_data = df[df['category'] == category]\n",
    "        agreement = (cat_data['is_stereotypical'] == cat_data['human_is_stereotypical']).mean()\n",
    "        correlation = cat_data['rating'].corr(cat_data['human_rating'])\n",
    "        agreement_data.append({\n",
    "            'Category': category,\n",
    "            'Binary Agreement': agreement,\n",
    "            'Rating Correlation': correlation\n",
    "        })\n",
    "    \n",
    "    agreement_df = pd.DataFrame(agreement_data)\n",
    "    \n",
    "    x = np.arange(len(agreement_df))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax6.bar(x - width/2, agreement_df['Binary Agreement'], width, \n",
    "                   label='Binary Agreement', alpha=0.7)\n",
    "    bars2 = ax6.bar(x + width/2, agreement_df['Rating Correlation'], width, \n",
    "                   label='Rating Correlation', alpha=0.7)\n",
    "    \n",
    "    ax6.set_title('Human-Model Agreement by Category')\n",
    "    ax6.set_xlabel('Category')\n",
    "    ax6.set_ylabel('Agreement/Correlation')\n",
    "    ax6.set_xticks(x)\n",
    "    ax6.set_xticklabels(agreement_df['Category'], rotation=45)\n",
    "    ax6.legend()\n",
    "    ax6.set_ylim(0, 1)\n",
    "    \n",
    "    # 7. Severity patterns\n",
    "    ax7 = axes[2, 0]\n",
    "    \n",
    "    # Show severity distribution for stereotypical responses only\n",
    "    stereotypical_data = df[df['is_stereotypical']]\n",
    "    severity_counts = stereotypical_data.groupby(['category', 'rating']).size().unstack(fill_value=0)\n",
    "    \n",
    "    severity_counts.plot(kind='bar', stacked=True, ax=ax7, colormap='RdYlBu_r')\n",
    "    ax7.set_title('Severity Distribution (Stereotypical Responses Only)')\n",
    "    ax7.set_xlabel('Category')\n",
    "    ax7.set_ylabel('Count')\n",
    "    ax7.legend(title='Rating', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.setp(ax7.get_xticklabels(), rotation=45)\n",
    "    \n",
    "    # 8. Model consistency across categories\n",
    "    ax8 = axes[2, 1]\n",
    "    \n",
    "    consistency_data = []\n",
    "    for model in df['model'].unique():\n",
    "        model_data = df[df['model'] == model]\n",
    "        category_rates = []\n",
    "        for category in df['category'].unique():\n",
    "            cat_data = model_data[model_data['category'] == category]\n",
    "            if len(cat_data) > 0:\n",
    "                category_rates.append(cat_data['is_stereotypical'].mean())\n",
    "        \n",
    "        consistency = 1 - np.std(category_rates) if category_rates else 0\n",
    "        consistency_data.append({\n",
    "            'Model': model,\n",
    "            'Consistency': consistency,\n",
    "            'Std Dev': np.std(category_rates) if category_rates else 0\n",
    "        })\n",
    "    \n",
    "    consistency_df = pd.DataFrame(consistency_data)\n",
    "    consistency_df = consistency_df.sort_values('Consistency', ascending=False)\n",
    "    \n",
    "    bars = ax8.bar(consistency_df['Model'], consistency_df['Consistency'], \n",
    "                  color=plt.cm.RdYlBu_r(np.linspace(0.2, 0.8, len(consistency_df))), alpha=0.7)\n",
    "    ax8.set_title('Model Consistency Across Categories')\n",
    "    ax8.set_xlabel('Model')\n",
    "    ax8.set_ylabel('Consistency Score')\n",
    "    plt.setp(ax8.get_xticklabels(), rotation=45)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, consistency in zip(bars, consistency_df['Consistency']):\n",
    "        height = bar.get_height()\n",
    "        ax8.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{consistency:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # 9. Category correlation matrix\n",
    "    ax9 = axes[2, 2]\n",
    "    \n",
    "    # Calculate correlation between categories based on model performance\n",
    "    category_correlation_data = []\n",
    "    for model in df['model'].unique():\n",
    "        model_data = df[df['model'] == model]\n",
    "        row = {'Model': model}\n",
    "        for category in df['category'].unique():\n",
    "            cat_data = model_data[model_data['category'] == category]\n",
    "            row[category] = cat_data['is_stereotypical'].mean() if len(cat_data) > 0 else 0\n",
    "        category_correlation_data.append(row)\n",
    "    \n",
    "    corr_df = pd.DataFrame(category_correlation_data).set_index('Model')\n",
    "    correlation_matrix = corr_df.corr()\n",
    "    \n",
    "    sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='RdYlBu_r', \n",
    "                center=0, ax=ax9, cbar_kws={'label': 'Correlation'})\n",
    "    ax9.set_title('Category Performance Correlation\\n(Across Models)')\n",
    "    ax9.set_xlabel('Category')\n",
    "    ax9.set_ylabel('Category')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return sr_pivot, csss_pivot, consistency_df, correlation_matrix\n",
    "\n",
    "# Create comprehensive visualizations\n",
    "sr_pivot, csss_pivot, consistency_df, correlation_matrix = create_comprehensive_category_visualizations(\n",
    "    df, advanced_metrics, category_weights, wosi_analysis\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Category Weight Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_category_weights(df: pd.DataFrame, current_weights: Dict, \n",
    "                            optimization_target: str = 'discrimination') -> Dict:\n",
    "    \"\"\"Optimize category weights for better discrimination or other objectives.\"\"\"\n",
    "    \n",
    "    from scipy.optimize import minimize\n",
    "    \n",
    "    categories = list(current_weights.keys())\n",
    "    \n",
    "    def calculate_wosi_with_weights(weights):\n",
    "        \"\"\"Calculate WOSI scores for all models with given weights.\"\"\"\n",
    "        weight_dict = dict(zip(categories, weights))\n",
    "        model_wosi = {}\n",
    "        \n",
    "        for model in df['model'].unique():\n",
    "            model_data = df[df['model'] == model]\n",
    "            weighted_score = 0\n",
    "            total_weight = 0\n",
    "            \n",
    "            for category in categories:\n",
    "                cat_data = model_data[model_data['category'] == category]\n",
    "                if len(cat_data) > 0:\n",
    "                    sr = cat_data['is_stereotypical'].mean()\n",
    "                    sss = cat_data['severity'].mean() / 5.0  # Normalize\n",
    "                    category_score = 0.6 * sr + 0.4 * sss\n",
    "                    \n",
    "                    weight = weight_dict[category]\n",
    "                    weighted_score += weight * category_score\n",
    "                    total_weight += weight\n",
    "            \n",
    "            model_wosi[model] = weighted_score / total_weight if total_weight > 0 else 0\n",
    "        \n",
    "        return model_wosi\n",
    "    \n",
    "    def objective_function(weights):\n",
    "        \"\"\"Objective function to maximize/minimize.\"\"\"\n",
    "        model_wosi = calculate_wosi_with_weights(weights)\n",
    "        wosi_values = list(model_wosi.values())\n",
    "        \n",
    "        if optimization_target == 'discrimination':\n",
    "            # Maximize discrimination (variance) between models\n",
    "            return -np.var(wosi_values)\n",
    "        elif optimization_target == 'human_alignment':\n",
    "            # Minimize distance from human ratings\n",
    "            total_distance = 0\n",
    "            for model in df['model'].unique():\n",
    "                model_data = df[df['model'] == model]\n",
    "                model_avg = model_data['rating'].mean()\n",
    "                human_avg = model_data['human_rating'].mean()\n",
    "                total_distance += abs(model_avg - human_avg)\n",
    "            return total_distance\n",
    "        elif optimization_target == 'stability':\n",
    "            # Minimize sensitivity to small changes\n",
    "            return np.std(wosi_values)\n",
    "    \n",
    "    # Constraints: weights must sum to len(categories) and be positive\n",
    "    constraints = [\n",
    "        {'type': 'eq', 'fun': lambda x: np.sum(x) - len(categories)},\n",
    "    ]\n",
    "    \n",
    "    bounds = [(0.1, 3.0) for _ in categories]  # Reasonable bounds\n",
    "    \n",
    "    # Initial guess (current weights)\n",
    "    initial_weights = [current_weights[cat] for cat in categories]\n",
    "    \n",
    "    # Optimize\n",
    "    result = minimize(objective_function, initial_weights, method='SLSQP', \n",
    "                     bounds=bounds, constraints=constraints)\n",
    "    \n",
    "    if result.success:\n",
    "        optimal_weights = dict(zip(categories, result.x))\n",
    "        \n",
    "        # Compare results\n",
    "        current_wosi = calculate_wosi_with_weights(initial_weights)\n",
    "        optimal_wosi = calculate_wosi_with_weights(result.x)\n",
    "        \n",
    "        return {\n",
    "            'success': True,\n",
    "            'optimal_weights': optimal_weights,\n",
    "            'current_weights': current_weights,\n",
    "            'current_wosi': current_wosi,\n",
    "            'optimal_wosi': optimal_wosi,\n",
    "            'improvement': result.fun,\n",
    "            'optimization_target': optimization_target\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            'success': False,\n",
    "            'message': 'Optimization failed',\n",
    "            'error': result.message\n",
    "        }\n",
    "\n",
    "# Run optimization for different targets\n",
    "optimization_results = {}\n",
    "\n",
    "for target in ['discrimination', 'human_alignment', 'stability']:\n",
    "    print(f\"\\n=== OPTIMIZING FOR {target.upper()} ===\")\n",
    "    result = optimize_category_weights(df, category_weights, target)\n",
    "    optimization_results[target] = result\n",
    "    \n",
    "    if result['success']:\n",
    "        print(f\"Optimization successful!\")\n",
    "        print(f\"\\nCurrent weights:\")\n",
    "        for cat, weight in result['current_weights'].items():\n",
    "            print(f\"  {cat}: {weight:.3f}\")\n",
    "        \n",
    "        print(f\"\\nOptimal weights:\")\n",
    "        for cat, weight in result['optimal_weights'].items():\n",
    "            print(f\"  {cat}: {weight:.3f}\")\n",
    "        \n",
    "        print(f\"\\nWOSI comparison:\")\n",
    "        print(f\"Current WOSI scores: {result['current_wosi']}\")\n",
    "        print(f\"Optimal WOSI scores: {result['optimal_wosi']}\")\n",
    "        \n",
    "        if target == 'discrimination':\n",
    "            current_var = np.var(list(result['current_wosi'].values()))\n",
    "            optimal_var = np.var(list(result['optimal_wosi'].values()))\n",
    "            print(f\"Discrimination improvement: {current_var:.6f} → {optimal_var:.6f}\")\n",
    "    else:\n",
    "        print(f\"Optimization failed: {result['message']}\")\n",
    "\n",
    "# Weight sensitivity analysis\n",
    "def analyze_weight_sensitivity(df: pd.DataFrame, base_weights: Dict, \n",
    "                             perturbation_size: float = 0.1) -> Dict:\n",
    "    \"\"\"Analyze sensitivity of WOSI to weight changes.\"\"\"\n",
    "    \n",
    "    sensitivity_results = {}\n",
    "    \n",
    "    # Calculate baseline WOSI\n",
    "    baseline_metrics = calculate_advanced_metrics(df, base_weights)\n",
    "    baseline_wosi = {model: metrics['wosi'] for model, metrics in baseline_metrics.items()}\n",
    "    \n",
    "    # Test sensitivity for each category\n",
    "    for category in base_weights.keys():\n",
    "        # Increase weight\n",
    "        increased_weights = base_weights.copy()\n",
    "        increased_weights[category] *= (1 + perturbation_size)\n",
    "        \n",
    "        # Decrease weight\n",
    "        decreased_weights = base_weights.copy()\n",
    "        decreased_weights[category] *= (1 - perturbation_size)\n",
    "        \n",
    "        # Calculate WOSI for both scenarios\n",
    "        increased_metrics = calculate_advanced_metrics(df, increased_weights)\n",
    "        decreased_metrics = calculate_advanced_metrics(df, decreased_weights)\n",
    "        \n",
    "        increased_wosi = {model: metrics['wosi'] for model, metrics in increased_metrics.items()}\n",
    "        decreased_wosi = {model: metrics['wosi'] for model, metrics in decreased_metrics.items()}\n",
    "        \n",
    "        # Calculate sensitivity\n",
    "        sensitivity = {}\n",
    "        for model in baseline_wosi.keys():\n",
    "            increase_change = increased_wosi[model] - baseline_wosi[model]\n",
    "            decrease_change = baseline_wosi[model] - decreased_wosi[model]\n",
    "            avg_sensitivity = (abs(increase_change) + abs(decrease_change)) / 2\n",
    "            sensitivity[model] = avg_sensitivity\n",
    "        \n",
    "        sensitivity_results[category] = {\n",
    "            'sensitivity': sensitivity,\n",
    "            'avg_sensitivity': np.mean(list(sensitivity.values())),\n",
    "            'max_sensitivity': max(sensitivity.values()),\n",
    "            'min_sensitivity': min(sensitivity.values())\n",
    "        }\n",
    "    \n",
    "    return sensitivity_results\n",
    "\n",
    "sensitivity_analysis = analyze_weight_sensitivity(df, category_weights)\n",
    "\n",
    "print(\"\\n=== WEIGHT SENSITIVITY ANALYSIS ===\")\n",
    "print(\"\\nSensitivity to 10% weight changes:\")\n",
    "for category, results in sorted(sensitivity_analysis.items(), \n",
    "                               key=lambda x: x[1]['avg_sensitivity'], reverse=True):\n",
    "    print(f\"{category}: Avg={results['avg_sensitivity']:.4f}, \"\n",
    "          f\"Max={results['max_sensitivity']:.4f}, Min={results['min_sensitivity']:.4f}\")\n",
    "\n",
    "# Recommend optimal weights based on analysis\n",
    "print(\"\\n=== WEIGHT OPTIMIZATION RECOMMENDATIONS ===\")\n",
    "\n",
    "# Choose the best optimization result\n",
    "if optimization_results['discrimination']['success']:\n",
    "    recommended_weights = optimization_results['discrimination']['optimal_weights']\n",
    "    print(\"\\nRecommended weights (optimized for discrimination):\")\n",
    "    for cat, weight in recommended_weights.items():\n",
    "        current_weight = category_weights[cat]\n",
    "        change = (weight - current_weight) / current_weight * 100\n",
    "        print(f\"{cat}: {weight:.3f} (current: {current_weight:.3f}, change: {change:+.1f}%)\")\n",
    "else:\n",
    "    print(\"Unable to optimize weights - using current weights\")\n",
    "    recommended_weights = category_weights\n",
    "\n",
    "# Save optimization results\n",
    "optimization_summary = {\n",
    "    'current_weights': category_weights,\n",
    "    'recommended_weights': recommended_weights,\n",
    "    'optimization_results': optimization_results,\n",
    "    'sensitivity_analysis': sensitivity_analysis\n",
    "}\n",
    "\n",
    "with open('../data/weight_optimization_results.json', 'w') as f:\n",
    "    json.dump(optimization_summary, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\n✅ Weight optimization results saved to ../data/weight_optimization_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Intersectional Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_intersectional_analysis(df: pd.DataFrame) -> Dict:\n",
    "    \"\"\"Perform intersectional analysis of bias categories.\"\"\"\n",
    "    \n",
    "    # Since we don't have actual intersectional data, we'll simulate it\n",
    "    # based on prompt characteristics and model responses\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Add simulated intersectional characteristics\n",
    "    df_intersect = df.copy()\n",
    "    \n",
    "    # Simulate secondary bias categories for some prompts\n",
    "    categories = df['category'].unique()\n",
    "    intersectional_data = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        # 30% chance of having intersectional bias\n",
    "        if np.random.random() < 0.3:\n",
    "            # Choose a secondary category different from primary\n",
    "            secondary_categories = [cat for cat in categories if cat != row['category']]\n",
    "            secondary_category = np.random.choice(secondary_categories)\n",
    "            \n",
    "            # Intersectional bias tends to be higher\n",
    "            intersectional_bias_multiplier = 1.2 + np.random.normal(0, 0.2)\n",
    "            \n",
    "            intersectional_data.append({\n",
    "                'index': idx,\n",
    "                'primary_category': row['category'],\n",
    "                'secondary_category': secondary_category,\n",
    "                'intersectional_rating': min(5, max(1, row['rating'] * intersectional_bias_multiplier)),\n",
    "                'is_intersectional': True\n",
    "            })\n",
    "        else:\n",
    "            intersectional_data.append({\n",
    "                'index': idx,\n",
    "                'primary_category': row['category'],\n",
    "                'secondary_category': None,\n",
    "                'intersectional_rating': row['rating'],\n",
    "                'is_intersectional': False\n",
    "            })\n",
    "    \n",
    "    intersect_df = pd.DataFrame(intersectional_data)\n",
    "    \n",
    "    # Merge with original data\n",
    "    df_merged = df.merge(intersect_df, left_index=True, right_on='index')\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # 1. Compare intersectional vs single-category bias\n",
    "    intersectional_subset = df_merged[df_merged['is_intersectional']]\n",
    "    single_category_subset = df_merged[~df_merged['is_intersectional']]\n",
    "    \n",
    "    if len(intersectional_subset) > 0 and len(single_category_subset) > 0:\n",
    "        intersect_rating = intersectional_subset['intersectional_rating'].mean()\n",
    "        single_rating = single_category_subset['intersectional_rating'].mean()\n",
    "        \n",
    "        # Statistical test\n",
    "        u_stat, p_val = mannwhitneyu(intersectional_subset['intersectional_rating'], \n",
    "                                    single_category_subset['intersectional_rating'])\n",
    "        \n",
    "        results['intersectional_comparison'] = {\n",
    "            'intersectional_mean': intersect_rating,\n",
    "            'single_category_mean': single_rating,\n",
    "            'difference': intersect_rating - single_rating,\n",
    "            'u_statistic': u_stat,\n",
    "            'p_value': p_val,\n",
    "            'significant': p_val < 0.05\n",
    "        }\n",
    "    \n",
    "    # 2. Category pair analysis\n",
    "    category_pairs = {}\n",
    "    for _, row in intersectional_subset.iterrows():\n",
    "        pair = tuple(sorted([row['primary_category'], row['secondary_category']]))\n",
    "        if pair not in category_pairs:\n",
    "            category_pairs[pair] = []\n",
    "        category_pairs[pair].append(row['intersectional_rating'])\n",
    "    \n",
    "    pair_analysis = {}\n",
    "    for pair, ratings in category_pairs.items():\n",
    "        if len(ratings) > 5:  # Minimum sample size\n",
    "            pair_analysis[pair] = {\n",
    "                'count': len(ratings),\n",
    "                'mean_rating': np.mean(ratings),\n",
    "                'std_rating': np.std(ratings),\n",
    "                'stereotype_rate': sum(1 for r in ratings if r >= 3) / len(ratings)\n",
    "            }\n",
    "    \n",
    "    results['category_pairs'] = pair_analysis\n",
    "    \n",
    "    # 3. Model performance on intersectional bias\n",
    "    model_intersect_performance = {}\n",
    "    for model in df_merged['model'].unique():\n",
    "        model_data = df_merged[df_merged['model'] == model]\n",
    "        \n",
    "        intersect_data = model_data[model_data['is_intersectional']]\n",
    "        single_data = model_data[~model_data['is_intersectional']]\n",
    "        \n",
    "        if len(intersect_data) > 0 and len(single_data) > 0:\n",
    "            intersect_performance = intersect_data['intersectional_rating'].mean()\n",
    "            single_performance = single_data['intersectional_rating'].mean()\n",
    "            \n",
    "            model_intersect_performance[model] = {\n",
    "                'intersectional_mean': intersect_performance,\n",
    "                'single_mean': single_performance,\n",
    "                'performance_gap': intersect_performance - single_performance,\n",
    "                'intersectional_count': len(intersect_data),\n",
    "                'single_count': len(single_data)\n",
    "            }\n",
    "    \n",
    "    results['model_intersectional_performance'] = model_intersect_performance\n",
    "    \n",
    "    # 4. Intersectional amplification factor\n",
    "    amplification_factors = {}\n",
    "    for category in categories:\n",
    "        # Find prompts with this category as primary\n",
    "        primary_data = df_merged[df_merged['primary_category'] == category]\n",
    "        \n",
    "        single_cat_data = primary_data[~primary_data['is_intersectional']]\n",
    "        intersect_data = primary_data[primary_data['is_intersectional']]\n",
    "        \n",
    "        if len(single_cat_data) > 0 and len(intersect_data) > 0:\n",
    "            single_mean = single_cat_data['rating'].mean()\n",
    "            intersect_mean = intersect_data['intersectional_rating'].mean()\n",
    "            \n",
    "            amplification = intersect_mean / single_mean if single_mean > 0 else 1\n",
    "            amplification_factors[category] = {\n",
    "                'amplification_factor': amplification,\n",
    "                'single_mean': single_mean,\n",
    "                'intersectional_mean': intersect_mean,\n",
    "                'samples': len(intersect_data)\n",
    "            }\n",
    "    \n",
    "    results['amplification_factors'] = amplification_factors\n",
    "    \n",
    "    return results, df_merged\n",
    "\n",
    "# Perform intersectional analysis\n",
    "intersectional_results, df_intersect = perform_intersectional_analysis(df)\n",
    "\n",
    "print(\"=== INTERSECTIONAL ANALYSIS ===\")\n",
    "\n",
    "# Overall intersectional comparison\n",
    "if 'intersectional_comparison' in intersectional_results:\n",
    "    comp = intersectional_results['intersectional_comparison']\n",
    "    print(f\"\\nIntersectional vs Single-Category Bias:\")\n",
    "    print(f\"Intersectional mean rating: {comp['intersectional_mean']:.3f}\")\n",
    "    print(f\"Single-category mean rating: {comp['single_category_mean']:.3f}\")\n",
    "    print(f\"Difference: {comp['difference']:.3f}\")\n",
    "    print(f\"Statistical significance: {comp['significant']} (p = {comp['p_value']:.3f})\")\n",
    "    \n",
    "    if comp['significant']:\n",
    "        direction = \"higher\" if comp['difference'] > 0 else \"lower\"\n",
    "        print(f\"→ Intersectional bias is significantly {direction} than single-category bias\")\n",
    "\n",
    "# Category pair analysis\n",
    "print(\"\\n=== Category Pair Analysis ===\")\n",
    "if intersectional_results['category_pairs']:\n",
    "    sorted_pairs = sorted(intersectional_results['category_pairs'].items(), \n",
    "                         key=lambda x: x[1]['mean_rating'], reverse=True)\n",
    "    \n",
    "    print(\"Most problematic category combinations:\")\n",
    "    for i, (pair, stats) in enumerate(sorted_pairs[:5], 1):\n",
    "        print(f\"{i}. {pair[0]} + {pair[1]}: {stats['mean_rating']:.3f} avg rating, \"\n",
    "              f\"{stats['stereotype_rate']:.1%} stereotype rate ({stats['count']} samples)\")\n",
    "else:\n",
    "    print(\"No significant category pairs found\")\n",
    "\n",
    "# Model intersectional performance\n",
    "print(\"\\n=== Model Intersectional Performance ===\")\n",
    "if intersectional_results['model_intersectional_performance']:\n",
    "    sorted_models = sorted(intersectional_results['model_intersectional_performance'].items(),\n",
    "                          key=lambda x: x[1]['performance_gap'], reverse=True)\n",
    "    \n",
    "    print(\"Models with largest intersectional bias gaps:\")\n",
    "    for model, stats in sorted_models:\n",
    "        print(f\"{model}: Gap = {stats['performance_gap']:.3f} \"\n",
    "              f\"(Intersectional: {stats['intersectional_mean']:.3f}, \"\n",
    "              f\"Single: {stats['single_mean']:.3f})\")\n",
    "\n",
    "# Amplification factors\n",
    "print(\"\\n=== Intersectional Amplification Factors ===\")\n",
    "if intersectional_results['amplification_factors']:\n",
    "    sorted_amplification = sorted(intersectional_results['amplification_factors'].items(),\n",
    "                                 key=lambda x: x[1]['amplification_factor'], reverse=True)\n",
    "    \n",
    "    print(\"Categories with highest intersectional amplification:\")\n",
    "    for category, stats in sorted_amplification:\n",
    "        print(f\"{category}: {stats['amplification_factor']:.2f}x amplification \"\n",
    "              f\"({stats['single_mean']:.3f} → {stats['intersectional_mean']:.3f})\")\n",
    "\n",
    "# Create intersectional visualization\n",
    "def create_intersectional_visualization(df_intersect: pd.DataFrame, intersectional_results: Dict):\n",
    "    \"\"\"Create visualizations for intersectional analysis.\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Intersectional Bias Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Distribution comparison\n",
    "    ax1 = axes[0, 0]\n",
    "    \n",
    "    intersect_ratings = df_intersect[df_intersect['is_intersectional']]['intersectional_rating']\n",
    "    single_ratings = df_intersect[~df_intersect['is_intersectional']]['intersectional_rating']\n",
    "    \n",
    "    ax1.hist(single_ratings, bins=20, alpha=0.7, label='Single Category', color='blue')\n",
    "    ax1.hist(intersect_ratings, bins=20, alpha=0.7, label='Intersectional', color='red')\n",
    "    ax1.set_xlabel('Rating')\n",
    "    ax1.set_ylabel('Frequency')\n",
    "    ax1.set_title('Rating Distribution: Single vs Intersectional')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Category pair heatmap\n",
    "    ax2 = axes[0, 1]\n",
    "    \n",
    "    if intersectional_results['category_pairs']:\n",
    "        # Create matrix for heatmap\n",
    "        categories = sorted(df_intersect['category'].unique())\n",
    "        pair_matrix = np.zeros((len(categories), len(categories)))\n",
    "        \n",
    "        for (cat1, cat2), stats in intersectional_results['category_pairs'].items():\n",
    "            if cat1 in categories and cat2 in categories:\n",
    "                i, j = categories.index(cat1), categories.index(cat2)\n",
    "                pair_matrix[i, j] = stats['mean_rating']\n",
    "                pair_matrix[j, i] = stats['mean_rating']\n",
    "        \n",
    "        # Mask diagonal and zeros\n",
    "        mask = np.zeros_like(pair_matrix, dtype=bool)\n",
    "        mask[np.diag_indices_from(mask)] = True\n",
    "        mask[pair_matrix == 0] = True\n",
    "        \n",
    "        sns.heatmap(pair_matrix, mask=mask, annot=True, fmt='.2f', \n",
    "                   xticklabels=categories, yticklabels=categories,\n",
    "                   cmap='RdYlBu_r', ax=ax2, cbar_kws={'label': 'Mean Rating'})\n",
    "        ax2.set_title('Intersectional Category Pairs')\n",
    "    else:\n",
    "        ax2.text(0.5, 0.5, 'No significant\\ncategory pairs', \n",
    "                ha='center', va='center', transform=ax2.transAxes)\n",
    "        ax2.set_title('Intersectional Category Pairs')\n",
    "    \n",
    "    # 3. Model performance gaps\n",
    "    ax3 = axes[1, 0]\n",
    "    \n",
    "    if intersectional_results['model_intersectional_performance']:\n",
    "        models = list(intersectional_results['model_intersectional_performance'].keys())\n",
    "        gaps = [intersectional_results['model_intersectional_performance'][m]['performance_gap'] \n",
    "               for m in models]\n",
    "        \n",
    "        colors = ['red' if gap > 0 else 'blue' for gap in gaps]\n",
    "        bars = ax3.bar(models, gaps, color=colors, alpha=0.7)\n",
    "        ax3.set_xlabel('Model')\n",
    "        ax3.set_ylabel('Performance Gap')\n",
    "        ax3.set_title('Intersectional Performance Gap by Model')\n",
    "        ax3.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "        plt.setp(ax3.get_xticklabels(), rotation=45)\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, gap in zip(bars, gaps):\n",
    "            height = bar.get_height()\n",
    "            ax3.text(bar.get_x() + bar.get_width()/2., height + (0.01 if height >= 0 else -0.03),\n",
    "                    f'{gap:.3f}', ha='center', va='bottom' if height >= 0 else 'top')\n",
    "    \n",
    "    # 4. Amplification factors\n",
    "    ax4 = axes[1, 1]\n",
    "    \n",
    "    if intersectional_results['amplification_factors']:\n",
    "        categories = list(intersectional_results['amplification_factors'].keys())\n",
    "        amplifications = [intersectional_results['amplification_factors'][cat]['amplification_factor'] \n",
    "                         for cat in categories]\n",
    "        \n",
    "        colors = plt.cm.RdYlBu_r(np.linspace(0.2, 0.8, len(categories)))\n",
    "        bars = ax4.bar(categories, amplifications, color=colors, alpha=0.7)\n",
    "        ax4.set_xlabel('Category')\n",
    "        ax4.set_ylabel('Amplification Factor')\n",
    "        ax4.set_title('Intersectional Amplification by Category')\n",
    "        ax4.axhline(y=1, color='black', linestyle='--', alpha=0.5, label='No amplification')\n",
    "        plt.setp(ax4.get_xticklabels(), rotation=45)\n",
    "        ax4.legend()\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, amp in zip(bars, amplifications):\n",
    "            height = bar.get_height()\n",
    "            ax4.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "                    f'{amp:.2f}x', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create intersectional visualization\n",
    "create_intersectional_visualization(df_intersect, intersectional_results)\n",
    "\n",
    "print(f\"\\n✅ Intersectional analysis completed with {len(df_intersect[df_intersect['is_intersectional']])} intersectional cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comprehensive_category_report(df: pd.DataFrame, category_metrics: Dict, \n",
    "                                       advanced_metrics: Dict, wosi_analysis: Dict,\n",
    "                                       optimization_results: Dict, intersectional_results: Dict) -> Dict:\n",
    "    \"\"\"Create comprehensive category analysis report.\"\"\"\n",
    "    \n",
    "    report = {\n",
    "        'analysis_overview': {\n",
    "            'total_responses': len(df),\n",
    "            'total_models': df['model'].nunique(),\n",
    "            'total_categories': df['category'].nunique(),\n",
    "            'categories': sorted(df['category'].unique()),\n",
    "            'analysis_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        },\n",
    "        'category_performance': category_metrics,\n",
    "        'advanced_metrics': {\n",
    "            'wosi_rankings': sorted(advanced_metrics.items(), key=lambda x: x[1]['wosi']),\n",
    "            'wosi_analysis': wosi_analysis,\n",
    "            'category_csss': {model: {cat: metrics['csss'] for cat, metrics in data['category_metrics'].items()}\n",
    "                             for model, data in advanced_metrics.items()}\n",
    "        },\n",
    "        'optimization_analysis': {\n",
    "            'current_weights': category_weights,\n",
    "            'optimization_results': optimization_results,\n",
    "            'recommended_weights': optimization_results.get('discrimination', {}).get('optimal_weights', category_weights)\n",
    "        },\n",
    "        'intersectional_analysis': intersectional_results,\n",
    "        'key_findings': [],\n",
    "        'recommendations': []\n",
    "    }\n",
    "    \n",
    "    # Generate key findings\n",
    "    findings = []\n",
    "    \n",
    "    # Category performance findings\n",
    "    worst_category = min(category_metrics.items(), key=lambda x: x[1]['binary_agreement'])\n",
    "    best_category = max(category_metrics.items(), key=lambda x: x[1]['binary_agreement'])\n",
    "    \n",
    "    findings.append(f\"📊 {worst_category[0]} shows lowest human-model agreement ({worst_category[1]['binary_agreement']:.1%})\")\n",
    "    findings.append(f\"✅ {best_category[0]} shows highest human-model agreement ({best_category[1]['binary_agreement']:.1%})\")\n",
    "    \n",
    "    # WOSI findings\n",
    "    best_wosi_model = min(advanced_metrics.items(), key=lambda x: x[1]['wosi'])\n",
    "    worst_wosi_model = max(advanced_metrics.items(), key=lambda x: x[1]['wosi'])\n",
    "    \n",
    "    findings.append(f\"🏆 {best_wosi_model[0]} achieved best WOSI score ({best_wosi_model[1]['wosi']:.3f})\")\n",
    "    findings.append(f\"⚠️ {worst_wosi_model[0]} needs improvement (WOSI: {worst_wosi_model[1]['wosi']:.3f})\")\n",
    "    \n",
    "    # Weight optimization findings\n",
    "    if optimization_results.get('discrimination', {}).get('success', False):\n",
    "        findings.append(\"🔧 Weight optimization successful - improved model discrimination\")\n",
    "    \n",
    "    # Intersectional findings\n",
    "    if 'intersectional_comparison' in intersectional_results:\n",
    "        intersect_comp = intersectional_results['intersectional_comparison']\n",
    "        if intersect_comp['significant']:\n",
    "            direction = \"higher\" if intersect_comp['difference'] > 0 else \"lower\"\n",
    "            findings.append(f\"🔍 Intersectional bias is significantly {direction} than single-category bias\")\n",
    "    \n",
    "    report['key_findings'] = findings\n",
    "    \n",
    "    # Generate recommendations\n",
    "    recommendations = []\n",
    "    \n",
    "    # Category-specific recommendations\n",
    "    for category, metrics in category_metrics.items():\n",
    "        if metrics['binary_agreement'] < 0.7:\n",
    "            recommendations.append(f\"🎯 {category}: Improve evaluation methods - low agreement ({metrics['binary_agreement']:.1%})\")\n",
    "        \n",
    "        if metrics['stereotype_rate'] > 0.6:\n",
    "            recommendations.append(f\"⚠️ {category}: High stereotype rate ({metrics['stereotype_rate']:.1%}) - needs attention\")\n",
    "    \n",
    "    # Model-specific recommendations\n",
    "    for model, data in advanced_metrics.items():\n",
    "        if data['wosi'] > 0.4:\n",
    "            recommendations.append(f\"🔄 {model}: High WOSI score ({data['wosi']:.3f}) - implement bias reduction\")\n",
    "    \n",
    "    # Weight recommendations\n",
    "    if optimization_results.get('discrimination', {}).get('success', False):\n",
    "        recommendations.append(\"📊 Consider adopting optimized category weights for better model discrimination\")\n",
    "    \n",
    "    # Intersectional recommendations\n",
    "    if intersectional_results.get('amplification_factors'):\n",
    "        high_amp_categories = [cat for cat, stats in intersectional_results['amplification_factors'].items() \n",
    "                              if stats['amplification_factor'] > 1.5]\n",
    "        if high_amp_categories:\n",
    "            recommendations.append(f\"🔍 Focus on intersectional evaluation for: {', '.join(high_amp_categories)}\")\n",
    "    \n",
    "    # Data quality recommendations\n",
    "    total_responses = sum(metrics['total_responses'] for metrics in category_metrics.values())\n",
    "    avg_per_category = total_responses / len(category_metrics)\n",
    "    \n",
    "    small_categories = [cat for cat, metrics in category_metrics.items() \n",
    "                       if metrics['total_responses'] < avg_per_category * 0.5]\n",
    "    if small_categories:\n",
    "        recommendations.append(f\"📈 Increase data collection for under-represented categories: {', '.join(small_categories)}\")\n",
    "    \n",
    "    report['recommendations'] = recommendations\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Create comprehensive report\n",
    "category_report = create_comprehensive_category_report(\n",
    "    df, category_metrics, advanced_metrics, wosi_analysis, \n",
    "    optimization_results, intersectional_results\n",
    ")\n",
    "\n",
    "print(\"=== COMPREHENSIVE CATEGORY ANALYSIS REPORT ===\")\n",
    "print(f\"Generated: {category_report['analysis_overview']['analysis_date']}\")\n",
    "\n",
    "print(f\"\\n📊 ANALYSIS OVERVIEW\")\n",
    "overview = category_report['analysis_overview']\n",
    "print(f\"Total responses: {overview['total_responses']}\")\n",
    "print(f\"Models analyzed: {overview['total_models']}\")\n",
    "print(f\"Categories analyzed: {overview['total_categories']}\")\n",
    "print(f\"Categories: {', '.join(overview['categories'])}\")\n",
    "\n",
    "print(f\"\\n🔍 KEY FINDINGS\")\n",
    "for finding in category_report['key_findings']:\n",
    "    print(f\"• {finding}\")\n",
    "\n",
    "print(f\"\\n💡 RECOMMENDATIONS\")\n",
    "for rec in category_report['recommendations']:\n",
    "    print(f\"• {rec}\")\n",
    "\n",
    "print(f\"\\n🏆 TOP PERFORMERS\")\n",
    "wosi_rankings = category_report['advanced_metrics']['wosi_rankings']\n",
    "print(\"WOSI Rankings (Lower is Better):\")\n",
    "for i, (model, data) in enumerate(wosi_rankings[:3], 1):\n",
    "    print(f\"  {i}. {model}: {data['wosi']:.3f}\")\n",
    "\n",
    "print(f\"\\n📈 CATEGORY PERFORMANCE SUMMARY\")\n",
    "for category, metrics in category_report['category_performance'].items():\n",
    "    print(f\"{category}: SR={metrics['stereotype_rate']:.1%}, CSSS={metrics['csss']:.2f}, Agreement={metrics['binary_agreement']:.1%}\")\n",
    "\n",
    "# Export comprehensive data\n",
    "export_data = {\n",
    "    'category_analysis': df.to_dict('records'),\n",
    "    'category_metrics': category_metrics,\n",
    "    'advanced_metrics': advanced_metrics,\n",
    "    'wosi_analysis': wosi_analysis,\n",
    "    'optimization_results': optimization_results,\n",
    "    'intersectional_results': intersectional_results\n",
    "}\n",
    "\n",
    "# Save main analysis data\n",
    "df.to_csv('../data/category_analysis_data.csv', index=False)\n",
    "print(f\"\\n✅ Category analysis data exported to ../data/category_analysis_data.csv\")\n",
    "\n",
    "# Save comprehensive report\n",
    "with open('../data/category_analysis_report.json', 'w') as f:\n",
    "    json.dump(category_report, f, indent=2, default=str)\n",
    "print(f\"✅ Comprehensive report saved to ../data/category_analysis_report.json\")\n",
    "\n",
    "# Save detailed analysis data\n",
    "with open('../data/category_analysis_detailed.json', 'w') as f:\n",
    "    json.dump(export_data, f, indent=2, default=str)\n",
    "print(f\"✅ Detailed analysis data saved to ../data/category_analysis_detailed.json\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"BIAS CATEGORIES ANALYSIS COMPLETE\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook provided a comprehensive deep-dive analysis of bias categories in the StereoWipe benchmark, including:\n",
    "\n",
    "### Key Analyses Performed:\n",
    "\n",
    "1. **Category-wise Performance Analysis**: Detailed evaluation of stereotype rates, severity scores, and human agreement across all bias categories\n",
    "\n",
    "2. **CSSS and WOSI Deep Dive**: Advanced metrics analysis including:\n",
    "   - Conditional Stereotype Severity Score (CSSS) calculation and interpretation\n",
    "   - Weighted Overall Stereotyping Index (WOSI) computation and rankings\n",
    "   - Component analysis showing the contribution of different factors\n",
    "\n",
    "3. **Cross-category Statistical Analysis**: Rigorous statistical testing including:\n",
    "   - Chi-square tests for stereotype rate differences\n",
    "   - Kruskal-Wallis tests for rating distributions\n",
    "   - Pairwise Mann-Whitney U tests for specific category comparisons\n",
    "\n",
    "4. **Category Weight Optimization**: Systematic optimization of category weights for:\n",
    "   - Model discrimination maximization\n",
    "   - Human alignment improvement\n",
    "   - Stability enhancement\n",
    "   - Sensitivity analysis\n",
    "\n",
    "5. **Intersectional Analysis**: Exploration of bias amplification when multiple categories intersect, including:\n",
    "   - Intersectional vs. single-category bias comparison\n",
    "   - Category pair analysis\n",
    "   - Model performance on intersectional cases\n",
    "   - Amplification factor calculations\n",
    "\n",
    "6. **Comprehensive Visualization**: Rich visualizations showing:\n",
    "   - Heatmaps of performance across models and categories\n",
    "   - WOSI component breakdowns\n",
    "   - Category correlation matrices\n",
    "   - Intersectional bias patterns\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "- **Category Performance Varies**: Different bias categories show distinct patterns in terms of stereotype rates, severity, and human-model agreement\n",
    "- **WOSI Provides Comprehensive Ranking**: The weighted index effectively combines multiple factors for holistic model evaluation\n",
    "- **Weight Optimization Improves Discrimination**: Systematic optimization can enhance the benchmark's ability to distinguish between models\n",
    "- **Intersectional Bias is Amplified**: Multiple bias categories often combine to create stronger stereotypical responses\n",
    "- **Model Consistency Varies**: Some models perform consistently across categories while others show high variance\n",
    "\n",
    "### Practical Implications:\n",
    "\n",
    "1. **Targeted Improvement**: Models can focus on specific categories where they perform poorly\n",
    "2. **Evaluation Refinement**: Category weights can be optimized based on specific objectives\n",
    "3. **Intersectional Awareness**: Evaluation should include intersectional cases for comprehensive assessment\n",
    "4. **Data Collection Priorities**: Under-represented categories need more data collection\n",
    "5. **Metric Selection**: CSSS and WOSI provide complementary insights to basic stereotype rates\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Implementation**: Apply optimized weights and recommendations to improve the benchmark\n",
    "2. **Validation**: Test findings with additional data and real-world applications\n",
    "3. **Publication**: Use comprehensive analysis for research papers and presentations\n",
    "4. **Continuous Monitoring**: Set up ongoing category-specific performance tracking\n",
    "5. **Intersectional Expansion**: Develop more sophisticated intersectional evaluation methods\n",
    "\n",
    "This analysis provides a solid foundation for understanding and improving bias evaluation across different categories, enabling more nuanced and effective stereotype detection in language models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}