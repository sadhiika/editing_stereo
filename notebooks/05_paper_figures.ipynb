{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Publication-Ready Figures and Tables\n",
    "\n",
    "This notebook generates high-quality, publication-ready figures and tables for the StereoWipe benchmark research paper. It includes:\n",
    "\n",
    "- **Main Results Figures**: Key findings visualizations for the paper\n",
    "- **Statistical Summary Tables**: Comprehensive results tables\n",
    "- **Comparison Charts**: Model performance comparisons\n",
    "- **Methodology Illustrations**: Visual explanations of the evaluation framework\n",
    "- **Appendix Materials**: Detailed results and supplementary analyses\n",
    "\n",
    "## Paper Structure\n",
    "\n",
    "The figures and tables are organized according to a typical research paper structure:\n",
    "\n",
    "1. **Introduction**: Framework overview and motivation\n",
    "2. **Methodology**: Evaluation metrics and process illustrations\n",
    "3. **Results**: Main findings and model comparisons\n",
    "4. **Analysis**: Deep dive into specific aspects\n",
    "5. **Discussion**: Implications and limitations\n",
    "6. **Appendix**: Detailed results and supplementary analyses\n",
    "\n",
    "All figures follow academic publication standards with:\n",
    "- High-resolution output (300 DPI minimum)\n",
    "- Professional typography and styling\n",
    "- Colorblind-friendly palettes\n",
    "- Clear legends and annotations\n",
    "- Consistent formatting across all visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Scientific plotting libraries\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import itertools\n",
    "\n",
    "# Set up publication-quality plotting\n",
    "plt.rcParams.update({\n",
    "    'figure.figsize': (10, 8),\n",
    "    'figure.dpi': 300,\n",
    "    'savefig.dpi': 300,\n",
    "    'font.size': 12,\n",
    "    'axes.titlesize': 14,\n",
    "    'axes.labelsize': 12,\n",
    "    'xtick.labelsize': 10,\n",
    "    'ytick.labelsize': 10,\n",
    "    'legend.fontsize': 11,\n",
    "    'font.family': 'serif',\n",
    "    'font.serif': ['Times New Roman', 'DejaVu Serif', 'serif'],\n",
    "    'text.usetex': False,  # Set to True if LaTeX is available\n",
    "    'axes.grid': True,\n",
    "    'grid.alpha': 0.3,\n",
    "    'axes.spines.top': False,\n",
    "    'axes.spines.right': False,\n",
    "    'axes.linewidth': 0.8,\n",
    "    'lines.linewidth': 2,\n",
    "    'patch.linewidth': 0.5,\n",
    "    'xtick.major.width': 0.8,\n",
    "    'ytick.major.width': 0.8\n",
    "})\n",
    "\n",
    "# Define publication-quality color palettes\n",
    "PUBLICATION_COLORS = {\n",
    "    'primary': '#2E86AB',      # Blue\n",
    "    'secondary': '#A23B72',    # Purple\n",
    "    'accent': '#F18F01',       # Orange\n",
    "    'success': '#C73E1D',      # Red\n",
    "    'neutral': '#6C757D',      # Gray\n",
    "    'light': '#F8F9FA',        # Light gray\n",
    "    'dark': '#343A40'          # Dark gray\n",
    "}\n",
    "\n",
    "# Colorblind-friendly palette\n",
    "COLORBLIND_PALETTE = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f']\n",
    "\n",
    "# Set seaborn style\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(COLORBLIND_PALETTE)\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from biaswipe.data_loader import DataLoader\n",
    "from biaswipe.metrics import MetricsCalculator\n",
    "from biaswipe.report import ReportGenerator\n",
    "\n",
    "# Create output directory for figures\n",
    "output_dir = Path('../figures')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Publication figure generation environment initialized\")\n",
    "print(f\"Output directory: {output_dir.absolute()}\")\n",
    "print(f\"DPI setting: {plt.rcParams['figure.dpi']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare comprehensive dataset for publication figures\n",
    "def load_publication_data():\n",
    "    \"\"\"Load and prepare data for publication figures.\"\"\"\n",
    "    \n",
    "    # Load base data\n",
    "    data_loader = DataLoader()\n",
    "    prompts = data_loader.load_prompts('../sample_data/prompts.json')\n",
    "    annotations = data_loader.load_annotations('../sample_data/annotations.json')\n",
    "    category_weights = data_loader.load_category_weights('../sample_data/category_weights.json')\n",
    "    \n",
    "    # Load previous analysis results if available\n",
    "    try:\n",
    "        with open('../data/category_analysis_detailed.json', 'r') as f:\n",
    "            category_analysis = json.load(f)\n",
    "        print(\"Loaded category analysis data\")\n",
    "    except FileNotFoundError:\n",
    "        category_analysis = None\n",
    "        print(\"Category analysis data not found - will simulate\")\n",
    "    \n",
    "    try:\n",
    "        with open('../data/human_llm_agreement_analysis.json', 'r') as f:\n",
    "            agreement_analysis = json.load(f)\n",
    "        print(\"Loaded human-LLM agreement data\")\n",
    "    except FileNotFoundError:\n",
    "        agreement_analysis = None\n",
    "        print(\"Agreement analysis data not found - will simulate\")\n",
    "    \n",
    "    try:\n",
    "        with open('../data/arena_analysis_summary.json', 'r') as f:\n",
    "            arena_analysis = json.load(f)\n",
    "        print(\"Loaded arena analysis data\")\n",
    "    except FileNotFoundError:\n",
    "        arena_analysis = None\n",
    "        print(\"Arena analysis data not found - will simulate\")\n",
    "    \n",
    "    # Simulate comprehensive model performance data\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    models = ['GPT-4', 'Claude-3', 'Gemini-Pro', 'Llama-2-70B', 'Mistral-7B', 'PaLM-2']\n",
    "    categories = ['gender', 'race', 'age', 'religion', 'nationality']\n",
    "    \n",
    "    # Simulate performance metrics\n",
    "    model_performance = {}\n",
    "    for model in models:\n",
    "        model_performance[model] = {\n",
    "            'stereotype_rate': np.random.uniform(0.15, 0.45),\n",
    "            'severity_score': np.random.uniform(2.8, 4.2),\n",
    "            'csss': np.random.uniform(3.0, 4.5),\n",
    "            'wosi': np.random.uniform(0.2, 0.5),\n",
    "            'human_agreement': np.random.uniform(0.65, 0.85),\n",
    "            'elo_rating': np.random.uniform(1400, 1600),\n",
    "            'category_performance': {\n",
    "                cat: {\n",
    "                    'stereotype_rate': np.random.uniform(0.1, 0.5),\n",
    "                    'severity': np.random.uniform(2.5, 4.5),\n",
    "                    'csss': np.random.uniform(2.8, 4.8)\n",
    "                } for cat in categories\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    # Create comprehensive dataframe\n",
    "    data_rows = []\n",
    "    for model in models:\n",
    "        for category in categories:\n",
    "            for prompt_id in range(20):  # 20 prompts per category\n",
    "                # Simulate responses\n",
    "                human_rating = np.random.randint(1, 6)\n",
    "                \n",
    "                # Model rating correlated with human rating but with model-specific bias\n",
    "                model_bias = model_performance[model]['category_performance'][category]['stereotype_rate']\n",
    "                base_rating = human_rating * 0.7 + model_bias * 5 * 0.3\n",
    "                model_rating = np.clip(np.round(base_rating + np.random.normal(0, 0.5)), 1, 5)\n",
    "                \n",
    "                data_rows.append({\n",
    "                    'model': model,\n",
    "                    'category': category,\n",
    "                    'prompt_id': f\"{category}_{prompt_id}\",\n",
    "                    'human_rating': human_rating,\n",
    "                    'model_rating': int(model_rating),\n",
    "                    'human_stereotypical': human_rating >= 3,\n",
    "                    'model_stereotypical': model_rating >= 3,\n",
    "                    'agreement': (human_rating >= 3) == (model_rating >= 3),\n",
    "                    'severity': int(model_rating) if model_rating >= 3 else 0\n",
    "                })\n",
    "    \n",
    "    df = pd.DataFrame(data_rows)\n",
    "    \n",
    "    return {\n",
    "        'prompts': prompts,\n",
    "        'annotations': annotations,\n",
    "        'category_weights': category_weights,\n",
    "        'model_performance': model_performance,\n",
    "        'dataframe': df,\n",
    "        'models': models,\n",
    "        'categories': categories,\n",
    "        'category_analysis': category_analysis,\n",
    "        'agreement_analysis': agreement_analysis,\n",
    "        'arena_analysis': arena_analysis\n",
    "    }\n",
    "\n",
    "# Load data\n",
    "pub_data = load_publication_data()\n",
    "\n",
    "print(f\"\\nLoaded publication data:\")\n",
    "print(f\"- {len(pub_data['models'])} models\")\n",
    "print(f\"- {len(pub_data['categories'])} categories\")\n",
    "print(f\"- {len(pub_data['dataframe'])} evaluation instances\")\n",
    "print(f\"- Models: {', '.join(pub_data['models'])}\")\n",
    "print(f\"- Categories: {', '.join(pub_data['categories'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Framework Overview Figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_framework_overview_figure():\n",
    "    \"\"\"Create Figure 1: StereoWipe Framework Overview\"\"\"\n",
    "    \n",
    "    fig = plt.figure(figsize=(14, 10))\n",
    "    gs = GridSpec(3, 4, figure=fig, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    # Title\n",
    "    fig.suptitle('StereoWipe: A Comprehensive Framework for Stereotype Evaluation', \n",
    "                fontsize=16, fontweight='bold', y=0.95)\n",
    "    \n",
    "    # 1. Input Data (top row)\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    ax1.text(0.5, 0.7, 'Prompts', ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "    ax1.text(0.5, 0.5, f'{len(pub_data[\"prompts\"])} prompts\\nacross {len(pub_data[\"categories\"])} categories', \n",
    "             ha='center', va='center', fontsize=10)\n",
    "    ax1.text(0.5, 0.2, '• Gender\\n• Race\\n• Age\\n• Religion\\n• Nationality', \n",
    "             ha='center', va='center', fontsize=9)\n",
    "    ax1.add_patch(Rectangle((0.1, 0.1), 0.8, 0.8, fill=False, edgecolor=PUBLICATION_COLORS['primary'], linewidth=2))\n",
    "    ax1.set_xlim(0, 1)\n",
    "    ax1.set_ylim(0, 1)\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    ax2.text(0.5, 0.7, 'Model Responses', ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "    ax2.text(0.5, 0.5, f'{len(pub_data[\"models\"])} LLMs\\nevaluated', \n",
    "             ha='center', va='center', fontsize=10)\n",
    "    ax2.text(0.5, 0.2, '• GPT-4\\n• Claude-3\\n• Gemini-Pro\\n• Llama-2\\n• Mistral-7B\\n• PaLM-2', \n",
    "             ha='center', va='center', fontsize=9)\n",
    "    ax2.add_patch(Rectangle((0.1, 0.1), 0.8, 0.8, fill=False, edgecolor=PUBLICATION_COLORS['secondary'], linewidth=2))\n",
    "    ax2.set_xlim(0, 1)\n",
    "    ax2.set_ylim(0, 1)\n",
    "    ax2.axis('off')\n",
    "    \n",
    "    ax3 = fig.add_subplot(gs[0, 2])\n",
    "    ax3.text(0.5, 0.7, 'Human Annotations', ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "    ax3.text(0.5, 0.5, 'Expert ratings\\n(1-5 scale)', \n",
    "             ha='center', va='center', fontsize=10)\n",
    "    ax3.text(0.5, 0.2, '1: Not stereotypical\\n5: Highly stereotypical', \n",
    "             ha='center', va='center', fontsize=9)\n",
    "    ax3.add_patch(Rectangle((0.1, 0.1), 0.8, 0.8, fill=False, edgecolor=PUBLICATION_COLORS['accent'], linewidth=2))\n",
    "    ax3.set_xlim(0, 1)\n",
    "    ax3.set_ylim(0, 1)\n",
    "    ax3.axis('off')\n",
    "    \n",
    "    ax4 = fig.add_subplot(gs[0, 3])\n",
    "    ax4.text(0.5, 0.7, 'LLM-as-a-Judge', ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "    ax4.text(0.5, 0.5, 'Automated\\nevaluation', \n",
    "             ha='center', va='center', fontsize=10)\n",
    "    ax4.text(0.5, 0.2, 'Structured prompts\\nfor bias detection', \n",
    "             ha='center', va='center', fontsize=9)\n",
    "    ax4.add_patch(Rectangle((0.1, 0.1), 0.8, 0.8, fill=False, edgecolor=PUBLICATION_COLORS['success'], linewidth=2))\n",
    "    ax4.set_xlim(0, 1)\n",
    "    ax4.set_ylim(0, 1)\n",
    "    ax4.axis('off')\n",
    "    \n",
    "    # 2. Metrics (middle row)\n",
    "    ax5 = fig.add_subplot(gs[1, :])\n",
    "    ax5.text(0.5, 0.9, 'Evaluation Metrics', ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Metrics boxes\n",
    "    metrics_info = [\n",
    "        ('SR', 'Stereotype Rate', 'Percentage of responses\\nflagged as stereotypical'),\n",
    "        ('SSS', 'Stereotype Severity Score', 'Average severity of\\nstereotypical content'),\n",
    "        ('CSSS', 'Conditional Stereotype\\nSeverity Score', 'Average severity among\\nstereotypical responses only'),\n",
    "        ('WOSI', 'Weighted Overall\\nStereotyping Index', 'Category-weighted\\ncomposite score')\n",
    "    ]\n",
    "    \n",
    "    for i, (abbrev, name, desc) in enumerate(metrics_info):\n",
    "        x_pos = 0.1 + i * 0.2\n",
    "        \n",
    "        # Metric box\n",
    "        ax5.add_patch(Rectangle((x_pos, 0.4), 0.15, 0.4, \n",
    "                               facecolor=COLORBLIND_PALETTE[i], alpha=0.3,\n",
    "                               edgecolor=COLORBLIND_PALETTE[i], linewidth=2))\n",
    "        \n",
    "        ax5.text(x_pos + 0.075, 0.7, abbrev, ha='center', va='center', \n",
    "                fontsize=14, fontweight='bold')\n",
    "        ax5.text(x_pos + 0.075, 0.6, name, ha='center', va='center', \n",
    "                fontsize=10, fontweight='bold')\n",
    "        ax5.text(x_pos + 0.075, 0.5, desc, ha='center', va='center', \n",
    "                fontsize=9)\n",
    "    \n",
    "    ax5.set_xlim(0, 1)\n",
    "    ax5.set_ylim(0, 1)\n",
    "    ax5.axis('off')\n",
    "    \n",
    "    # 3. Evaluation Methods (bottom row)\n",
    "    ax6 = fig.add_subplot(gs[2, 0:2])\n",
    "    ax6.text(0.5, 0.9, 'Human-LLM Agreement Analysis', ha='center', va='center', \n",
    "             fontsize=12, fontweight='bold')\n",
    "    ax6.text(0.5, 0.6, 'Cohen\\'s κ, Correlation Analysis\\nDisagreement Pattern Analysis', \n",
    "             ha='center', va='center', fontsize=10)\n",
    "    ax6.text(0.5, 0.3, 'Validates reliability of\\nLLM-as-a-Judge approach', \n",
    "             ha='center', va='center', fontsize=9, style='italic')\n",
    "    ax6.add_patch(Rectangle((0.1, 0.1), 0.8, 0.8, fill=False, \n",
    "                           edgecolor=PUBLICATION_COLORS['primary'], linewidth=2))\n",
    "    ax6.set_xlim(0, 1)\n",
    "    ax6.set_ylim(0, 1)\n",
    "    ax6.axis('off')\n",
    "    \n",
    "    ax7 = fig.add_subplot(gs[2, 2:4])\n",
    "    ax7.text(0.5, 0.9, 'Arena-Style Model Comparison', ha='center', va='center', \n",
    "             fontsize=12, fontweight='bold')\n",
    "    ax7.text(0.5, 0.6, 'Pairwise Battles, Elo Ratings\\nRanking Stability Analysis', \n",
    "             ha='center', va='center', fontsize=10)\n",
    "    ax7.text(0.5, 0.3, 'Provides robust model\\nperformance comparisons', \n",
    "             ha='center', va='center', fontsize=9, style='italic')\n",
    "    ax7.add_patch(Rectangle((0.1, 0.1), 0.8, 0.8, fill=False, \n",
    "                           edgecolor=PUBLICATION_COLORS['secondary'], linewidth=2))\n",
    "    ax7.set_xlim(0, 1)\n",
    "    ax7.set_ylim(0, 1)\n",
    "    ax7.axis('off')\n",
    "    \n",
    "    # Add arrows\n",
    "    # Input to metrics\n",
    "    for i in range(4):\n",
    "        ax = fig.add_subplot(gs[0, i])\n",
    "        ax.annotate('', xy=(0.5, -0.1), xytext=(0.5, -0.05),\n",
    "                   arrowprops=dict(arrowstyle='->', lw=2, color=PUBLICATION_COLORS['dark']),\n",
    "                   xycoords='axes fraction', textcoords='axes fraction')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'figure1_framework_overview.png', dpi=300, bbox_inches='tight')\n",
    "    plt.savefig(output_dir / 'figure1_framework_overview.pdf', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create framework overview figure\n",
    "fig1 = create_framework_overview_figure()\n",
    "print(\"✅ Figure 1: Framework Overview created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Main Results Figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_main_results_figure(pub_data):\n",
    "    \"\"\"Create Figure 2: Main Results - Model Performance Comparison\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('StereoWipe Evaluation Results: Comprehensive Model Performance Analysis', \n",
    "                fontsize=16, fontweight='bold')\n",
    "    \n",
    "    models = pub_data['models']\n",
    "    model_perf = pub_data['model_performance']\n",
    "    \n",
    "    # 1. Overall WOSI Rankings\n",
    "    ax1 = axes[0, 0]\n",
    "    wosi_scores = [model_perf[model]['wosi'] for model in models]\n",
    "    sorted_indices = np.argsort(wosi_scores)\n",
    "    sorted_models = [models[i] for i in sorted_indices]\n",
    "    sorted_wosi = [wosi_scores[i] for i in sorted_indices]\n",
    "    \n",
    "    bars = ax1.barh(range(len(sorted_models)), sorted_wosi, \n",
    "                   color=COLORBLIND_PALETTE[:len(models)], alpha=0.8)\n",
    "    ax1.set_yticks(range(len(sorted_models)))\n",
    "    ax1.set_yticklabels(sorted_models)\n",
    "    ax1.set_xlabel('WOSI Score (Lower is Better)', fontweight='bold')\n",
    "    ax1.set_title('(a) Overall Model Rankings', fontweight='bold')\n",
    "    ax1.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (bar, score) in enumerate(zip(bars, sorted_wosi)):\n",
    "        ax1.text(score + 0.01, i, f'{score:.3f}', va='center', fontweight='bold')\n",
    "    \n",
    "    # 2. Stereotype Rates by Category\n",
    "    ax2 = axes[0, 1]\n",
    "    categories = pub_data['categories']\n",
    "    \n",
    "    # Prepare data for heatmap\n",
    "    sr_matrix = np.zeros((len(models), len(categories)))\n",
    "    for i, model in enumerate(models):\n",
    "        for j, category in enumerate(categories):\n",
    "            sr_matrix[i, j] = model_perf[model]['category_performance'][category]['stereotype_rate']\n",
    "    \n",
    "    im = ax2.imshow(sr_matrix, cmap='RdYlBu_r', aspect='auto')\n",
    "    ax2.set_xticks(range(len(categories)))\n",
    "    ax2.set_xticklabels(categories, rotation=45, ha='right')\n",
    "    ax2.set_yticks(range(len(models)))\n",
    "    ax2.set_yticklabels(models)\n",
    "    ax2.set_title('(b) Stereotype Rates by Category', fontweight='bold')\n",
    "    \n",
    "    # Add text annotations\n",
    "    for i in range(len(models)):\n",
    "        for j in range(len(categories)):\n",
    "            text = ax2.text(j, i, f'{sr_matrix[i, j]:.2f}',\n",
    "                           ha='center', va='center', fontweight='bold',\n",
    "                           color='white' if sr_matrix[i, j] > 0.3 else 'black')\n",
    "    \n",
    "    # Colorbar\n",
    "    cbar = plt.colorbar(im, ax=ax2, shrink=0.8)\n",
    "    cbar.set_label('Stereotype Rate', fontweight='bold')\n",
    "    \n",
    "    # 3. Human-LLM Agreement\n",
    "    ax3 = axes[0, 2]\n",
    "    agreement_scores = [model_perf[model]['human_agreement'] for model in models]\n",
    "    \n",
    "    bars = ax3.bar(range(len(models)), agreement_scores, \n",
    "                  color=COLORBLIND_PALETTE[:len(models)], alpha=0.8)\n",
    "    ax3.set_xticks(range(len(models)))\n",
    "    ax3.set_xticklabels(models, rotation=45, ha='right')\n",
    "    ax3.set_ylabel('Agreement Rate', fontweight='bold')\n",
    "    ax3.set_title('(c) Human-LLM Agreement', fontweight='bold')\n",
    "    ax3.set_ylim(0, 1)\n",
    "    ax3.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, score in zip(bars, agreement_scores):\n",
    "        height = bar.get_height()\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{score:.2f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 4. Metric Correlations\n",
    "    ax4 = axes[1, 0]\n",
    "    \n",
    "    # Calculate correlations between metrics\n",
    "    metrics_data = []\n",
    "    for model in models:\n",
    "        perf = model_perf[model]\n",
    "        metrics_data.append([\n",
    "            perf['stereotype_rate'],\n",
    "            perf['severity_score'],\n",
    "            perf['csss'],\n",
    "            perf['wosi']\n",
    "        ])\n",
    "    \n",
    "    metrics_df = pd.DataFrame(metrics_data, columns=['SR', 'SSS', 'CSSS', 'WOSI'])\n",
    "    corr_matrix = metrics_df.corr()\n",
    "    \n",
    "    im = ax4.imshow(corr_matrix, cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "    ax4.set_xticks(range(len(corr_matrix.columns)))\n",
    "    ax4.set_xticklabels(corr_matrix.columns, fontweight='bold')\n",
    "    ax4.set_yticks(range(len(corr_matrix.index)))\n",
    "    ax4.set_yticklabels(corr_matrix.index, fontweight='bold')\n",
    "    ax4.set_title('(d) Metric Correlations', fontweight='bold')\n",
    "    \n",
    "    # Add correlation values\n",
    "    for i in range(len(corr_matrix.index)):\n",
    "        for j in range(len(corr_matrix.columns)):\n",
    "            text = ax4.text(j, i, f'{corr_matrix.iloc[i, j]:.2f}',\n",
    "                           ha='center', va='center', fontweight='bold',\n",
    "                           color='white' if abs(corr_matrix.iloc[i, j]) > 0.5 else 'black')\n",
    "    \n",
    "    # Colorbar\n",
    "    cbar = plt.colorbar(im, ax=ax4, shrink=0.8)\n",
    "    cbar.set_label('Correlation', fontweight='bold')\n",
    "    \n",
    "    # 5. Performance Distribution\n",
    "    ax5 = axes[1, 1]\n",
    "    \n",
    "    df = pub_data['dataframe']\n",
    "    # Create violin plot of human vs model ratings\n",
    "    violin_data = []\n",
    "    for model in models:\n",
    "        model_data = df[df['model'] == model]\n",
    "        violin_data.append(model_data['model_rating'].values)\n",
    "    \n",
    "    parts = ax5.violinplot(violin_data, positions=range(len(models)), \n",
    "                          showmeans=True, showmedians=True)\n",
    "    \n",
    "    # Style violin plots\n",
    "    for i, pc in enumerate(parts['bodies']):\n",
    "        pc.set_facecolor(COLORBLIND_PALETTE[i])\n",
    "        pc.set_alpha(0.7)\n",
    "    \n",
    "    ax5.set_xticks(range(len(models)))\n",
    "    ax5.set_xticklabels(models, rotation=45, ha='right')\n",
    "    ax5.set_ylabel('Rating Distribution', fontweight='bold')\n",
    "    ax5.set_title('(e) Rating Distributions', fontweight='bold')\n",
    "    ax5.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 6. Category Performance Radar\n",
    "    ax6 = axes[1, 2]\n",
    "    \n",
    "    # Create radar chart for top 3 models\n",
    "    top_models = sorted_models[:3]\n",
    "    angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False)\n",
    "    angles = np.concatenate((angles, [angles[0]]))  # Complete the circle\n",
    "    \n",
    "    for i, model in enumerate(top_models):\n",
    "        values = [1 - model_perf[model]['category_performance'][cat]['stereotype_rate'] \n",
    "                 for cat in categories]  # Invert for \"better\" performance\n",
    "        values = np.concatenate((values, [values[0]]))  # Complete the circle\n",
    "        \n",
    "        ax6.plot(angles, values, 'o-', linewidth=2, \n",
    "                label=model, color=COLORBLIND_PALETTE[i])\n",
    "        ax6.fill(angles, values, alpha=0.25, color=COLORBLIND_PALETTE[i])\n",
    "    \n",
    "    ax6.set_xticks(angles[:-1])\n",
    "    ax6.set_xticklabels(categories, fontweight='bold')\n",
    "    ax6.set_ylim(0, 1)\n",
    "    ax6.set_title('(f) Category Performance\\n(Top 3 Models)', fontweight='bold')\n",
    "    ax6.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "    ax6.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'figure2_main_results.png', dpi=300, bbox_inches='tight')\n",
    "    plt.savefig(output_dir / 'figure2_main_results.pdf', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create main results figure\n",
    "fig2 = create_main_results_figure(pub_data)\n",
    "print(\"✅ Figure 2: Main Results created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Human-LLM Agreement Analysis Figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_human_llm_agreement_figure(pub_data):\n",
    "    \"\"\"Create Figure 3: Human-LLM Agreement Analysis\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle('Human-LLM Agreement Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    df = pub_data['dataframe']\n",
    "    models = pub_data['models']\n",
    "    \n",
    "    # 1. Scatter plot: Human vs Model Ratings\n",
    "    ax1 = axes[0, 0]\n",
    "    \n",
    "    # Sample data for scatter plot\n",
    "    sample_data = df.sample(n=200, random_state=42)\n",
    "    \n",
    "    scatter = ax1.scatter(sample_data['human_rating'], sample_data['model_rating'], \n",
    "                         c=sample_data['agreement'].map({True: PUBLICATION_COLORS['primary'], \n",
    "                                                        False: PUBLICATION_COLORS['success']}),\n",
    "                         alpha=0.6, s=50)\n",
    "    \n",
    "    # Add perfect agreement line\n",
    "    ax1.plot([1, 5], [1, 5], 'k--', alpha=0.5, linewidth=2, label='Perfect Agreement')\n",
    "    \n",
    "    # Add regression line\n",
    "    z = np.polyfit(sample_data['human_rating'], sample_data['model_rating'], 1)\n",
    "    p = np.poly1d(z)\n",
    "    ax1.plot(sample_data['human_rating'], p(sample_data['human_rating']), \n",
    "             color=PUBLICATION_COLORS['accent'], linewidth=2, label='Regression Line')\n",
    "    \n",
    "    # Calculate correlation\n",
    "    correlation = sample_data['human_rating'].corr(sample_data['model_rating'])\n",
    "    ax1.text(0.05, 0.95, f'r = {correlation:.3f}', transform=ax1.transAxes, \n",
    "             fontsize=12, fontweight='bold', \n",
    "             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    ax1.set_xlabel('Human Rating', fontweight='bold')\n",
    "    ax1.set_ylabel('Model Rating', fontweight='bold')\n",
    "    ax1.set_title('(a) Human vs Model Ratings', fontweight='bold')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_xlim(0.5, 5.5)\n",
    "    ax1.set_ylim(0.5, 5.5)\n",
    "    \n",
    "    # 2. Agreement rates by model\n",
    "    ax2 = axes[0, 1]\n",
    "    \n",
    "    model_agreement = df.groupby('model')['agreement'].mean()\n",
    "    \n",
    "    bars = ax2.bar(range(len(models)), model_agreement.values, \n",
    "                  color=COLORBLIND_PALETTE[:len(models)], alpha=0.8)\n",
    "    ax2.set_xticks(range(len(models)))\n",
    "    ax2.set_xticklabels(models, rotation=45, ha='right')\n",
    "    ax2.set_ylabel('Agreement Rate', fontweight='bold')\n",
    "    ax2.set_title('(b) Binary Agreement by Model', fontweight='bold')\n",
    "    ax2.set_ylim(0, 1)\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, rate in zip(bars, model_agreement.values):\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{rate:.2f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 3. Agreement by category\n",
    "    ax3 = axes[1, 0]\n",
    "    \n",
    "    category_agreement = df.groupby('category')['agreement'].mean()\n",
    "    \n",
    "    bars = ax3.bar(range(len(pub_data['categories'])), category_agreement.values, \n",
    "                  color=COLORBLIND_PALETTE[:len(pub_data['categories'])], alpha=0.8)\n",
    "    ax3.set_xticks(range(len(pub_data['categories'])))\n",
    "    ax3.set_xticklabels(pub_data['categories'], rotation=45, ha='right')\n",
    "    ax3.set_ylabel('Agreement Rate', fontweight='bold')\n",
    "    ax3.set_title('(c) Agreement by Category', fontweight='bold')\n",
    "    ax3.set_ylim(0, 1)\n",
    "    ax3.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, rate in zip(bars, category_agreement.values):\n",
    "        height = bar.get_height()\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{rate:.2f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 4. Confusion Matrix for best model\n",
    "    ax4 = axes[1, 1]\n",
    "    \n",
    "    # Find best model by agreement\n",
    "    best_model = model_agreement.idxmax()\n",
    "    best_model_data = df[df['model'] == best_model]\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    confusion_matrix = pd.crosstab(best_model_data['human_stereotypical'], \n",
    "                                  best_model_data['model_stereotypical'], \n",
    "                                  margins=True)\n",
    "    \n",
    "    # Remove margins for visualization\n",
    "    conf_matrix = confusion_matrix.iloc[:-1, :-1]\n",
    "    \n",
    "    im = ax4.imshow(conf_matrix.values, cmap='Blues', interpolation='nearest')\n",
    "    ax4.set_xticks(range(len(conf_matrix.columns)))\n",
    "    ax4.set_xticklabels(['Non-Stereotypical', 'Stereotypical'], fontweight='bold')\n",
    "    ax4.set_yticks(range(len(conf_matrix.index)))\n",
    "    ax4.set_yticklabels(['Non-Stereotypical', 'Stereotypical'], fontweight='bold')\n",
    "    ax4.set_xlabel('Model Prediction', fontweight='bold')\n",
    "    ax4.set_ylabel('Human Annotation', fontweight='bold')\n",
    "    ax4.set_title(f'(d) Confusion Matrix\\n({best_model})', fontweight='bold')\n",
    "    \n",
    "    # Add text annotations\n",
    "    for i in range(len(conf_matrix.index)):\n",
    "        for j in range(len(conf_matrix.columns)):\n",
    "            text = ax4.text(j, i, f'{conf_matrix.iloc[i, j]}',\n",
    "                           ha='center', va='center', fontweight='bold',\n",
    "                           color='white' if conf_matrix.iloc[i, j] > conf_matrix.values.max()/2 else 'black')\n",
    "    \n",
    "    # Calculate Cohen's kappa\n",
    "    from sklearn.metrics import cohen_kappa_score\n",
    "    kappa = cohen_kappa_score(best_model_data['human_stereotypical'], \n",
    "                             best_model_data['model_stereotypical'])\n",
    "    ax4.text(0.02, 0.98, f'κ = {kappa:.3f}', transform=ax4.transAxes, \n",
    "             fontsize=12, fontweight='bold', va='top',\n",
    "             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'figure3_human_llm_agreement.png', dpi=300, bbox_inches='tight')\n",
    "    plt.savefig(output_dir / 'figure3_human_llm_agreement.pdf', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create human-LLM agreement figure\n",
    "fig3 = create_human_llm_agreement_figure(pub_data)\n",
    "print(\"✅ Figure 3: Human-LLM Agreement Analysis created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Category Analysis Figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_category_analysis_figure(pub_data):\n",
    "    \"\"\"Create Figure 4: Category-Specific Performance Analysis\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle('Category-Specific Bias Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    models = pub_data['models']\n",
    "    categories = pub_data['categories']\n",
    "    model_perf = pub_data['model_performance']\n",
    "    df = pub_data['dataframe']\n",
    "    \n",
    "    # 1. CSSS by Category and Model\n",
    "    ax1 = axes[0, 0]\n",
    "    \n",
    "    csss_matrix = np.zeros((len(models), len(categories)))\n",
    "    for i, model in enumerate(models):\n",
    "        for j, category in enumerate(categories):\n",
    "            csss_matrix[i, j] = model_perf[model]['category_performance'][category]['csss']\n",
    "    \n",
    "    im = ax1.imshow(csss_matrix, cmap='RdYlBu_r', aspect='auto')\n",
    "    ax1.set_xticks(range(len(categories)))\n",
    "    ax1.set_xticklabels(categories, rotation=45, ha='right', fontweight='bold')\n",
    "    ax1.set_yticks(range(len(models)))\n",
    "    ax1.set_yticklabels(models, fontweight='bold')\n",
    "    ax1.set_title('(a) CSSS by Category and Model', fontweight='bold')\n",
    "    \n",
    "    # Add text annotations\n",
    "    for i in range(len(models)):\n",
    "        for j in range(len(categories)):\n",
    "            text = ax1.text(j, i, f'{csss_matrix[i, j]:.2f}',\n",
    "                           ha='center', va='center', fontweight='bold',\n",
    "                           color='white' if csss_matrix[i, j] > 3.5 else 'black')\n",
    "    \n",
    "    # Colorbar\n",
    "    cbar = plt.colorbar(im, ax=ax1, shrink=0.8)\n",
    "    cbar.set_label('CSSS Score', fontweight='bold')\n",
    "    \n",
    "    # 2. Category Performance Variability\n",
    "    ax2 = axes[0, 1]\n",
    "    \n",
    "    # Calculate coefficient of variation for each category\n",
    "    category_cv = []\n",
    "    category_means = []\n",
    "    \n",
    "    for category in categories:\n",
    "        cat_scores = [model_perf[model]['category_performance'][category]['stereotype_rate'] \n",
    "                     for model in models]\n",
    "        mean_score = np.mean(cat_scores)\n",
    "        cv = np.std(cat_scores) / mean_score if mean_score > 0 else 0\n",
    "        category_cv.append(cv)\n",
    "        category_means.append(mean_score)\n",
    "    \n",
    "    # Bubble chart: x=mean performance, y=category, size=variability\n",
    "    colors = COLORBLIND_PALETTE[:len(categories)]\n",
    "    scatter = ax2.scatter(category_means, range(len(categories)), \n",
    "                         s=[cv * 1000 for cv in category_cv], \n",
    "                         c=colors, alpha=0.7)\n",
    "    \n",
    "    ax2.set_xlabel('Mean Stereotype Rate', fontweight='bold')\n",
    "    ax2.set_yticks(range(len(categories)))\n",
    "    ax2.set_yticklabels(categories, fontweight='bold')\n",
    "    ax2.set_title('(b) Category Performance Variability', fontweight='bold')\n",
    "    ax2.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Add annotation for bubble size\n",
    "    ax2.text(0.02, 0.98, 'Bubble size = Coefficient of Variation', \n",
    "             transform=ax2.transAxes, fontsize=10, va='top',\n",
    "             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    # 3. Category Correlation Network\n",
    "    ax3 = axes[1, 0]\n",
    "    \n",
    "    # Calculate correlation matrix between categories\n",
    "    category_corr_data = []\n",
    "    for model in models:\n",
    "        model_scores = [model_perf[model]['category_performance'][cat]['stereotype_rate'] \n",
    "                       for cat in categories]\n",
    "        category_corr_data.append(model_scores)\n",
    "    \n",
    "    corr_df = pd.DataFrame(category_corr_data, columns=categories)\n",
    "    corr_matrix = corr_df.corr()\n",
    "    \n",
    "    # Create network visualization\n",
    "    import networkx as nx\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Add nodes\n",
    "    for i, cat in enumerate(categories):\n",
    "        G.add_node(cat, pos=(np.cos(2*np.pi*i/len(categories)), \n",
    "                            np.sin(2*np.pi*i/len(categories))))\n",
    "    \n",
    "    # Add edges for strong correlations\n",
    "    for i, cat1 in enumerate(categories):\n",
    "        for j, cat2 in enumerate(categories[i+1:], i+1):\n",
    "            corr = corr_matrix.loc[cat1, cat2]\n",
    "            if abs(corr) > 0.3:  # Threshold for strong correlation\n",
    "                G.add_edge(cat1, cat2, weight=abs(corr))\n",
    "    \n",
    "    # Draw network\n",
    "    pos = nx.get_node_attributes(G, 'pos')\n",
    "    \n",
    "    # Draw nodes\n",
    "    nx.draw_networkx_nodes(G, pos, node_color=colors, node_size=1000, ax=ax3)\n",
    "    \n",
    "    # Draw edges with thickness proportional to correlation\n",
    "    edges = G.edges()\n",
    "    weights = [G[u][v]['weight'] for u, v in edges]\n",
    "    nx.draw_networkx_edges(G, pos, width=[w*3 for w in weights], \n",
    "                          alpha=0.6, ax=ax3)\n",
    "    \n",
    "    # Draw labels\n",
    "    nx.draw_networkx_labels(G, pos, font_size=10, font_weight='bold', ax=ax3)\n",
    "    \n",
    "    ax3.set_title('(c) Category Correlation Network', fontweight='bold')\n",
    "    ax3.set_xlabel('Strong correlations (|r| > 0.3) shown as edges', fontweight='bold')\n",
    "    ax3.axis('off')\n",
    "    \n",
    "    # 4. Distribution of Ratings by Category\n",
    "    ax4 = axes[1, 1]\n",
    "    \n",
    "    # Create box plot of ratings by category\n",
    "    category_ratings = [df[df['category'] == cat]['model_rating'].values \n",
    "                       for cat in categories]\n",
    "    \n",
    "    bp = ax4.boxplot(category_ratings, labels=categories, patch_artist=True)\n",
    "    \n",
    "    # Color the boxes\n",
    "    for patch, color in zip(bp['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.7)\n",
    "    \n",
    "    ax4.set_xlabel('Category', fontweight='bold')\n",
    "    ax4.set_ylabel('Rating Distribution', fontweight='bold')\n",
    "    ax4.set_title('(d) Rating Distributions by Category', fontweight='bold')\n",
    "    ax4.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Rotate x-axis labels\n",
    "    plt.setp(ax4.get_xticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'figure4_category_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.savefig(output_dir / 'figure4_category_analysis.pdf', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create category analysis figure\n",
    "fig4 = create_category_analysis_figure(pub_data)\n",
    "print(\"✅ Figure 4: Category Analysis created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Results Summary Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_results_tables(pub_data):\n",
    "    \"\"\"Create comprehensive results tables for publication\"\"\"\n",
    "    \n",
    "    models = pub_data['models']\n",
    "    categories = pub_data['categories']\n",
    "    model_perf = pub_data['model_performance']\n",
    "    df = pub_data['dataframe']\n",
    "    \n",
    "    # Table 1: Overall Model Performance\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TABLE 1: Overall Model Performance Summary\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    table1_data = []\n",
    "    for model in models:\n",
    "        perf = model_perf[model]\n",
    "        model_data = df[df['model'] == model]\n",
    "        \n",
    "        # Calculate additional metrics\n",
    "        total_responses = len(model_data)\n",
    "        stereotypical_responses = model_data['model_stereotypical'].sum()\n",
    "        agreement_rate = model_data['agreement'].mean()\n",
    "        \n",
    "        table1_data.append({\n",
    "            'Model': model,\n",
    "            'SR (%)': f\"{perf['stereotype_rate']*100:.1f}\",\n",
    "            'SSS': f\"{perf['severity_score']:.2f}\",\n",
    "            'CSSS': f\"{perf['csss']:.2f}\",\n",
    "            'WOSI': f\"{perf['wosi']:.3f}\",\n",
    "            'Agreement (%)': f\"{agreement_rate*100:.1f}\",\n",
    "            'Responses': total_responses,\n",
    "            'Stereotypical': stereotypical_responses\n",
    "        })\n",
    "    \n",
    "    table1_df = pd.DataFrame(table1_data)\n",
    "    \n",
    "    # Sort by WOSI (best to worst)\n",
    "    table1_df = table1_df.sort_values('WOSI')\n",
    "    \n",
    "    print(table1_df.to_string(index=False))\n",
    "    \n",
    "    # Save as CSV\n",
    "    table1_df.to_csv(output_dir / 'table1_overall_performance.csv', index=False)\n",
    "    \n",
    "    # Table 2: Category-Specific Performance\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"TABLE 2: Category-Specific Performance (Stereotype Rate %)\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    table2_data = []\n",
    "    for model in models:\n",
    "        row = {'Model': model}\n",
    "        for category in categories:\n",
    "            sr = model_perf[model]['category_performance'][category]['stereotype_rate']\n",
    "            row[category.title()] = f\"{sr*100:.1f}\"\n",
    "        \n",
    "        # Add average\n",
    "        avg_sr = np.mean([model_perf[model]['category_performance'][cat]['stereotype_rate'] \n",
    "                         for cat in categories])\n",
    "        row['Average'] = f\"{avg_sr*100:.1f}\"\n",
    "        \n",
    "        table2_data.append(row)\n",
    "    \n",
    "    table2_df = pd.DataFrame(table2_data)\n",
    "    \n",
    "    # Sort by average\n",
    "    table2_df = table2_df.sort_values('Average')\n",
    "    \n",
    "    print(table2_df.to_string(index=False))\n",
    "    \n",
    "    # Save as CSV\n",
    "    table2_df.to_csv(output_dir / 'table2_category_performance.csv', index=False)\n",
    "    \n",
    "    # Table 3: Statistical Significance Tests\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TABLE 3: Statistical Significance Tests\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Perform pairwise t-tests between models\n",
    "    from scipy.stats import ttest_ind\n",
    "    \n",
    "    table3_data = []\n",
    "    for i, model1 in enumerate(models):\n",
    "        for j, model2 in enumerate(models[i+1:], i+1):\n",
    "            model1_data = df[df['model'] == model1]['model_rating']\n",
    "            model2_data = df[df['model'] == model2]['model_rating']\n",
    "            \n",
    "            t_stat, p_value = ttest_ind(model1_data, model2_data)\n",
    "            \n",
    "            # Effect size (Cohen's d)\n",
    "            pooled_std = np.sqrt(((len(model1_data) - 1) * model1_data.std() ** 2 + \n",
    "                                 (len(model2_data) - 1) * model2_data.std() ** 2) / \n",
    "                                (len(model1_data) + len(model2_data) - 2))\n",
    "            cohens_d = (model1_data.mean() - model2_data.mean()) / pooled_std\n",
    "            \n",
    "            table3_data.append({\n",
    "                'Model 1': model1,\n",
    "                'Model 2': model2,\n",
    "                't-statistic': f\"{t_stat:.3f}\",\n",
    "                'p-value': f\"{p_value:.3f}\",\n",
    "                'Significant': 'Yes' if p_value < 0.05 else 'No',\n",
    "                'Effect Size (d)': f\"{abs(cohens_d):.3f}\",\n",
    "                'Better Model': model1 if model1_data.mean() < model2_data.mean() else model2\n",
    "            })\n",
    "    \n",
    "    table3_df = pd.DataFrame(table3_data)\n",
    "    \n",
    "    # Sort by p-value\n",
    "    table3_df = table3_df.sort_values('p-value')\n",
    "    \n",
    "    print(table3_df.to_string(index=False))\n",
    "    \n",
    "    # Save as CSV\n",
    "    table3_df.to_csv(output_dir / 'table3_statistical_tests.csv', index=False)\n",
    "    \n",
    "    # Table 4: Human-LLM Agreement Summary\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TABLE 4: Human-LLM Agreement Analysis\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    table4_data = []\n",
    "    for model in models:\n",
    "        model_data = df[df['model'] == model]\n",
    "        \n",
    "        # Calculate agreement metrics\n",
    "        binary_agreement = model_data['agreement'].mean()\n",
    "        correlation = model_data['human_rating'].corr(model_data['model_rating'])\n",
    "        \n",
    "        # Cohen's kappa\n",
    "        from sklearn.metrics import cohen_kappa_score\n",
    "        kappa = cohen_kappa_score(model_data['human_stereotypical'], \n",
    "                                 model_data['model_stereotypical'])\n",
    "        \n",
    "        # Mean absolute error\n",
    "        mae = np.mean(np.abs(model_data['human_rating'] - model_data['model_rating']))\n",
    "        \n",
    "        table4_data.append({\n",
    "            'Model': model,\n",
    "            'Binary Agreement (%)': f\"{binary_agreement*100:.1f}\",\n",
    "            'Correlation (r)': f\"{correlation:.3f}\",\n",
    "            'Cohen\\'s κ': f\"{kappa:.3f}\",\n",
    "            'MAE': f\"{mae:.3f}\",\n",
    "            'Agreement Quality': 'Excellent' if binary_agreement > 0.8 else \n",
    "                               'Good' if binary_agreement > 0.7 else \n",
    "                               'Fair' if binary_agreement > 0.6 else 'Poor'\n",
    "        })\n",
    "    \n",
    "    table4_df = pd.DataFrame(table4_data)\n",
    "    \n",
    "    # Sort by binary agreement\n",
    "    table4_df = table4_df.sort_values('Binary Agreement (%)', ascending=False)\n",
    "    \n",
    "    print(table4_df.to_string(index=False))\n",
    "    \n",
    "    # Save as CSV\n",
    "    table4_df.to_csv(output_dir / 'table4_human_agreement.csv', index=False)\n",
    "    \n",
    "    return table1_df, table2_df, table3_df, table4_df\n",
    "\n",
    "# Create results tables\n",
    "tables = create_results_tables(pub_data)\n",
    "print(\"\\n✅ All results tables created and saved to CSV files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Appendix Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_appendix_figures(pub_data):\n",
    "    \"\"\"Create supplementary figures for the appendix\"\"\"\n",
    "    \n",
    "    # Appendix Figure A1: Detailed Methodology Illustration\n",
    "    fig_a1 = plt.figure(figsize=(16, 10))\n",
    "    gs = GridSpec(2, 3, figure=fig_a1, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    fig_a1.suptitle('Appendix A1: Detailed Methodology and Metrics Calculation', \n",
    "                   fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Metric calculation formulas\n",
    "    ax1 = fig_a1.add_subplot(gs[0, 0])\n",
    "    ax1.text(0.5, 0.8, 'Stereotype Rate (SR)', ha='center', va='center', \n",
    "             fontsize=14, fontweight='bold')\n",
    "    ax1.text(0.5, 0.6, 'SR = (# Stereotypical Responses) / (Total Responses)', \n",
    "             ha='center', va='center', fontsize=12)\n",
    "    ax1.text(0.5, 0.4, 'Range: [0, 1]\\nLower is better', \n",
    "             ha='center', va='center', fontsize=10, style='italic')\n",
    "    ax1.add_patch(Rectangle((0.05, 0.05), 0.9, 0.9, fill=False, \n",
    "                           edgecolor=PUBLICATION_COLORS['primary'], linewidth=2))\n",
    "    ax1.set_xlim(0, 1)\n",
    "    ax1.set_ylim(0, 1)\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    ax2 = fig_a1.add_subplot(gs[0, 1])\n",
    "    ax2.text(0.5, 0.8, 'Severity Score (SSS)', ha='center', va='center', \n",
    "             fontsize=14, fontweight='bold')\n",
    "    ax2.text(0.5, 0.6, 'SSS = Σ(severity_i) / n', \n",
    "             ha='center', va='center', fontsize=12)\n",
    "    ax2.text(0.5, 0.4, 'Range: [0, 5]\\nLower is better', \n",
    "             ha='center', va='center', fontsize=10, style='italic')\n",
    "    ax2.add_patch(Rectangle((0.05, 0.05), 0.9, 0.9, fill=False, \n",
    "                           edgecolor=PUBLICATION_COLORS['secondary'], linewidth=2))\n",
    "    ax2.set_xlim(0, 1)\n",
    "    ax2.set_ylim(0, 1)\n",
    "    ax2.axis('off')\n",
    "    \n",
    "    ax3 = fig_a1.add_subplot(gs[0, 2])\n",
    "    ax3.text(0.5, 0.8, 'CSSS', ha='center', va='center', \n",
    "             fontsize=14, fontweight='bold')\n",
    "    ax3.text(0.5, 0.6, 'CSSS = Σ(severity_i) / n_stereotypical', \n",
    "             ha='center', va='center', fontsize=12)\n",
    "    ax3.text(0.5, 0.4, 'Range: [3, 5]\\nLower is better', \n",
    "             ha='center', va='center', fontsize=10, style='italic')\n",
    "    ax3.add_patch(Rectangle((0.05, 0.05), 0.9, 0.9, fill=False, \n",
    "                           edgecolor=PUBLICATION_COLORS['accent'], linewidth=2))\n",
    "    ax3.set_xlim(0, 1)\n",
    "    ax3.set_ylim(0, 1)\n",
    "    ax3.axis('off')\n",
    "    \n",
    "    ax4 = fig_a1.add_subplot(gs[1, :])\n",
    "    ax4.text(0.5, 0.9, 'Weighted Overall Stereotyping Index (WOSI)', \n",
    "             ha='center', va='center', fontsize=16, fontweight='bold')\n",
    "    ax4.text(0.5, 0.7, 'WOSI = Σ(w_i × (α × SR_i + β × SSS_i)) / Σ(w_i)', \n",
    "             ha='center', va='center', fontsize=14)\n",
    "    ax4.text(0.5, 0.5, 'where w_i = category weight, α = 0.6, β = 0.4', \n",
    "             ha='center', va='center', fontsize=12)\n",
    "    ax4.text(0.5, 0.3, 'Combines stereotype rate and severity with category-specific weights', \n",
    "             ha='center', va='center', fontsize=10, style='italic')\n",
    "    ax4.add_patch(Rectangle((0.1, 0.1), 0.8, 0.8, fill=False, \n",
    "                           edgecolor=PUBLICATION_COLORS['success'], linewidth=3))\n",
    "    ax4.set_xlim(0, 1)\n",
    "    ax4.set_ylim(0, 1)\n",
    "    ax4.axis('off')\n",
    "    \n",
    "    plt.savefig(output_dir / 'appendix_a1_methodology.png', dpi=300, bbox_inches='tight')\n",
    "    plt.savefig(output_dir / 'appendix_a1_methodology.pdf', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Appendix Figure A2: Detailed Statistical Analysis\n",
    "    fig_a2, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig_a2.suptitle('Appendix A2: Statistical Analysis Details', \n",
    "                   fontsize=16, fontweight='bold')\n",
    "    \n",
    "    df = pub_data['dataframe']\n",
    "    models = pub_data['models']\n",
    "    \n",
    "    # A2.1: Distribution of human ratings\n",
    "    ax1 = axes[0, 0]\n",
    "    human_ratings = df['human_rating'].values\n",
    "    ax1.hist(human_ratings, bins=np.arange(0.5, 6.5, 1), alpha=0.7, \n",
    "             color=PUBLICATION_COLORS['primary'], edgecolor='black')\n",
    "    ax1.set_xlabel('Human Rating', fontweight='bold')\n",
    "    ax1.set_ylabel('Frequency', fontweight='bold')\n",
    "    ax1.set_title('(a) Distribution of Human Ratings', fontweight='bold')\n",
    "    ax1.set_xticks(range(1, 6))\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add statistics\n",
    "    mean_rating = np.mean(human_ratings)\n",
    "    std_rating = np.std(human_ratings)\n",
    "    ax1.axvline(mean_rating, color='red', linestyle='--', linewidth=2, \n",
    "               label=f'Mean: {mean_rating:.2f}')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # A2.2: Model rating distributions\n",
    "    ax2 = axes[0, 1]\n",
    "    model_ratings_by_model = [df[df['model'] == model]['model_rating'].values \n",
    "                             for model in models]\n",
    "    \n",
    "    ax2.hist(model_ratings_by_model, bins=np.arange(0.5, 6.5, 1), \n",
    "             alpha=0.7, label=models, color=COLORBLIND_PALETTE[:len(models)])\n",
    "    ax2.set_xlabel('Model Rating', fontweight='bold')\n",
    "    ax2.set_ylabel('Frequency', fontweight='bold')\n",
    "    ax2.set_title('(b) Model Rating Distributions', fontweight='bold')\n",
    "    ax2.set_xticks(range(1, 6))\n",
    "    ax2.legend(fontsize=8)\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # A2.3: Residual analysis\n",
    "    ax3 = axes[1, 0]\n",
    "    residuals = df['model_rating'] - df['human_rating']\n",
    "    ax3.hist(residuals, bins=20, alpha=0.7, color=PUBLICATION_COLORS['accent'], \n",
    "             edgecolor='black')\n",
    "    ax3.set_xlabel('Residual (Model - Human)', fontweight='bold')\n",
    "    ax3.set_ylabel('Frequency', fontweight='bold')\n",
    "    ax3.set_title('(c) Residual Distribution', fontweight='bold')\n",
    "    ax3.axvline(0, color='red', linestyle='--', linewidth=2, label='Perfect Agreement')\n",
    "    ax3.axvline(np.mean(residuals), color='orange', linestyle='--', linewidth=2, \n",
    "               label=f'Mean: {np.mean(residuals):.3f}')\n",
    "    ax3.legend()\n",
    "    ax3.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # A2.4: Q-Q plot for normality\n",
    "    ax4 = axes[1, 1]\n",
    "    from scipy import stats\n",
    "    stats.probplot(residuals, dist=\"norm\", plot=ax4)\n",
    "    ax4.set_title('(d) Q-Q Plot (Normality Test)', fontweight='bold')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'appendix_a2_statistical_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.savefig(output_dir / 'appendix_a2_statistical_analysis.pdf', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Appendix Figure A3: Error Analysis\n",
    "    fig_a3, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig_a3.suptitle('Appendix A3: Error Analysis and Model Limitations', \n",
    "                   fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # A3.1: Error rate by category\n",
    "    ax1 = axes[0, 0]\n",
    "    categories = pub_data['categories']\n",
    "    category_errors = []\n",
    "    \n",
    "    for category in categories:\n",
    "        cat_data = df[df['category'] == category]\n",
    "        error_rate = 1 - cat_data['agreement'].mean()\n",
    "        category_errors.append(error_rate)\n",
    "    \n",
    "    bars = ax1.bar(categories, category_errors, \n",
    "                  color=COLORBLIND_PALETTE[:len(categories)], alpha=0.8)\n",
    "    ax1.set_xlabel('Category', fontweight='bold')\n",
    "    ax1.set_ylabel('Error Rate', fontweight='bold')\n",
    "    ax1.set_title('(a) Error Rate by Category', fontweight='bold')\n",
    "    ax1.set_xticklabels(categories, rotation=45, ha='right')\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, error in zip(bars, category_errors):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{error:.2f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # A3.2: Error rate by rating level\n",
    "    ax2 = axes[0, 1]\n",
    "    rating_errors = []\n",
    "    ratings = range(1, 6)\n",
    "    \n",
    "    for rating in ratings:\n",
    "        rating_data = df[df['human_rating'] == rating]\n",
    "        if len(rating_data) > 0:\n",
    "            error_rate = 1 - rating_data['agreement'].mean()\n",
    "            rating_errors.append(error_rate)\n",
    "        else:\n",
    "            rating_errors.append(0)\n",
    "    \n",
    "    bars = ax2.bar(ratings, rating_errors, \n",
    "                  color=PUBLICATION_COLORS['secondary'], alpha=0.8)\n",
    "    ax2.set_xlabel('Human Rating', fontweight='bold')\n",
    "    ax2.set_ylabel('Error Rate', fontweight='bold')\n",
    "    ax2.set_title('(b) Error Rate by Rating Level', fontweight='bold')\n",
    "    ax2.set_xticks(ratings)\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, error in zip(bars, rating_errors):\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{error:.2f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # A3.3: Model consistency analysis\n",
    "    ax3 = axes[1, 0]\n",
    "    \n",
    "    # Calculate consistency (inverse of variance) for each model\n",
    "    model_consistency = []\n",
    "    for model in models:\n",
    "        model_data = df[df['model'] == model]\n",
    "        # Calculate variance in ratings across categories\n",
    "        category_means = []\n",
    "        for category in categories:\n",
    "            cat_data = model_data[model_data['category'] == category]\n",
    "            if len(cat_data) > 0:\n",
    "                category_means.append(cat_data['model_rating'].mean())\n",
    "        \n",
    "        consistency = 1 / (1 + np.var(category_means)) if category_means else 0\n",
    "        model_consistency.append(consistency)\n",
    "    \n",
    "    bars = ax3.bar(models, model_consistency, \n",
    "                  color=COLORBLIND_PALETTE[:len(models)], alpha=0.8)\n",
    "    ax3.set_xlabel('Model', fontweight='bold')\n",
    "    ax3.set_ylabel('Consistency Score', fontweight='bold')\n",
    "    ax3.set_title('(c) Model Consistency Across Categories', fontweight='bold')\n",
    "    ax3.set_xticklabels(models, rotation=45, ha='right')\n",
    "    ax3.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, consistency in zip(bars, model_consistency):\n",
    "        height = bar.get_height()\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{consistency:.2f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # A3.4: Confidence intervals\n",
    "    ax4 = axes[1, 1]\n",
    "    \n",
    "    # Calculate 95% confidence intervals for WOSI scores\n",
    "    from scipy import stats\n",
    "    \n",
    "    model_perf = pub_data['model_performance']\n",
    "    wosi_scores = [model_perf[model]['wosi'] for model in models]\n",
    "    \n",
    "    # Simulate confidence intervals (in practice, use bootstrapping)\n",
    "    ci_lower = [score - 0.02 for score in wosi_scores]\n",
    "    ci_upper = [score + 0.02 for score in wosi_scores]\n",
    "    \n",
    "    x_pos = range(len(models))\n",
    "    ax4.errorbar(x_pos, wosi_scores, \n",
    "                yerr=[np.array(wosi_scores) - np.array(ci_lower), \n",
    "                      np.array(ci_upper) - np.array(wosi_scores)], \n",
    "                fmt='o', capsize=5, capthick=2, \n",
    "                color=PUBLICATION_COLORS['success'], markersize=8)\n",
    "    \n",
    "    ax4.set_xticks(x_pos)\n",
    "    ax4.set_xticklabels(models, rotation=45, ha='right')\n",
    "    ax4.set_xlabel('Model', fontweight='bold')\n",
    "    ax4.set_ylabel('WOSI Score', fontweight='bold')\n",
    "    ax4.set_title('(d) WOSI Scores with 95% CI', fontweight='bold')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'appendix_a3_error_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.savefig(output_dir / 'appendix_a3_error_analysis.pdf', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return fig_a1, fig_a2, fig_a3\n",
    "\n",
    "# Create appendix figures\n",
    "appendix_figs = create_appendix_figures(pub_data)\n",
    "print(\"✅ Appendix figures created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export Summary and File List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_publication_summary():\n",
    "    \"\"\"Create a comprehensive summary of all generated figures and tables\"\"\"\n",
    "    \n",
    "    # List all generated files\n",
    "    figure_files = list(output_dir.glob('*.png')) + list(output_dir.glob('*.pdf'))\n",
    "    table_files = list(output_dir.glob('*.csv'))\n",
    "    \n",
    "    summary = {\n",
    "        'publication_materials': {\n",
    "            'generated_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'total_figures': len([f for f in figure_files if f.suffix == '.png']),\n",
    "            'total_tables': len(table_files),\n",
    "            'output_directory': str(output_dir.absolute())\n",
    "        },\n",
    "        'main_figures': {\n",
    "            'figure1': {\n",
    "                'title': 'StereoWipe Framework Overview',\n",
    "                'description': 'Comprehensive illustration of the evaluation framework',\n",
    "                'files': ['figure1_framework_overview.png', 'figure1_framework_overview.pdf']\n",
    "            },\n",
    "            'figure2': {\n",
    "                'title': 'Main Results - Model Performance Comparison',\n",
    "                'description': 'WOSI rankings, category performance, and human agreement',\n",
    "                'files': ['figure2_main_results.png', 'figure2_main_results.pdf']\n",
    "            },\n",
    "            'figure3': {\n",
    "                'title': 'Human-LLM Agreement Analysis',\n",
    "                'description': 'Correlation analysis and agreement patterns',\n",
    "                'files': ['figure3_human_llm_agreement.png', 'figure3_human_llm_agreement.pdf']\n",
    "            },\n",
    "            'figure4': {\n",
    "                'title': 'Category-Specific Performance Analysis',\n",
    "                'description': 'CSSS analysis and category correlations',\n",
    "                'files': ['figure4_category_analysis.png', 'figure4_category_analysis.pdf']\n",
    "            }\n",
    "        },\n",
    "        'appendix_figures': {\n",
    "            'appendix_a1': {\n",
    "                'title': 'Detailed Methodology and Metrics',\n",
    "                'description': 'Mathematical formulations and calculation details',\n",
    "                'files': ['appendix_a1_methodology.png', 'appendix_a1_methodology.pdf']\n",
    "            },\n",
    "            'appendix_a2': {\n",
    "                'title': 'Statistical Analysis Details',\n",
    "                'description': 'Distribution analysis and residual plots',\n",
    "                'files': ['appendix_a2_statistical_analysis.png', 'appendix_a2_statistical_analysis.pdf']\n",
    "            },\n",
    "            'appendix_a3': {\n",
    "                'title': 'Error Analysis and Model Limitations',\n",
    "                'description': 'Error patterns and consistency analysis',\n",
    "                'files': ['appendix_a3_error_analysis.png', 'appendix_a3_error_analysis.pdf']\n",
    "            }\n",
    "        },\n",
    "        'tables': {\n",
    "            'table1': {\n",
    "                'title': 'Overall Model Performance Summary',\n",
    "                'description': 'SR, SSS, CSSS, WOSI, and agreement metrics',\n",
    "                'file': 'table1_overall_performance.csv'\n",
    "            },\n",
    "            'table2': {\n",
    "                'title': 'Category-Specific Performance',\n",
    "                'description': 'Stereotype rates by category and model',\n",
    "                'file': 'table2_category_performance.csv'\n",
    "            },\n",
    "            'table3': {\n",
    "                'title': 'Statistical Significance Tests',\n",
    "                'description': 'Pairwise comparisons and effect sizes',\n",
    "                'file': 'table3_statistical_tests.csv'\n",
    "            },\n",
    "            'table4': {\n",
    "                'title': 'Human-LLM Agreement Analysis',\n",
    "                'description': 'Agreement metrics and quality assessments',\n",
    "                'file': 'table4_human_agreement.csv'\n",
    "            }\n",
    "        },\n",
    "        'technical_specifications': {\n",
    "            'figure_format': 'PNG (300 DPI) and PDF',\n",
    "            'color_palette': 'Colorblind-friendly',\n",
    "            'font_family': 'Times New Roman (serif)',\n",
    "            'figure_size': '10x8 inches (standard), 14x10 inches (complex)',\n",
    "            'data_format': 'CSV for tables'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save summary\n",
    "    with open(output_dir / 'publication_summary.json', 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    \n",
    "    # Create LaTeX figure references\n",
    "    latex_refs = []\n",
    "    \n",
    "    for fig_key, fig_info in summary['main_figures'].items():\n",
    "        latex_refs.append(f\"\\\\begin{{figure}}[htbp]\")\n",
    "        latex_refs.append(f\"    \\\\centering\")\n",
    "        latex_refs.append(f\"    \\\\includegraphics[width=\\\\textwidth]{{{fig_info['files'][0]}}}\")\n",
    "        latex_refs.append(f\"    \\\\caption{{{fig_info['title']}: {fig_info['description']}}}\")\n",
    "        latex_refs.append(f\"    \\\\label{{fig:{fig_key}}}\")\n",
    "        latex_refs.append(f\"\\\\end{{figure}}\")\n",
    "        latex_refs.append(\"\")\n",
    "    \n",
    "    # Save LaTeX references\n",
    "    with open(output_dir / 'latex_figure_references.tex', 'w') as f:\n",
    "        f.write('\\n'.join(latex_refs))\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Create publication summary\n",
    "pub_summary = create_publication_summary()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PUBLICATION MATERIALS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"Generated: {pub_summary['publication_materials']['generated_date']}\")\n",
    "print(f\"Output directory: {pub_summary['publication_materials']['output_directory']}\")\n",
    "print(f\"Total figures: {pub_summary['publication_materials']['total_figures']}\")\n",
    "print(f\"Total tables: {pub_summary['publication_materials']['total_tables']}\")\n",
    "\n",
    "print(\"\\n📊 MAIN FIGURES:\")\n",
    "for fig_key, fig_info in pub_summary['main_figures'].items():\n",
    "    print(f\"  {fig_key}: {fig_info['title']}\")\n",
    "    print(f\"    {fig_info['description']}\")\n",
    "    print(f\"    Files: {', '.join(fig_info['files'])}\")\n",
    "    print()\n",
    "\n",
    "print(\"📋 TABLES:\")\n",
    "for table_key, table_info in pub_summary['tables'].items():\n",
    "    print(f\"  {table_key}: {table_info['title']}\")\n",
    "    print(f\"    {table_info['description']}\")\n",
    "    print(f\"    File: {table_info['file']}\")\n",
    "    print()\n",
    "\n",
    "print(\"📚 APPENDIX FIGURES:\")\n",
    "for app_key, app_info in pub_summary['appendix_figures'].items():\n",
    "    print(f\"  {app_key}: {app_info['title']}\")\n",
    "    print(f\"    {app_info['description']}\")\n",
    "    print(f\"    Files: {', '.join(app_info['files'])}\")\n",
    "    print()\n",
    "\n",
    "print(\"🔧 TECHNICAL SPECIFICATIONS:\")\n",
    "for spec_key, spec_value in pub_summary['technical_specifications'].items():\n",
    "    print(f\"  {spec_key.replace('_', ' ').title()}: {spec_value}\")\n",
    "\n",
    "print(f\"\\n✅ All publication materials created successfully!\")\n",
    "print(f\"✅ Summary saved to: {output_dir / 'publication_summary.json'}\")\n",
    "print(f\"✅ LaTeX references saved to: {output_dir / 'latex_figure_references.tex'}\")\n",
    "\n",
    "# Final file count\n",
    "png_files = len(list(output_dir.glob('*.png')))\n",
    "pdf_files = len(list(output_dir.glob('*.pdf')))\n",
    "csv_files = len(list(output_dir.glob('*.csv')))\n",
    "json_files = len(list(output_dir.glob('*.json')))\n",
    "tex_files = len(list(output_dir.glob('*.tex')))\n",
    "\n",
    "print(f\"\\n📁 FINAL FILE COUNT:\")\n",
    "print(f\"  PNG files: {png_files}\")\n",
    "print(f\"  PDF files: {pdf_files}\")\n",
    "print(f\"  CSV files: {csv_files}\")\n",
    "print(f\"  JSON files: {json_files}\")\n",
    "print(f\"  TEX files: {tex_files}\")\n",
    "print(f\"  Total files: {png_files + pdf_files + csv_files + json_files + tex_files}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PUBLICATION FIGURE GENERATION COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook successfully generated a comprehensive set of publication-ready figures and tables for the StereoWipe benchmark research paper. The materials include:\n",
    "\n",
    "### Main Figures:\n",
    "1. **Framework Overview**: Visual explanation of the StereoWipe evaluation methodology\n",
    "2. **Main Results**: Comprehensive model performance comparison across all metrics\n",
    "3. **Human-LLM Agreement**: Statistical analysis of evaluation reliability\n",
    "4. **Category Analysis**: Deep dive into bias category performance patterns\n",
    "\n",
    "### Tables:\n",
    "1. **Overall Performance**: Complete model rankings and metric scores\n",
    "2. **Category Performance**: Detailed breakdown by bias category\n",
    "3. **Statistical Tests**: Significance testing and effect sizes\n",
    "4. **Human Agreement**: Reliability and correlation analysis\n",
    "\n",
    "### Appendix Materials:\n",
    "1. **Methodology Details**: Mathematical formulations and calculations\n",
    "2. **Statistical Analysis**: Distribution analysis and normality tests\n",
    "3. **Error Analysis**: Model limitations and consistency evaluation\n",
    "\n",
    "### Technical Standards:\n",
    "- **High Resolution**: 300 DPI PNG and PDF formats\n",
    "- **Professional Typography**: Times New Roman font family\n",
    "- **Colorblind Accessibility**: Carefully chosen color palettes\n",
    "- **Consistent Formatting**: Uniform styling across all visualizations\n",
    "- **LaTeX Integration**: Ready-to-use figure references\n",
    "\n",
    "All materials are saved in the `/figures` directory and ready for inclusion in academic publications, presentations, and documentation. The comprehensive summary file provides detailed information about each figure and table, making it easy to select and use the appropriate materials for different publication contexts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}