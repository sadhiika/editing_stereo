{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arena Analysis: Model Comparison and Elo Ratings\n",
    "\n",
    "This notebook implements arena-style analysis for comparing LLM performance on stereotype evaluation. The analysis includes:\n",
    "\n",
    "- **Arena Battle Simulation**: Pairwise comparisons between models based on stereotype scores\n",
    "- **Elo Rating System**: Dynamic rating system for model ranking\n",
    "- **Preference Patterns**: Analysis of human and judge preferences\n",
    "- **Statistical Significance**: Validation of model performance differences\n",
    "- **Visualization**: Comprehensive charts and rankings\n",
    "\n",
    "## Background\n",
    "\n",
    "Arena analysis provides a robust framework for comparing models by:\n",
    "- Avoiding direct scoring biases through pairwise comparisons\n",
    "- Providing interpretable rankings through Elo ratings\n",
    "- Enabling statistical testing of performance differences\n",
    "- Supporting both human and automated evaluation\n",
    "\n",
    "The arena approach is particularly valuable for stereotype evaluation where absolute scores may be less meaningful than relative performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy import stats\n",
    "from scipy.stats import binomial, beta, chi2_contingency\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import networkx as nx\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (14, 10)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from biaswipe.data_loader import DataLoader\n",
    "from biaswipe.metrics import MetricsCalculator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation and Model Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate multiple model responses for arena analysis\n",
    "def simulate_model_responses(prompts: Dict, annotations: Dict, n_models: int = 5) -> Dict:\n",
    "    \"\"\"Simulate responses from multiple LLM models with different bias characteristics.\"\"\"\n",
    "    \n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    \n",
    "    # Define model characteristics\n",
    "    models = {\n",
    "        'GPT-4': {'bias_tendency': 0.3, 'variance': 0.8, 'category_effects': {'gender': 0.2, 'race': 0.1}},\n",
    "        'Claude-3': {'bias_tendency': 0.25, 'variance': 0.7, 'category_effects': {'religion': 0.15, 'age': 0.1}},\n",
    "        'Gemini-Pro': {'bias_tendency': 0.35, 'variance': 0.9, 'category_effects': {'nationality': 0.2, 'gender': 0.15}},\n",
    "        'Llama-2': {'bias_tendency': 0.4, 'variance': 1.0, 'category_effects': {'race': 0.25, 'religion': 0.2}},\n",
    "        'Mistral-7B': {'bias_tendency': 0.45, 'variance': 1.1, 'category_effects': {'age': 0.2, 'nationality': 0.15}}\n",
    "    }\n",
    "    \n",
    "    model_responses = {}\n",
    "    \n",
    "    for model_name, model_config in models.items():\n",
    "        model_responses[model_name] = {}\n",
    "        \n",
    "        for prompt_id, prompt in prompts.items():\n",
    "            if prompt_id in annotations:\n",
    "                human_rating = annotations[prompt_id]['human_rating']\n",
    "                category = prompt['category']\n",
    "                \n",
    "                # Base tendency towards bias\n",
    "                base_bias = model_config['bias_tendency']\n",
    "                \n",
    "                # Category-specific effects\n",
    "                category_effect = model_config['category_effects'].get(category, 0)\n",
    "                \n",
    "                # Correlation with human rating\n",
    "                human_influence = 0.6 * (human_rating - 1) / 4  # Normalize to 0-1\n",
    "                \n",
    "                # Generate simulated response rating\n",
    "                noise = np.random.normal(0, model_config['variance'] * 0.3)\n",
    "                \n",
    "                # Combine factors\n",
    "                combined_score = (\n",
    "                    0.4 * base_bias + \n",
    "                    0.3 * category_effect + \n",
    "                    0.5 * human_influence + \n",
    "                    0.2 * noise\n",
    "                )\n",
    "                \n",
    "                # Convert to 1-5 scale\n",
    "                rating = 1 + 4 * np.clip(combined_score, 0, 1)\n",
    "                rating = np.clip(np.round(rating), 1, 5)\n",
    "                \n",
    "                # Calculate stereotype metrics\n",
    "                is_stereotypical = rating >= 3\n",
    "                severity = rating if is_stereotypical else 0\n",
    "                \n",
    "                model_responses[model_name][prompt_id] = {\n",
    "                    'rating': int(rating),\n",
    "                    'is_stereotypical': is_stereotypical,\n",
    "                    'severity': int(severity),\n",
    "                    'category': category\n",
    "                }\n",
    "    \n",
    "    return model_responses\n",
    "\n",
    "# Load data and simulate model responses\n",
    "data_loader = DataLoader()\n",
    "prompts = data_loader.load_prompts('../sample_data/prompts.json')\n",
    "annotations = data_loader.load_annotations('../sample_data/annotations.json')\n",
    "category_weights = data_loader.load_category_weights('../sample_data/category_weights.json')\n",
    "\n",
    "model_responses = simulate_model_responses(prompts, annotations)\n",
    "\n",
    "print(f\"Simulated responses for {len(model_responses)} models\")\n",
    "print(f\"Models: {list(model_responses.keys())}\")\n",
    "print(f\"Prompts per model: {len(list(model_responses.values())[0])}\")\n",
    "\n",
    "# Calculate basic metrics for each model\n",
    "model_metrics = {}\n",
    "for model_name, responses in model_responses.items():\n",
    "    stereotype_rate = sum(1 for r in responses.values() if r['is_stereotypical']) / len(responses)\n",
    "    avg_rating = sum(r['rating'] for r in responses.values()) / len(responses)\n",
    "    avg_severity = sum(r['severity'] for r in responses.values() if r['is_stereotypical']) / max(1, sum(1 for r in responses.values() if r['is_stereotypical']))\n",
    "    \n",
    "    model_metrics[model_name] = {\n",
    "        'stereotype_rate': stereotype_rate,\n",
    "        'avg_rating': avg_rating,\n",
    "        'avg_severity': avg_severity if not np.isnan(avg_severity) else 0\n",
    "    }\n",
    "\n",
    "print(\"\\n=== Model Performance Overview ===\")\n",
    "for model, metrics in model_metrics.items():\n",
    "    print(f\"{model}: SR={metrics['stereotype_rate']:.2%}, Avg={metrics['avg_rating']:.2f}, Severity={metrics['avg_severity']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Arena Battle Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArenaSystem:\n",
    "    \"\"\"Arena system for pairwise model comparisons with Elo ratings.\"\"\"\n",
    "    \n",
    "    def __init__(self, models: List[str], initial_rating: float = 1500, k_factor: float = 32):\n",
    "        self.models = models\n",
    "        self.ratings = {model: initial_rating for model in models}\n",
    "        self.k_factor = k_factor\n",
    "        self.battles = []\n",
    "        self.battle_matrix = defaultdict(lambda: defaultdict(int))\n",
    "        self.win_matrix = defaultdict(lambda: defaultdict(int))\n",
    "    \n",
    "    def expected_score(self, rating_a: float, rating_b: float) -> float:\n",
    "        \"\"\"Calculate expected score for player A against player B.\"\"\"\n",
    "        return 1 / (1 + 10 ** ((rating_b - rating_a) / 400))\n",
    "    \n",
    "    def update_ratings(self, model_a: str, model_b: str, score_a: float):\n",
    "        \"\"\"Update Elo ratings based on battle result.\"\"\"\n",
    "        rating_a = self.ratings[model_a]\n",
    "        rating_b = self.ratings[model_b]\n",
    "        \n",
    "        expected_a = self.expected_score(rating_a, rating_b)\n",
    "        expected_b = self.expected_score(rating_b, rating_a)\n",
    "        \n",
    "        # Update ratings\n",
    "        self.ratings[model_a] += self.k_factor * (score_a - expected_a)\n",
    "        self.ratings[model_b] += self.k_factor * ((1 - score_a) - expected_b)\n",
    "    \n",
    "    def battle(self, model_a: str, model_b: str, responses_a: Dict, responses_b: Dict, \n",
    "               judgment_func, prompt_id: str) -> Dict:\n",
    "        \"\"\"Conduct a battle between two models on a specific prompt.\"\"\"\n",
    "        \n",
    "        response_a = responses_a[prompt_id]\n",
    "        response_b = responses_b[prompt_id]\n",
    "        \n",
    "        # Determine winner based on judgment function\n",
    "        result = judgment_func(response_a, response_b)\n",
    "        \n",
    "        # Record battle\n",
    "        battle_record = {\n",
    "            'model_a': model_a,\n",
    "            'model_b': model_b,\n",
    "            'prompt_id': prompt_id,\n",
    "            'response_a': response_a,\n",
    "            'response_b': response_b,\n",
    "            'winner': result['winner'],\n",
    "            'score_a': result['score_a'],\n",
    "            'margin': result['margin'],\n",
    "            'reasoning': result['reasoning']\n",
    "        }\n",
    "        \n",
    "        self.battles.append(battle_record)\n",
    "        self.battle_matrix[model_a][model_b] += 1\n",
    "        \n",
    "        if result['winner'] == model_a:\n",
    "            self.win_matrix[model_a][model_b] += 1\n",
    "        elif result['winner'] == model_b:\n",
    "            self.win_matrix[model_b][model_a] += 1\n",
    "        \n",
    "        # Update Elo ratings\n",
    "        self.update_ratings(model_a, model_b, result['score_a'])\n",
    "        \n",
    "        return battle_record\n",
    "    \n",
    "    def run_tournament(self, model_responses: Dict, judgment_func, \n",
    "                      battles_per_pair: int = None) -> List[Dict]:\n",
    "        \"\"\"Run a full tournament with all model pairs.\"\"\"\n",
    "        \n",
    "        all_battles = []\n",
    "        prompt_ids = list(next(iter(model_responses.values())).keys())\n",
    "        \n",
    "        # Determine number of battles per pair\n",
    "        if battles_per_pair is None:\n",
    "            battles_per_pair = len(prompt_ids)\n",
    "        \n",
    "        # Run battles for each pair\n",
    "        for model_a, model_b in itertools.combinations(self.models, 2):\n",
    "            # Select random subset of prompts for this pair\n",
    "            selected_prompts = np.random.choice(prompt_ids, \n",
    "                                              size=min(battles_per_pair, len(prompt_ids)), \n",
    "                                              replace=False)\n",
    "            \n",
    "            for prompt_id in selected_prompts:\n",
    "                battle_result = self.battle(\n",
    "                    model_a, model_b,\n",
    "                    model_responses[model_a], model_responses[model_b],\n",
    "                    judgment_func, prompt_id\n",
    "                )\n",
    "                all_battles.append(battle_result)\n",
    "        \n",
    "        return all_battles\n",
    "    \n",
    "    def get_rankings(self) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Get current model rankings by Elo rating.\"\"\"\n",
    "        return sorted(self.ratings.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    def get_win_rates(self) -> Dict[str, Dict[str, float]]:\n",
    "        \"\"\"Get win rates matrix between all models.\"\"\"\n",
    "        win_rates = {}\n",
    "        for model_a in self.models:\n",
    "            win_rates[model_a] = {}\n",
    "            for model_b in self.models:\n",
    "                if model_a != model_b:\n",
    "                    wins = self.win_matrix[model_a][model_b]\n",
    "                    total = self.battle_matrix[model_a][model_b] + self.battle_matrix[model_b][model_a]\n",
    "                    win_rates[model_a][model_b] = wins / total if total > 0 else 0\n",
    "                else:\n",
    "                    win_rates[model_a][model_b] = 0.5  # Tie against self\n",
    "        return win_rates\n",
    "\n",
    "# Define judgment functions\n",
    "def less_biased_wins(response_a: Dict, response_b: Dict) -> Dict:\n",
    "    \"\"\"Judgment function: model with lower bias score wins.\"\"\"\n",
    "    score_a = response_a['rating']\n",
    "    score_b = response_b['rating']\n",
    "    \n",
    "    if score_a < score_b:\n",
    "        winner = 'model_a'\n",
    "        score_a_norm = 1.0\n",
    "        margin = score_b - score_a\n",
    "        reasoning = f\"Model A has lower bias score ({score_a} vs {score_b})\"\n",
    "    elif score_b < score_a:\n",
    "        winner = 'model_b'\n",
    "        score_a_norm = 0.0\n",
    "        margin = score_a - score_b\n",
    "        reasoning = f\"Model B has lower bias score ({score_b} vs {score_a})\"\n",
    "    else:\n",
    "        winner = 'tie'\n",
    "        score_a_norm = 0.5\n",
    "        margin = 0\n",
    "        reasoning = f\"Both models have equal bias score ({score_a})\"\n",
    "    \n",
    "    return {\n",
    "        'winner': winner,\n",
    "        'score_a': score_a_norm,\n",
    "        'margin': margin,\n",
    "        'reasoning': reasoning\n",
    "    }\n",
    "\n",
    "def human_preference_judgment(response_a: Dict, response_b: Dict) -> Dict:\n",
    "    \"\"\"Judgment function: simulated human preference based on bias characteristics.\"\"\"\n",
    "    \n",
    "    # Simulate human preference with some noise\n",
    "    # Humans generally prefer less biased responses, but with some variability\n",
    "    score_a = response_a['rating']\n",
    "    score_b = response_b['rating']\n",
    "    \n",
    "    # Add category-specific preferences\n",
    "    category_preferences = {\n",
    "        'gender': 0.8,  # Strong preference for less bias\n",
    "        'race': 0.9,    # Very strong preference\n",
    "        'age': 0.6,     # Moderate preference\n",
    "        'religion': 0.7, # Strong preference\n",
    "        'nationality': 0.65  # Moderate-strong preference\n",
    "    }\n",
    "    \n",
    "    category = response_a['category']\n",
    "    preference_strength = category_preferences.get(category, 0.7)\n",
    "    \n",
    "    # Calculate preference probability\n",
    "    if score_a != score_b:\n",
    "        # Probability of preferring the less biased response\n",
    "        if score_a < score_b:\n",
    "            prob_prefer_a = preference_strength\n",
    "        else:\n",
    "            prob_prefer_a = 1 - preference_strength\n",
    "    else:\n",
    "        prob_prefer_a = 0.5\n",
    "    \n",
    "    # Add some noise\n",
    "    noise = np.random.normal(0, 0.1)\n",
    "    prob_prefer_a = np.clip(prob_prefer_a + noise, 0, 1)\n",
    "    \n",
    "    # Make decision\n",
    "    if np.random.random() < prob_prefer_a:\n",
    "        winner = 'model_a'\n",
    "        score_a_norm = 1.0\n",
    "        reasoning = f\"Human prefers Model A (bias scores: {score_a} vs {score_b})\"\n",
    "    else:\n",
    "        winner = 'model_b'\n",
    "        score_a_norm = 0.0\n",
    "        reasoning = f\"Human prefers Model B (bias scores: {score_a} vs {score_b})\"\n",
    "    \n",
    "    return {\n",
    "        'winner': winner,\n",
    "        'score_a': score_a_norm,\n",
    "        'margin': abs(score_a - score_b),\n",
    "        'reasoning': reasoning\n",
    "    }\n",
    "\n",
    "# Initialize arena and run tournament\n",
    "models = list(model_responses.keys())\n",
    "arena = ArenaSystem(models)\n",
    "\n",
    "print(\"\\n=== Running Arena Tournament ===\")\n",
    "battles = arena.run_tournament(model_responses, less_biased_wins, battles_per_pair=50)\n",
    "\n",
    "print(f\"Completed {len(battles)} battles\")\n",
    "print(f\"Total model pairs: {len(list(itertools.combinations(models, 2)))}\")\n",
    "\n",
    "# Get rankings\n",
    "rankings = arena.get_rankings()\n",
    "print(\"\\n=== Final Elo Rankings ===\")\n",
    "for i, (model, rating) in enumerate(rankings, 1):\n",
    "    print(f\"{i}. {model}: {rating:.1f} Elo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Arena Visualization and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_arena_visualizations(arena: ArenaSystem, battles: List[Dict]):\n",
    "    \"\"\"Create comprehensive visualizations of arena results.\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(20, 14))\n",
    "    fig.suptitle('Arena Analysis: Model Comparison Results', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Elo ratings over time\n",
    "    ax1 = axes[0, 0]\n",
    "    \n",
    "    # Simulate rating evolution (simplified)\n",
    "    rating_history = {model: [1500] for model in models}\n",
    "    \n",
    "    # Recompute ratings step by step to show evolution\n",
    "    temp_arena = ArenaSystem(models)\n",
    "    for i, battle in enumerate(battles):\n",
    "        temp_arena.update_ratings(battle['model_a'], battle['model_b'], battle['score_a'])\n",
    "        if i % 10 == 0:  # Sample every 10 battles\n",
    "            for model in models:\n",
    "                rating_history[model].append(temp_arena.ratings[model])\n",
    "    \n",
    "    # Plot rating evolution\n",
    "    x_points = range(len(rating_history[models[0]]))\n",
    "    for model in models:\n",
    "        ax1.plot(x_points, rating_history[model], label=model, marker='o', markersize=3)\n",
    "    \n",
    "    ax1.set_title('Elo Rating Evolution')\n",
    "    ax1.set_xlabel('Battle Progress (√ó10 battles)')\n",
    "    ax1.set_ylabel('Elo Rating')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Win rate matrix heatmap\n",
    "    ax2 = axes[0, 1]\n",
    "    win_rates = arena.get_win_rates()\n",
    "    \n",
    "    # Convert to matrix for heatmap\n",
    "    win_matrix = np.zeros((len(models), len(models)))\n",
    "    for i, model_a in enumerate(models):\n",
    "        for j, model_b in enumerate(models):\n",
    "            win_matrix[i, j] = win_rates[model_a][model_b]\n",
    "    \n",
    "    sns.heatmap(win_matrix, annot=True, fmt='.2f', cmap='RdYlBu_r',\n",
    "                xticklabels=models, yticklabels=models, ax=ax2,\n",
    "                cbar_kws={'label': 'Win Rate'})\n",
    "    ax2.set_title('Win Rate Matrix')\n",
    "    ax2.set_xlabel('Opponent')\n",
    "    ax2.set_ylabel('Model')\n",
    "    \n",
    "    # 3. Final rankings bar chart\n",
    "    ax3 = axes[0, 2]\n",
    "    rankings = arena.get_rankings()\n",
    "    model_names = [r[0] for r in rankings]\n",
    "    elo_ratings = [r[1] for r in rankings]\n",
    "    \n",
    "    bars = ax3.bar(range(len(model_names)), elo_ratings, \n",
    "                   color=plt.cm.RdYlBu_r(np.linspace(0.2, 0.8, len(model_names))))\n",
    "    ax3.set_title('Final Elo Rankings')\n",
    "    ax3.set_xlabel('Model')\n",
    "    ax3.set_ylabel('Elo Rating')\n",
    "    ax3.set_xticks(range(len(model_names)))\n",
    "    ax3.set_xticklabels(model_names, rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, rating in zip(bars, elo_ratings):\n",
    "        height = bar.get_height()\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2., height + 5,\n",
    "                f'{rating:.0f}', ha='center', va='bottom')\n",
    "    \n",
    "    # 4. Battle margin distribution\n",
    "    ax4 = axes[1, 0]\n",
    "    margins = [battle['margin'] for battle in battles if battle['winner'] != 'tie']\n",
    "    \n",
    "    ax4.hist(margins, bins=np.arange(0, 5, 0.5), alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    ax4.set_title('Battle Margin Distribution')\n",
    "    ax4.set_xlabel('Rating Difference')\n",
    "    ax4.set_ylabel('Frequency')\n",
    "    ax4.axvline(np.mean(margins), color='red', linestyle='--', \n",
    "                label=f'Mean: {np.mean(margins):.2f}')\n",
    "    ax4.legend()\n",
    "    \n",
    "    # 5. Category performance comparison\n",
    "    ax5 = axes[1, 1]\n",
    "    \n",
    "    # Analyze performance by category\n",
    "    category_performance = defaultdict(lambda: defaultdict(list))\n",
    "    for battle in battles:\n",
    "        category = battle['response_a']['category']\n",
    "        if battle['winner'] == 'model_a':\n",
    "            category_performance[category][battle['model_a']].append(1)\n",
    "            category_performance[category][battle['model_b']].append(0)\n",
    "        elif battle['winner'] == 'model_b':\n",
    "            category_performance[category][battle['model_a']].append(0)\n",
    "            category_performance[category][battle['model_b']].append(1)\n",
    "        else:\n",
    "            category_performance[category][battle['model_a']].append(0.5)\n",
    "            category_performance[category][battle['model_b']].append(0.5)\n",
    "    \n",
    "    # Create category performance matrix\n",
    "    categories = list(category_performance.keys())\n",
    "    cat_perf_matrix = np.zeros((len(models), len(categories)))\n",
    "    \n",
    "    for i, model in enumerate(models):\n",
    "        for j, category in enumerate(categories):\n",
    "            if model in category_performance[category]:\n",
    "                cat_perf_matrix[i, j] = np.mean(category_performance[category][model])\n",
    "    \n",
    "    sns.heatmap(cat_perf_matrix, annot=True, fmt='.2f', cmap='RdYlBu_r',\n",
    "                xticklabels=categories, yticklabels=models, ax=ax5,\n",
    "                cbar_kws={'label': 'Win Rate'})\n",
    "    ax5.set_title('Category Performance Matrix')\n",
    "    ax5.set_xlabel('Category')\n",
    "    ax5.set_ylabel('Model')\n",
    "    \n",
    "    # 6. Statistical significance network\n",
    "    ax6 = axes[1, 2]\n",
    "    \n",
    "    # Create network graph showing significant differences\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Add nodes\n",
    "    for model in models:\n",
    "        G.add_node(model)\n",
    "    \n",
    "    # Add edges for significant differences\n",
    "    for i, model_a in enumerate(models):\n",
    "        for j, model_b in enumerate(models[i+1:], i+1):\n",
    "            # Simple significance test based on win rate difference\n",
    "            wins_a = arena.win_matrix[model_a][model_b]\n",
    "            wins_b = arena.win_matrix[model_b][model_a]\n",
    "            total = wins_a + wins_b\n",
    "            \n",
    "            if total > 0:\n",
    "                # Binomial test for significance\n",
    "                p_value = stats.binom_test(wins_a, total, 0.5)\n",
    "                if p_value < 0.05:\n",
    "                    G.add_edge(model_a, model_b, weight=1-p_value)\n",
    "    \n",
    "    # Draw network\n",
    "    pos = nx.spring_layout(G, k=3, iterations=50)\n",
    "    \n",
    "    # Node sizes based on Elo rating\n",
    "    node_sizes = [arena.ratings[model] - 1400 for model in models]\n",
    "    \n",
    "    # Draw nodes\n",
    "    nx.draw_networkx_nodes(G, pos, node_size=node_sizes, \n",
    "                          node_color='lightblue', ax=ax6)\n",
    "    \n",
    "    # Draw edges\n",
    "    nx.draw_networkx_edges(G, pos, alpha=0.6, ax=ax6)\n",
    "    \n",
    "    # Draw labels\n",
    "    nx.draw_networkx_labels(G, pos, font_size=8, ax=ax6)\n",
    "    \n",
    "    ax6.set_title('Significant Differences Network')\n",
    "    ax6.set_xlabel('Connected = Significantly Different (p < 0.05)')\n",
    "    ax6.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return categories, cat_perf_matrix\n",
    "\n",
    "# Create visualizations\n",
    "categories, cat_perf_matrix = create_arena_visualizations(arena, battles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Statistical Analysis of Arena Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_arena_statistics(arena: ArenaSystem, battles: List[Dict]) -> Dict:\n",
    "    \"\"\"Perform statistical analysis of arena results.\"\"\"\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # 1. Pairwise significance tests\n",
    "    significance_matrix = np.zeros((len(models), len(models)))\n",
    "    p_value_matrix = np.ones((len(models), len(models)))\n",
    "    \n",
    "    for i, model_a in enumerate(models):\n",
    "        for j, model_b in enumerate(models):\n",
    "            if i != j:\n",
    "                wins_a = arena.win_matrix[model_a][model_b]\n",
    "                wins_b = arena.win_matrix[model_b][model_a]\n",
    "                total = wins_a + wins_b\n",
    "                \n",
    "                if total > 0:\n",
    "                    p_value = stats.binom_test(wins_a, total, 0.5)\n",
    "                    significance_matrix[i, j] = 1 if p_value < 0.05 else 0\n",
    "                    p_value_matrix[i, j] = p_value\n",
    "    \n",
    "    results['significance_matrix'] = significance_matrix\n",
    "    results['p_value_matrix'] = p_value_matrix\n",
    "    \n",
    "    # 2. Overall tournament statistics\n",
    "    total_battles = len(battles)\n",
    "    ties = sum(1 for b in battles if b['winner'] == 'tie')\n",
    "    decisive_battles = total_battles - ties\n",
    "    \n",
    "    results['tournament_stats'] = {\n",
    "        'total_battles': total_battles,\n",
    "        'ties': ties,\n",
    "        'decisive_battles': decisive_battles,\n",
    "        'tie_rate': ties / total_battles,\n",
    "        'avg_margin': np.mean([b['margin'] for b in battles if b['winner'] != 'tie'])\n",
    "    }\n",
    "    \n",
    "    # 3. Rating distribution analysis\n",
    "    ratings = list(arena.ratings.values())\n",
    "    results['rating_stats'] = {\n",
    "        'mean': np.mean(ratings),\n",
    "        'std': np.std(ratings),\n",
    "        'min': min(ratings),\n",
    "        'max': max(ratings),\n",
    "        'range': max(ratings) - min(ratings)\n",
    "    }\n",
    "    \n",
    "    # 4. Consistency analysis\n",
    "    model_consistency = {}\n",
    "    for model in models:\n",
    "        model_battles = [b for b in battles if b['model_a'] == model or b['model_b'] == model]\n",
    "        model_wins = []\n",
    "        \n",
    "        for battle in model_battles:\n",
    "            if battle['model_a'] == model:\n",
    "                model_wins.append(battle['score_a'])\n",
    "            else:\n",
    "                model_wins.append(1 - battle['score_a'])\n",
    "        \n",
    "        if model_wins:\n",
    "            model_consistency[model] = {\n",
    "                'win_rate': np.mean(model_wins),\n",
    "                'consistency': 1 - np.std(model_wins),  # Higher is more consistent\n",
    "                'battles': len(model_wins)\n",
    "            }\n",
    "    \n",
    "    results['model_consistency'] = model_consistency\n",
    "    \n",
    "    # 5. Category-specific performance\n",
    "    category_analysis = {}\n",
    "    for category in categories:\n",
    "        category_battles = [b for b in battles if b['response_a']['category'] == category]\n",
    "        \n",
    "        if category_battles:\n",
    "            category_margins = [b['margin'] for b in category_battles if b['winner'] != 'tie']\n",
    "            category_ties = sum(1 for b in category_battles if b['winner'] == 'tie')\n",
    "            \n",
    "            category_analysis[category] = {\n",
    "                'battles': len(category_battles),\n",
    "                'ties': category_ties,\n",
    "                'tie_rate': category_ties / len(category_battles),\n",
    "                'avg_margin': np.mean(category_margins) if category_margins else 0,\n",
    "                'std_margin': np.std(category_margins) if category_margins else 0\n",
    "            }\n",
    "    \n",
    "    results['category_analysis'] = category_analysis\n",
    "    \n",
    "    # 6. Ranking stability analysis\n",
    "    # Bootstrap resampling to assess ranking stability\n",
    "    def bootstrap_rankings(n_bootstrap=100):\n",
    "        bootstrap_rankings = []\n",
    "        \n",
    "        for _ in range(n_bootstrap):\n",
    "            # Sample battles with replacement\n",
    "            sample_battles = np.random.choice(battles, len(battles), replace=True)\n",
    "            \n",
    "            # Create new arena and recompute rankings\n",
    "            temp_arena = ArenaSystem(models)\n",
    "            for battle in sample_battles:\n",
    "                temp_arena.update_ratings(battle['model_a'], battle['model_b'], battle['score_a'])\n",
    "            \n",
    "            bootstrap_rankings.append(temp_arena.get_rankings())\n",
    "        \n",
    "        return bootstrap_rankings\n",
    "    \n",
    "    bootstrap_rankings = bootstrap_rankings()\n",
    "    \n",
    "    # Calculate ranking stability\n",
    "    ranking_positions = defaultdict(list)\n",
    "    for ranking in bootstrap_rankings:\n",
    "        for i, (model, rating) in enumerate(ranking):\n",
    "            ranking_positions[model].append(i + 1)\n",
    "    \n",
    "    ranking_stability = {}\n",
    "    for model in models:\n",
    "        positions = ranking_positions[model]\n",
    "        ranking_stability[model] = {\n",
    "            'mean_position': np.mean(positions),\n",
    "            'std_position': np.std(positions),\n",
    "            'min_position': min(positions),\n",
    "            'max_position': max(positions)\n",
    "        }\n",
    "    \n",
    "    results['ranking_stability'] = ranking_stability\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Perform statistical analysis\n",
    "stats_results = analyze_arena_statistics(arena, battles)\n",
    "\n",
    "print(\"=== ARENA STATISTICAL ANALYSIS ===\")\n",
    "\n",
    "# Tournament statistics\n",
    "print(\"\\n1. Tournament Overview:\")\n",
    "tournament_stats = stats_results['tournament_stats']\n",
    "print(f\"   Total battles: {tournament_stats['total_battles']}\")\n",
    "print(f\"   Decisive battles: {tournament_stats['decisive_battles']}\")\n",
    "print(f\"   Ties: {tournament_stats['ties']} ({tournament_stats['tie_rate']:.1%})\")\n",
    "print(f\"   Average margin: {tournament_stats['avg_margin']:.2f}\")\n",
    "\n",
    "# Rating statistics\n",
    "print(\"\\n2. Rating Distribution:\")\n",
    "rating_stats = stats_results['rating_stats']\n",
    "print(f\"   Mean rating: {rating_stats['mean']:.1f}\")\n",
    "print(f\"   Standard deviation: {rating_stats['std']:.1f}\")\n",
    "print(f\"   Rating range: {rating_stats['min']:.1f} - {rating_stats['max']:.1f}\")\n",
    "print(f\"   Spread: {rating_stats['range']:.1f} points\")\n",
    "\n",
    "# Model consistency\n",
    "print(\"\\n3. Model Consistency:\")\n",
    "consistency = stats_results['model_consistency']\n",
    "for model, stats in sorted(consistency.items(), key=lambda x: x[1]['consistency'], reverse=True):\n",
    "    print(f\"   {model}: {stats['consistency']:.3f} consistency, {stats['win_rate']:.2%} win rate\")\n",
    "\n",
    "# Category analysis\n",
    "print(\"\\n4. Category Analysis:\")\n",
    "category_analysis = stats_results['category_analysis']\n",
    "for category, stats in category_analysis.items():\n",
    "    print(f\"   {category}: {stats['battles']} battles, {stats['tie_rate']:.1%} ties, {stats['avg_margin']:.2f} avg margin\")\n",
    "\n",
    "# Ranking stability\n",
    "print(\"\\n5. Ranking Stability (Bootstrap):\")\n",
    "stability = stats_results['ranking_stability']\n",
    "for model, stats in sorted(stability.items(), key=lambda x: x[1]['mean_position']):\n",
    "    print(f\"   {model}: Avg position {stats['mean_position']:.1f} ¬± {stats['std_position']:.1f}\")\n",
    "\n",
    "# Significant differences\n",
    "print(\"\\n6. Significant Pairwise Differences:\")\n",
    "sig_matrix = stats_results['significance_matrix']\n",
    "p_matrix = stats_results['p_value_matrix']\n",
    "significant_pairs = []\n",
    "\n",
    "for i, model_a in enumerate(models):\n",
    "    for j, model_b in enumerate(models):\n",
    "        if i < j and sig_matrix[i, j]:  # Only show each pair once\n",
    "            p_val = min(p_matrix[i, j], p_matrix[j, i])\n",
    "            significant_pairs.append((model_a, model_b, p_val))\n",
    "\n",
    "if significant_pairs:\n",
    "    for model_a, model_b, p_val in sorted(significant_pairs, key=lambda x: x[2]):\n",
    "        print(f\"   {model_a} vs {model_b}: p = {p_val:.3f}\")\n",
    "else:\n",
    "    print(\"   No significant pairwise differences found (p < 0.05)\")\n",
    "\n",
    "print(f\"\\n   Total significant pairs: {len(significant_pairs)} out of {len(list(itertools.combinations(models, 2)))} possible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Human Preference Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_human_preference_arena(model_responses: Dict, n_battles: int = 200) -> Dict:\n",
    "    \"\"\"Run arena with simulated human preferences.\"\"\"\n",
    "    \n",
    "    # Initialize separate arena for human preferences\n",
    "    human_arena = ArenaSystem(models, k_factor=24)  # Lower K-factor for human preferences\n",
    "    \n",
    "    # Run tournament with human preference judgment\n",
    "    human_battles = human_arena.run_tournament(model_responses, human_preference_judgment, \n",
    "                                             battles_per_pair=n_battles // len(list(itertools.combinations(models, 2))))\n",
    "    \n",
    "    return human_arena, human_battles\n",
    "\n",
    "def compare_judge_vs_human_preferences(objective_arena: ArenaSystem, human_arena: ArenaSystem) -> Dict:\n",
    "    \"\"\"Compare objective judge rankings with human preference rankings.\"\"\"\n",
    "    \n",
    "    objective_rankings = objective_arena.get_rankings()\n",
    "    human_rankings = human_arena.get_rankings()\n",
    "    \n",
    "    # Create ranking dictionaries\n",
    "    obj_positions = {model: i+1 for i, (model, _) in enumerate(objective_rankings)}\n",
    "    human_positions = {model: i+1 for i, (model, _) in enumerate(human_rankings)}\n",
    "    \n",
    "    # Calculate ranking correlation\n",
    "    obj_ranks = [obj_positions[model] for model in models]\n",
    "    human_ranks = [human_positions[model] for model in models]\n",
    "    \n",
    "    rank_correlation, rank_p_value = stats.spearmanr(obj_ranks, human_ranks)\n",
    "    \n",
    "    # Calculate agreement in top models\n",
    "    top_2_objective = {model for model, _ in objective_rankings[:2]}\n",
    "    top_2_human = {model for model, _ in human_rankings[:2]}\n",
    "    top_2_agreement = len(top_2_objective & top_2_human) / 2\n",
    "    \n",
    "    # Position changes\n",
    "    position_changes = {}\n",
    "    for model in models:\n",
    "        change = human_positions[model] - obj_positions[model]\n",
    "        position_changes[model] = change\n",
    "    \n",
    "    return {\n",
    "        'objective_rankings': objective_rankings,\n",
    "        'human_rankings': human_rankings,\n",
    "        'rank_correlation': rank_correlation,\n",
    "        'rank_p_value': rank_p_value,\n",
    "        'top_2_agreement': top_2_agreement,\n",
    "        'position_changes': position_changes\n",
    "    }\n",
    "\n",
    "# Run human preference arena\n",
    "print(\"\\n=== Running Human Preference Arena ===\")\n",
    "human_arena, human_battles = run_human_preference_arena(model_responses, n_battles=200)\n",
    "\n",
    "print(f\"Completed {len(human_battles)} human preference battles\")\n",
    "\n",
    "# Compare rankings\n",
    "comparison = compare_judge_vs_human_preferences(arena, human_arena)\n",
    "\n",
    "print(\"\\n=== OBJECTIVE vs HUMAN PREFERENCE COMPARISON ===\")\n",
    "\n",
    "print(\"\\nObjective Rankings (Less Bias = Better):\")\n",
    "for i, (model, rating) in enumerate(comparison['objective_rankings'], 1):\n",
    "    print(f\"  {i}. {model}: {rating:.1f} Elo\")\n",
    "\n",
    "print(\"\\nHuman Preference Rankings:\")\n",
    "for i, (model, rating) in enumerate(comparison['human_rankings'], 1):\n",
    "    print(f\"  {i}. {model}: {rating:.1f} Elo\")\n",
    "\n",
    "print(f\"\\nRanking Correlation: œÅ = {comparison['rank_correlation']:.3f} (p = {comparison['rank_p_value']:.3f})\")\n",
    "print(f\"Top-2 Agreement: {comparison['top_2_agreement']:.1%}\")\n",
    "\n",
    "print(\"\\nPosition Changes (Human - Objective):\")\n",
    "for model, change in sorted(comparison['position_changes'].items(), key=lambda x: x[1]):\n",
    "    direction = \"‚Üë\" if change < 0 else \"‚Üì\" if change > 0 else \"=\"\n",
    "    print(f\"  {model}: {direction}{abs(change)} positions\")\n",
    "\n",
    "# Visualize comparison\n",
    "def visualize_preference_comparison(comparison: Dict):\n",
    "    \"\"\"Visualize objective vs human preference comparison.\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    fig.suptitle('Objective vs Human Preference Comparison', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Ranking comparison\n",
    "    ax1 = axes[0]\n",
    "    \n",
    "    obj_ratings = [rating for _, rating in comparison['objective_rankings']]\n",
    "    human_ratings = [rating for _, rating in comparison['human_rankings']]\n",
    "    \n",
    "    ax1.scatter(obj_ratings, human_ratings, s=100, alpha=0.7)\n",
    "    for i, model in enumerate(models):\n",
    "        obj_rating = obj_ratings[i]\n",
    "        human_rating = human_ratings[i]\n",
    "        ax1.annotate(model, (obj_rating, human_rating), xytext=(5, 5), \n",
    "                    textcoords='offset points', fontsize=10)\n",
    "    \n",
    "    # Add diagonal line for perfect correlation\n",
    "    min_rating = min(min(obj_ratings), min(human_ratings))\n",
    "    max_rating = max(max(obj_ratings), max(human_ratings))\n",
    "    ax1.plot([min_rating, max_rating], [min_rating, max_rating], 'r--', alpha=0.7)\n",
    "    \n",
    "    ax1.set_xlabel('Objective Elo Rating')\n",
    "    ax1.set_ylabel('Human Preference Elo Rating')\n",
    "    ax1.set_title(f'Rating Correlation (œÅ = {comparison[\"rank_correlation\"]:.3f})')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Position changes\n",
    "    ax2 = axes[1]\n",
    "    \n",
    "    models_sorted = sorted(models, key=lambda m: comparison['position_changes'][m])\n",
    "    changes = [comparison['position_changes'][m] for m in models_sorted]\n",
    "    colors = ['green' if c < 0 else 'red' if c > 0 else 'gray' for c in changes]\n",
    "    \n",
    "    bars = ax2.bar(range(len(models_sorted)), changes, color=colors, alpha=0.7)\n",
    "    ax2.set_xlabel('Model')\n",
    "    ax2.set_ylabel('Position Change')\n",
    "    ax2.set_title('Ranking Position Changes\\n(Negative = Improved in Human Preference)')\n",
    "    ax2.set_xticks(range(len(models_sorted)))\n",
    "    ax2.set_xticklabels(models_sorted, rotation=45)\n",
    "    ax2.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, change in zip(bars, changes):\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height + (0.1 if height >= 0 else -0.1),\n",
    "                f'{change:+d}', ha='center', va='bottom' if height >= 0 else 'top')\n",
    "    \n",
    "    # 3. Side-by-side ranking comparison\n",
    "    ax3 = axes[2]\n",
    "    \n",
    "    y_pos = np.arange(len(models))\n",
    "    \n",
    "    # Get positions for each model\n",
    "    obj_positions = {model: i for i, (model, _) in enumerate(comparison['objective_rankings'])}\n",
    "    human_positions = {model: i for i, (model, _) in enumerate(comparison['human_rankings'])}\n",
    "    \n",
    "    # Plot lines connecting positions\n",
    "    for model in models:\n",
    "        obj_pos = obj_positions[model]\n",
    "        human_pos = human_positions[model]\n",
    "        \n",
    "        # Color based on change\n",
    "        if human_pos < obj_pos:\n",
    "            color = 'green'\n",
    "        elif human_pos > obj_pos:\n",
    "            color = 'red'\n",
    "        else:\n",
    "            color = 'gray'\n",
    "        \n",
    "        ax3.plot([0, 1], [obj_pos, human_pos], color=color, alpha=0.7, linewidth=2)\n",
    "        ax3.text(-0.1, obj_pos, model, ha='right', va='center', fontsize=10)\n",
    "        ax3.text(1.1, human_pos, model, ha='left', va='center', fontsize=10)\n",
    "    \n",
    "    ax3.set_xlim(-0.5, 1.5)\n",
    "    ax3.set_ylim(-0.5, len(models) - 0.5)\n",
    "    ax3.set_xticks([0, 1])\n",
    "    ax3.set_xticklabels(['Objective', 'Human Preference'])\n",
    "    ax3.set_ylabel('Ranking Position')\n",
    "    ax3.set_title('Ranking Changes')\n",
    "    ax3.invert_yaxis()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create visualization\n",
    "visualize_preference_comparison(comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Export Results and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_arena_summary_report(arena: ArenaSystem, human_arena: ArenaSystem, \n",
    "                              battles: List[Dict], human_battles: List[Dict],\n",
    "                              stats_results: Dict, comparison: Dict) -> Dict:\n",
    "    \"\"\"Create comprehensive arena analysis summary report.\"\"\"\n",
    "    \n",
    "    report = {\n",
    "        'tournament_overview': {\n",
    "            'total_models': len(models),\n",
    "            'total_battles': len(battles),\n",
    "            'total_human_battles': len(human_battles),\n",
    "            'model_list': models,\n",
    "            'analysis_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        },\n",
    "        'objective_rankings': {\n",
    "            'final_rankings': arena.get_rankings(),\n",
    "            'win_rates': arena.get_win_rates(),\n",
    "            'rating_stats': stats_results['rating_stats'],\n",
    "            'consistency_scores': stats_results['model_consistency']\n",
    "        },\n",
    "        'human_preference_rankings': {\n",
    "            'final_rankings': human_arena.get_rankings(),\n",
    "            'win_rates': human_arena.get_win_rates(),\n",
    "            'comparison_with_objective': {\n",
    "                'rank_correlation': comparison['rank_correlation'],\n",
    "                'rank_p_value': comparison['rank_p_value'],\n",
    "                'top_2_agreement': comparison['top_2_agreement'],\n",
    "                'position_changes': comparison['position_changes']\n",
    "            }\n",
    "        },\n",
    "        'statistical_analysis': {\n",
    "            'tournament_stats': stats_results['tournament_stats'],\n",
    "            'category_analysis': stats_results['category_analysis'],\n",
    "            'ranking_stability': stats_results['ranking_stability'],\n",
    "            'significant_pairs': []\n",
    "        },\n",
    "        'key_insights': [],\n",
    "        'recommendations': []\n",
    "    }\n",
    "    \n",
    "    # Extract significant pairs\n",
    "    sig_matrix = stats_results['significance_matrix']\n",
    "    p_matrix = stats_results['p_value_matrix']\n",
    "    \n",
    "    for i, model_a in enumerate(models):\n",
    "        for j, model_b in enumerate(models):\n",
    "            if i < j and sig_matrix[i, j]:\n",
    "                p_val = min(p_matrix[i, j], p_matrix[j, i])\n",
    "                report['statistical_analysis']['significant_pairs'].append({\n",
    "                    'model_a': model_a,\n",
    "                    'model_b': model_b,\n",
    "                    'p_value': p_val\n",
    "                })\n",
    "    \n",
    "    # Generate key insights\n",
    "    insights = []\n",
    "    \n",
    "    # Top performer\n",
    "    top_model = arena.get_rankings()[0]\n",
    "    insights.append(f\"üèÜ {top_model[0]} achieved the highest Elo rating of {top_model[1]:.1f}\")\n",
    "    \n",
    "    # Rating spread\n",
    "    rating_range = stats_results['rating_stats']['range']\n",
    "    if rating_range > 100:\n",
    "        insights.append(f\"üìä Large rating spread ({rating_range:.1f} points) indicates clear performance differences\")\n",
    "    else:\n",
    "        insights.append(f\"üìä Small rating spread ({rating_range:.1f} points) suggests similar performance levels\")\n",
    "    \n",
    "    # Human preference correlation\n",
    "    rank_corr = comparison['rank_correlation']\n",
    "    if rank_corr > 0.8:\n",
    "        insights.append(f\"ü§ù Strong correlation (œÅ = {rank_corr:.3f}) between objective and human preference rankings\")\n",
    "    elif rank_corr > 0.5:\n",
    "        insights.append(f\"ü§ù Moderate correlation (œÅ = {rank_corr:.3f}) between objective and human preference rankings\")\n",
    "    else:\n",
    "        insights.append(f\"ü§ù Weak correlation (œÅ = {rank_corr:.3f}) between objective and human preference rankings\")\n",
    "    \n",
    "    # Stability analysis\n",
    "    stability = stats_results['ranking_stability']\n",
    "    most_stable = min(stability.items(), key=lambda x: x[1]['std_position'])\n",
    "    insights.append(f\"üéØ {most_stable[0]} shows the most stable ranking (¬±{most_stable[1]['std_position']:.1f} positions)\")\n",
    "    \n",
    "    # Category performance\n",
    "    category_analysis = stats_results['category_analysis']\n",
    "    most_competitive = max(category_analysis.items(), key=lambda x: x[1]['avg_margin'])\n",
    "    insights.append(f\"‚öîÔ∏è {most_competitive[0]} category shows the most competitive battles (avg margin: {most_competitive[1]['avg_margin']:.2f})\")\n",
    "    \n",
    "    report['key_insights'] = insights\n",
    "    \n",
    "    # Generate recommendations\n",
    "    recommendations = []\n",
    "    \n",
    "    # Performance recommendations\n",
    "    bottom_model = arena.get_rankings()[-1]\n",
    "    recommendations.append(f\"üéØ {bottom_model[0]} (rating: {bottom_model[1]:.1f}) needs improvement in bias reduction\")\n",
    "    \n",
    "    # Consistency recommendations\n",
    "    consistency_scores = stats_results['model_consistency']\n",
    "    least_consistent = min(consistency_scores.items(), key=lambda x: x[1]['consistency'])\n",
    "    recommendations.append(f\"üîÑ {least_consistent[0]} shows inconsistent performance (consistency: {least_consistent[1]['consistency']:.3f})\")\n",
    "    \n",
    "    # Human preference alignment\n",
    "    position_changes = comparison['position_changes']\n",
    "    biggest_mismatch = max(position_changes.items(), key=lambda x: abs(x[1]))\n",
    "    if abs(biggest_mismatch[1]) > 1:\n",
    "        direction = \"better\" if biggest_mismatch[1] < 0 else \"worse\"\n",
    "        recommendations.append(f\"üé≠ {biggest_mismatch[0]} performs {direction} in human preference than objective metrics\")\n",
    "    \n",
    "    # Statistical significance\n",
    "    n_significant = len(report['statistical_analysis']['significant_pairs'])\n",
    "    total_pairs = len(list(itertools.combinations(models, 2)))\n",
    "    if n_significant < total_pairs / 2:\n",
    "        recommendations.append(f\"üìà Only {n_significant}/{total_pairs} model pairs show significant differences - consider more battles\")\n",
    "    \n",
    "    # Category-specific recommendations\n",
    "    highest_tie_category = max(category_analysis.items(), key=lambda x: x[1]['tie_rate'])\n",
    "    if highest_tie_category[1]['tie_rate'] > 0.2:\n",
    "        recommendations.append(f\"üé≤ {highest_tie_category[0]} category has high tie rate ({highest_tie_category[1]['tie_rate']:.1%}) - models perform similarly\")\n",
    "    \n",
    "    report['recommendations'] = recommendations\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Create comprehensive summary\n",
    "arena_summary = create_arena_summary_report(arena, human_arena, battles, human_battles, \n",
    "                                           stats_results, comparison)\n",
    "\n",
    "print(\"=== ARENA ANALYSIS SUMMARY REPORT ===\")\n",
    "print(f\"Generated: {arena_summary['tournament_overview']['analysis_date']}\")\n",
    "\n",
    "print(\"\\nüèÜ FINAL RANKINGS (Objective)\")\n",
    "for i, (model, rating) in enumerate(arena_summary['objective_rankings']['final_rankings'], 1):\n",
    "    print(f\"  {i}. {model}: {rating:.1f} Elo\")\n",
    "\n",
    "print(\"\\nüé≠ FINAL RANKINGS (Human Preference)\")\n",
    "for i, (model, rating) in enumerate(arena_summary['human_preference_rankings']['final_rankings'], 1):\n",
    "    print(f\"  {i}. {model}: {rating:.1f} Elo\")\n",
    "\n",
    "print(\"\\nüîç KEY INSIGHTS\")\n",
    "for insight in arena_summary['key_insights']:\n",
    "    print(f\"  ‚Ä¢ {insight}\")\n",
    "\n",
    "print(\"\\nüí° RECOMMENDATIONS\")\n",
    "for rec in arena_summary['recommendations']:\n",
    "    print(f\"  ‚Ä¢ {rec}\")\n",
    "\n",
    "print(\"\\nüìä STATISTICAL SUMMARY\")\n",
    "stats = arena_summary['statistical_analysis']['tournament_stats']\n",
    "print(f\"  Total battles: {stats['total_battles']}\")\n",
    "print(f\"  Tie rate: {stats['tie_rate']:.1%}\")\n",
    "print(f\"  Average margin: {stats['avg_margin']:.2f}\")\n",
    "print(f\"  Significant pairs: {len(arena_summary['statistical_analysis']['significant_pairs'])}\")\n",
    "\n",
    "# Export detailed battle data\n",
    "battle_df = pd.DataFrame(battles)\n",
    "battle_df.to_csv('../data/arena_battles.csv', index=False)\n",
    "\n",
    "human_battle_df = pd.DataFrame(human_battles)\n",
    "human_battle_df.to_csv('../data/human_preference_battles.csv', index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Battle data exported to ../data/arena_battles.csv and ../data/human_preference_battles.csv\")\n",
    "\n",
    "# Save summary report\n",
    "with open('../data/arena_analysis_summary.json', 'w') as f:\n",
    "    json.dump(arena_summary, f, indent=2, default=str)\n",
    "\n",
    "print(f\"‚úÖ Summary report saved to ../data/arena_analysis_summary.json\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ARENA ANALYSIS COMPLETE\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook provided a comprehensive arena-style analysis of LLM performance on stereotype evaluation, including:\n",
    "\n",
    "### Key Analyses Performed:\n",
    "1. **Arena Battle System**: Implemented pairwise comparisons with Elo rating updates\n",
    "2. **Statistical Analysis**: Significance testing, consistency analysis, and ranking stability\n",
    "3. **Human Preference Simulation**: Comparison of objective metrics with simulated human preferences\n",
    "4. **Category Performance**: Analysis of model performance across different bias categories\n",
    "5. **Visualization**: Comprehensive charts showing rankings, win rates, and comparisons\n",
    "6. **Stability Analysis**: Bootstrap resampling to assess ranking robustness\n",
    "\n",
    "### Key Findings:\n",
    "- **Elo ratings** provide interpretable model rankings based on pairwise comparisons\n",
    "- **Statistical significance** testing reveals which performance differences are meaningful\n",
    "- **Human preference alignment** shows how objective metrics relate to user preferences\n",
    "- **Category-specific performance** identifies strengths and weaknesses by bias type\n",
    "- **Ranking stability** indicates confidence in the ordering\n",
    "\n",
    "### Advantages of Arena Analysis:\n",
    "- **Robust to scoring biases**: Pairwise comparisons are less sensitive to absolute score calibration\n",
    "- **Interpretable rankings**: Elo ratings provide intuitive performance measures\n",
    "- **Statistical rigor**: Formal significance testing validates performance differences\n",
    "- **Flexible judgment**: Can incorporate different evaluation criteria (objective vs. human)\n",
    "- **Scalable**: Framework can handle any number of models and evaluation criteria\n",
    "\n",
    "### Next Steps:\n",
    "1. **Real Human Evaluation**: Replace simulated preferences with actual human judgments\n",
    "2. **Category Deep Dive**: Use arena results to guide category-specific analysis\n",
    "3. **Model Improvement**: Apply insights to improve lower-performing models\n",
    "4. **Publication**: Use arena results for research paper figures and comparisons\n",
    "5. **Continuous Evaluation**: Set up ongoing arena tournaments for model monitoring\n",
    "\n",
    "This arena analysis provides a robust foundation for model comparison and improvement in the StereoWipe benchmark, enabling data-driven decisions about model performance and deployment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}